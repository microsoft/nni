# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, Microsoft
# This file is distributed under the same license as the NNI package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: NNI \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-05-08 16:52+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:13
msgid ""
"Click :ref:`here "
"<sphx_glr_download_tutorials_hpo_quickstart_pytorch_main.py>` to download"
" the full example code"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:22
msgid "HPO Quickstart with PyTorch"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:23
msgid ""
"This tutorial optimizes the model in `official PyTorch quickstart`_ with "
"auto-tuning."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:25
msgid "The tutorial consists of 4 steps:"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:27
msgid "Modify the model for auto-tuning."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:28
msgid "Define hyperparameters' search space."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:29
msgid "Configure the experiment."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:30
msgid "Run the experiment."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:37
msgid "Step 1: Prepare the model"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:38
msgid "In first step, we need to prepare the model to be tuned."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:40
msgid ""
"The model should be put in a separate script. It will be evaluated many "
"times concurrently, and possibly will be trained on distributed "
"platforms."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:44
msgid "In this tutorial, the model is defined in :doc:`model.py <model>`."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:46
msgid "In short, it is a PyTorch model with 3 additional API calls:"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:48
msgid ""
"Use :func:`nni.get_next_parameter` to fetch the hyperparameters to be "
"evalutated."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:49
msgid ""
"Use :func:`nni.report_intermediate_result` to report per-epoch accuracy "
"metrics."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:50
msgid "Use :func:`nni.report_final_result` to report final accuracy."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:52
msgid "Please understand the model code before continue to next step."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:57
msgid "Step 2: Define search space"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:58
msgid ""
"In model code, we have prepared 3 hyperparameters to be tuned: "
"*features*, *lr*, and *momentum*."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:61
msgid ""
"Here we need to define their *search space* so the tuning algorithm can "
"sample them in desired range."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:63
msgid "Assuming we have following prior knowledge for these hyperparameters:"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:65
msgid "*features* should be one of 128, 256, 512, 1024."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:66
msgid ""
"*lr* should be a float between 0.0001 and 0.1, and it follows exponential"
" distribution."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:67
msgid "*momentum* should be a float between 0 and 1."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:69
msgid ""
"In NNI, the space of *features* is called ``choice``; the space of *lr* "
"is called ``loguniform``; and the space of *momentum* is called "
"``uniform``. You may have noticed, these names are derived from "
"``numpy.random``."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:74
msgid ""
"For full specification of search space, check :doc:`the reference "
"</hpo/search_space>`."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:76
msgid "Now we can define the search space as follow:"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:99
msgid "Step 3: Configure the experiment"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:100
msgid ""
"NNI uses an *experiment* to manage the HPO process. The *experiment "
"config* defines how to train the models and how to explore the search "
"space."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:103
msgid ""
"In this tutorial we use a *local* mode experiment, which means models "
"will be trained on local machine, without using any special training "
"platform."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:122
msgid "Now we start to configure the experiment."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:125
msgid "Configure trial code"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:126
msgid ""
"In NNI evaluation of each hyperparameter set is called a *trial*. So the "
"model script is called *trial code*."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:144
msgid ""
"When ``trial_code_directory`` is a relative path, it relates to current "
"working directory. To run ``main.py`` in a different path, you can set "
"trial code directory to ``Path(__file__).parent``. (`__file__ "
"<https://docs.python.org/3.10/reference/datamodel.html#index-43>`__ is "
"only available in standard Python, not in Jupyter Notebook.)"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:151
msgid ""
"If you are using Linux system without Conda, you may need to change "
"``\"python model.py\"`` to ``\"python3 model.py\"``."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:157
msgid "Configure search space"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:175
msgid "Configure tuning algorithm"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:176
msgid "Here we use :doc:`TPE tuner </hpo/tuners>`."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:195
msgid "Configure how many trials to run"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:196
msgid ""
"Here we evaluate 10 sets of hyperparameters in total, and concurrently "
"evaluate 2 sets at a time."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:213
msgid "You may also set ``max_experiment_duration = '1h'`` to limit running time."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:215
msgid ""
"If neither ``max_trial_number`` nor ``max_experiment_duration`` are set, "
"the experiment will run forever until you press Ctrl-C."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:220
msgid ""
"``max_trial_number`` is set to 10 here for a fast example. In real world "
"it should be set to a larger number. With default config TPE tuner "
"requires 20 trials to warm up."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:227
msgid "Step 4: Run the experiment"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:228
msgid ""
"Now the experiment is ready. Choose a port and launch it. (Here we use "
"port 8080.)"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:230
msgid ""
"You can use the web portal to view experiment status: "
"http://localhost:8080."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:244
#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:281
msgid "Out:"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:260
msgid "After the experiment is done"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:261
msgid "Everything is done and it is safe to exit now. The following are optional."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:263
msgid ""
"If you are using standard Python instead of Jupyter Notebook, you can add"
" ``input()`` or ``signal.pause()`` to prevent Python from exiting, "
"allowing you to view the web portal after the experiment is done."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:293
msgid ""
":meth:`nni.experiment.Experiment.stop` is automatically invoked when "
"Python exits, so it can be omitted in your code."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:296
msgid ""
"After the experiment is stopped, you can run "
":meth:`nni.experiment.Experiment.view` to restart web portal."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:300
msgid ""
"This example uses :doc:`Python API </reference/experiment>` to create "
"experiment."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:302
msgid ""
"You can also create and manage experiments with :doc:`command line tool "
"<../hpo_nnictl/nnictl>`."
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:307
msgid "**Total running time of the script:** ( 1 minutes  24.367 seconds)"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:322
msgid ":download:`Download Python source code: main.py <main.py>`"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:328
msgid ":download:`Download Jupyter notebook: main.ipynb <main.ipynb>`"
msgstr ""

#: ../../source/tutorials/hpo_quickstart_pytorch/main.rst:335
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""

#~ msgid "**Total running time of the script:** ( 2 minutes  15.810 seconds)"
#~ msgstr ""

#~ msgid "NNI HPO Quickstart with PyTorch"
#~ msgstr ""

#~ msgid ""
#~ "There is also a :doc:`TensorFlow "
#~ "version<../hpo_quickstart_tensorflow/main>` if you "
#~ "prefer it."
#~ msgstr ""

#~ msgid ""
#~ "You can also create and manage "
#~ "experiments with :doc:`command line tool "
#~ "</reference/nnictl>`."
#~ msgstr ""

#~ msgid "**Total running time of the script:** ( 1 minutes  24.393 seconds)"
#~ msgstr ""

#~ msgid "**Total running time of the script:** ( 0 minutes  58.337 seconds)"
#~ msgstr ""

#~ msgid "**Total running time of the script:** ( 1 minutes  30.730 seconds)"
#~ msgstr ""

#~ msgid ""
#~ "Click :ref:`here "
#~ "<sphx_glr_download_tutorials_hello_nas.py>` to download"
#~ " the full example code"
#~ msgstr ""

#~ msgid ""
#~ "Currently, PyTorch is the only supported"
#~ " framework by Retiarii, and we have"
#~ " only tested **PyTorch 1.7 to 1.10**."
#~ " This tutorial assumes PyTorch context "
#~ "but it should also apply to other"
#~ " frameworks, which is in our future"
#~ " plan."
#~ msgstr ""

#~ msgid ""
#~ "Defining a base model is almost "
#~ "the same as defining a PyTorch (or"
#~ " TensorFlow) model. Usually, you only "
#~ "need to replace the code ``import "
#~ "torch.nn as nn`` with ``import "
#~ "nni.retiarii.nn.pytorch as nn`` to use "
#~ "our wrapped PyTorch modules."
#~ msgstr ""

#~ msgid ""
#~ "Always keep in mind that you "
#~ "should use ``import nni.retiarii.nn.pytorch as"
#~ " nn`` and :meth:`nni.retiarii.model_wrapper`. "
#~ "Many mistakes are a result of "
#~ "forgetting one of those. Also, please"
#~ " use ``torch.nn`` for submodules of "
#~ "``nn.init``, e.g., ``torch.nn.init`` instead "
#~ "of ``nn.init``."
#~ msgstr ""

#~ msgid "Define Model Mutations"
#~ msgstr ""

#~ msgid ""
#~ "This example uses two mutation APIs, "
#~ ":class:`nn.LayerChoice <nni.retiarii.nn.pytorch.LayerChoice>`"
#~ " and :class:`nn.InputChoice "
#~ "<nni.retiarii.nn.pytorch.ValueChoice>`. :class:`nn.LayerChoice"
#~ " <nni.retiarii.nn.pytorch.LayerChoice>` takes a "
#~ "list of candidate modules (two in "
#~ "this example), one will be chosen "
#~ "for each sampled model. It can be"
#~ " used like normal PyTorch module. "
#~ ":class:`nn.InputChoice <nni.retiarii.nn.pytorch.ValueChoice>`"
#~ " takes a list of candidate values,"
#~ " one will be chosen to take "
#~ "effect for each sampled model."
#~ msgstr ""

#~ msgid ""
#~ "Retiarii supports many :doc:`exploration "
#~ "strategies </nas/exploration_strategy>`."
#~ msgstr ""

#~ msgid ""
#~ "Retiarii has provided :doc:`built-in "
#~ "model evaluators </nas/evaluator>`, but to "
#~ "start with, it is recommended to "
#~ "use :class:`FunctionalEvaluator "
#~ "<nni.retiarii.evaluator.FunctionalEvaluator>`, that is,"
#~ " to wrap your own training and "
#~ "evaluation code with one single "
#~ "function. This function should receive "
#~ "one single model class and uses "
#~ ":func:`nni.report_final_result` to report the "
#~ "final score of this model."
#~ msgstr ""

#~ msgid ""
#~ "It is recommended that the "
#~ "``evaluate_model`` here accepts no additional"
#~ " arguments other than ``model_cls``. "
#~ "However, in the :doc:`advanced tutorial "
#~ "</nas/evaluator>`, we will show how to"
#~ " use additional arguments in case you"
#~ " actually need those. In future, we"
#~ " will support mutation on the "
#~ "arguments of evaluators, which is "
#~ "commonly called \"Hyper-parmeter tuning\"."
#~ msgstr ""

#~ msgid ""
#~ "The following configurations are useful "
#~ "to control how many trials to run"
#~ " at most / at the same time."
#~ msgstr ""

#~ msgid ""
#~ "Remember to set the following config "
#~ "if you want to GPU. ``use_active_gpu``"
#~ " should be set true if you wish"
#~ " to use an occupied GPU (possibly "
#~ "running a GUI)."
#~ msgstr ""

#~ msgid ""
#~ "Users can also run Retiarii Experiment"
#~ " with :doc:`different training services "
#~ "</experiment/training_service/overview>` besides ``local``"
#~ " training service."
#~ msgstr ""

#~ msgid ""
#~ "The output is ``json`` object which "
#~ "records the mutation actions of the "
#~ "top model. If users want to output"
#~ " source code of the top model, "
#~ "they can use :ref:`graph-based execution"
#~ " engine <graph-based-execution-engine>` "
#~ "for the experiment, by simply adding "
#~ "the following two lines."
#~ msgstr ""

#~ msgid "**Total running time of the script:** ( 2 minutes  4.499 seconds)"
#~ msgstr ""

#~ msgid ""
#~ ":ref:`Go to the end "
#~ "<sphx_glr_download_tutorials_hello_nas.py>` to download"
#~ " the full example code"
#~ msgstr ""

#~ msgid "Hello, NAS!"
#~ msgstr ""

#~ msgid ""
#~ "This is the 101 tutorial of Neural"
#~ " Architecture Search (NAS) on NNI. In"
#~ " this tutorial, we will search for"
#~ " a neural architecture on MNIST "
#~ "dataset with the help of NAS "
#~ "framework of NNI, i.e., *Retiarii*. We"
#~ " use multi-trial NAS as an "
#~ "example to show how to construct "
#~ "and explore a model space."
#~ msgstr ""

#~ msgid ""
#~ "There are mainly three crucial "
#~ "components for a neural architecture "
#~ "search task, namely,"
#~ msgstr ""

#~ msgid "Model search space that defines a set of models to explore."
#~ msgstr ""

#~ msgid "A proper strategy as the method to explore this model space."
#~ msgstr ""

#~ msgid ""
#~ "A model evaluator that reports the "
#~ "performance of every model in the "
#~ "space."
#~ msgstr ""

#~ msgid ""
#~ "Currently, PyTorch is the only supported"
#~ " framework by Retiarii, and we have"
#~ " only tested **PyTorch 1.9 to 1.13**."
#~ " This tutorial assumes PyTorch context "
#~ "but it should also apply to other"
#~ " frameworks, which is in our future"
#~ " plan."
#~ msgstr ""

#~ msgid "Define your Model Space"
#~ msgstr ""

#~ msgid ""
#~ "Model space is defined by users to"
#~ " express a set of models that "
#~ "users want to explore, which contains"
#~ " potentially good-performing models. In "
#~ "this framework, a model space is "
#~ "defined with two parts: a base "
#~ "model and possible mutations on the "
#~ "base model."
#~ msgstr ""

#~ msgid "Define Base Model"
#~ msgstr ""

#~ msgid ""
#~ "Defining a base model is almost "
#~ "the same as defining a PyTorch (or"
#~ " TensorFlow) model."
#~ msgstr ""

#~ msgid "Below is a very simple example of defining a base model."
#~ msgstr ""

#~ msgid "Define Model Variations"
#~ msgstr ""

#~ msgid ""
#~ "A base model is only one concrete"
#~ " model not a model space. We "
#~ "provide :doc:`API and Primitives "
#~ "</nas/construct_space>` for users to express"
#~ " how the base model can be "
#~ "mutated. That is, to build a model"
#~ " space which includes many models."
#~ msgstr ""

#~ msgid "Based on the above base model, we can define a model space as below."
#~ msgstr ""

#~ msgid "This results in the following code:"
#~ msgstr ""

#~ msgid ""
#~ "This example uses two mutation APIs, "
#~ ":class:`nn.LayerChoice <nni.nas.nn.pytorch.LayerChoice>` "
#~ "and :func:`nni.choice`. :class:`nn.LayerChoice "
#~ "<nni.nas.nn.pytorch.LayerChoice>` takes a list "
#~ "of candidate modules (two in this "
#~ "example), one will be chosen for "
#~ "each sampled model. It can be used"
#~ " like normal PyTorch module. "
#~ ":func:`nni.choice` is used as parameter "
#~ "of `MutableDropout`, which then takes "
#~ "the result as dropout rate."
#~ msgstr ""

#~ msgid ""
#~ "More detailed API description and usage"
#~ " can be found :doc:`here "
#~ "</nas/construct_space>`."
#~ msgstr ""

#~ msgid ""
#~ "We are actively enriching the mutation"
#~ " APIs, to facilitate easy construction "
#~ "of model space. If the currently "
#~ "supported mutation APIs cannot express "
#~ "your model space, please refer to "
#~ ":doc:`this doc </nas/mutator>` for customizing"
#~ " mutators."
#~ msgstr ""

#~ msgid "Explore the Defined Model Space"
#~ msgstr ""

#~ msgid ""
#~ "There are basically two exploration "
#~ "approaches: (1) search by evaluating "
#~ "each sampled model independently, which "
#~ "is the search approach in :ref"
#~ ":`multi-trial NAS <multi-trial-nas>` "
#~ "and (2) one-shot weight-sharing "
#~ "based search, which is used in "
#~ "one-shot NAS. We demonstrate the "
#~ "first approach in this tutorial. Users"
#~ " can refer to :ref:`here <one-"
#~ "shot-nas>` for the second approach."
#~ msgstr ""

#~ msgid ""
#~ "First, users need to pick a proper"
#~ " exploration strategy to explore the "
#~ "defined model space. Second, users need"
#~ " to pick or customize a model "
#~ "evaluator to evaluate the performance of"
#~ " each explored model."
#~ msgstr ""

#~ msgid "Pick an exploration strategy"
#~ msgstr ""

#~ msgid ""
#~ "NNI NAS supports many :doc:`exploration "
#~ "strategies </nas/exploration_strategy>`."
#~ msgstr ""

#~ msgid "Simply choosing (i.e., instantiate) an exploration strategy as below."
#~ msgstr ""

#~ msgid "Pick or customize a model evaluator"
#~ msgstr ""

#~ msgid ""
#~ "In the exploration process, the "
#~ "exploration strategy repeatedly generates new"
#~ " models. A model evaluator is for "
#~ "training and validating each generated "
#~ "model to obtain the model's performance."
#~ " The performance is sent to the "
#~ "exploration strategy for the strategy to"
#~ " generate better models."
#~ msgstr ""

#~ msgid ""
#~ "NNI NAS has provided :doc:`built-in "
#~ "model evaluators </nas/evaluator>`, but to "
#~ "start with, it is recommended to "
#~ "use :class:`FunctionalEvaluator "
#~ "<nni.nas.evaluator.FunctionalEvaluator>`, that is, "
#~ "to wrap your own training and "
#~ "evaluation code with one single "
#~ "function. This function should receive "
#~ "one single model class and uses "
#~ ":func:`nni.report_final_result` to report the "
#~ "final score of this model."
#~ msgstr ""

#~ msgid ""
#~ "An example here creates a simple "
#~ "evaluator that runs on MNIST dataset,"
#~ " trains for 2 epochs, and reports "
#~ "its validation accuracy."
#~ msgstr ""

#~ msgid "Create the evaluator"
#~ msgstr ""

#~ msgid ""
#~ "The ``train_epoch`` and ``test_epoch`` here"
#~ " can be any customized function, "
#~ "where users can write their own "
#~ "training recipe."
#~ msgstr ""

#~ msgid ""
#~ "It is recommended that the "
#~ "``evaluate_model`` here accepts no additional"
#~ " arguments other than ``model``. However,"
#~ " in the :doc:`advanced tutorial "
#~ "</nas/evaluator>`, we will show how to"
#~ " use additional arguments in case you"
#~ " actually need those. In future, we"
#~ " will support mutation on the "
#~ "arguments of evaluators, which is "
#~ "commonly called \"Hyper-parameter tuning\"."
#~ msgstr ""

#~ msgid "Launch an Experiment"
#~ msgstr ""

#~ msgid ""
#~ "After all the above are prepared, "
#~ "it is time to start an experiment"
#~ " to do the model search. An "
#~ "example is shown below."
#~ msgstr ""

#~ msgid ""
#~ "Different from HPO experiment, NAS "
#~ "experiment will generate an experiment "
#~ "config automatically. It should work for"
#~ " most cases. For example, when using"
#~ " multi-trial strategies, local training "
#~ "service with concurrency 1 will be "
#~ "used by default. Users can customize "
#~ "the config. For example,"
#~ msgstr ""

#~ msgid ""
#~ "Remember to set the following config "
#~ "if you want to GPU. ``use_active_gpu``"
#~ " should be set true if you wish"
#~ " to use an occupied GPU (possibly "
#~ "running a GUI)::"
#~ msgstr ""

#~ msgid ""
#~ "Launch the experiment. The experiment "
#~ "should take several minutes to finish"
#~ " on a workstation with 2 GPUs."
#~ msgstr ""

#~ msgid ""
#~ "Users can also run NAS Experiment "
#~ "with :doc:`different training services "
#~ "</experiment/training_service/overview>` besides ``local``"
#~ " training service."
#~ msgstr ""

#~ msgid "Visualize the Experiment"
#~ msgstr ""

#~ msgid ""
#~ "Users can visualize their experiment in"
#~ " the same way as visualizing a "
#~ "normal hyper-parameter tuning experiment. "
#~ "For example, open ``localhost:8081`` in "
#~ "your browser, 8081 is the port "
#~ "that you set in ``exp.run``. Please "
#~ "refer to :doc:`here "
#~ "</experiment/web_portal/web_portal>` for details."
#~ msgstr ""

#~ msgid ""
#~ "We support visualizing models with "
#~ "3rd-party visualization engines (like "
#~ "`Netron <https://netron.app/>`__). This can be"
#~ " used by clicking ``Visualization`` in "
#~ "detail panel for each trial. Note "
#~ "that current visualization is based on"
#~ " `onnx <https://onnx.ai/>`__ , thus "
#~ "visualization is not feasible if the "
#~ "model cannot be exported into onnx."
#~ msgstr ""

#~ msgid ""
#~ "Built-in evaluators (e.g., Classification) "
#~ "will automatically export the model into"
#~ " a file. For your own evaluator, "
#~ "you need to save your file into"
#~ " ``$NNI_OUTPUT_DIR/model.onnx`` to make this "
#~ "work. For instance,"
#~ msgstr ""

#~ msgid "Relaunch the experiment, and a button is shown on Web portal."
#~ msgstr ""

#~ msgid "Export Top Models"
#~ msgstr ""

#~ msgid ""
#~ "Users can export top models after "
#~ "the exploration is done using "
#~ "``export_top_models``."
#~ msgstr ""

#~ msgid "**Total running time of the script:** ( 13 minutes  8.330 seconds)"
#~ msgstr ""

#~ msgid ":download:`Download Python source code: hello_nas.py <hello_nas.py>`"
#~ msgstr ""

#~ msgid ""
#~ ":download:`Download Jupyter notebook: "
#~ "hello_nas.ipynb <hello_nas.ipynb>`"
#~ msgstr ""

#~ msgid ""
#~ "Click :ref:`here "
#~ "<sphx_glr_download_tutorials_pruning_quick_start_mnist.py>` to"
#~ " download the full example code"
#~ msgstr ""

#~ msgid "Pruning Quickstart"
#~ msgstr ""

#~ msgid "Here is a three-minute video to get you started with model pruning."
#~ msgstr ""

#~ msgid ""
#~ "Model pruning is a technique to "
#~ "reduce the model size and computation"
#~ " by reducing model weight size or "
#~ "intermediate state size. There are three"
#~ " common practices for pruning a DNN"
#~ " model:"
#~ msgstr ""

#~ msgid ""
#~ "Pre-training a model -> Pruning "
#~ "the model -> Fine-tuning the "
#~ "pruned model"
#~ msgstr ""

#~ msgid ""
#~ "Pruning a model during training (i.e.,"
#~ " pruning aware training) -> Fine-"
#~ "tuning the pruned model"
#~ msgstr ""

#~ msgid "Pruning a model -> Training the pruned model from scratch"
#~ msgstr ""

#~ msgid ""
#~ "NNI supports all of the above "
#~ "pruning practices by working on the "
#~ "key pruning stage. Following this "
#~ "tutorial for a quick look at how"
#~ " to use NNI to prune a model"
#~ " in a common practice."
#~ msgstr ""

#~ msgid "Preparation"
#~ msgstr ""

#~ msgid ""
#~ "In this tutorial, we use a simple"
#~ " model and pre-trained on MNIST "
#~ "dataset. If you are familiar with "
#~ "defining a model and training in "
#~ "pytorch, you can skip directly to "
#~ "`Pruning Model`_."
#~ msgstr ""

#~ msgid "Pruning Model"
#~ msgstr ""

#~ msgid ""
#~ "Using L1NormPruner to prune the model"
#~ " and generate the masks. Usually, a"
#~ " pruner requires original model and "
#~ "``config_list`` as its inputs. Detailed "
#~ "about how to write ``config_list`` "
#~ "please refer :doc:`compression config "
#~ "specification <../compression/compression_config_list>`."
#~ msgstr ""

#~ msgid ""
#~ "The following `config_list` means all "
#~ "layers whose type is `Linear` or "
#~ "`Conv2d` will be pruned, except the "
#~ "layer named `fc3`, because `fc3` is "
#~ "`exclude`. The final sparsity ratio for"
#~ " each layer is 50%. The layer "
#~ "named `fc3` will not be pruned."
#~ msgstr ""

#~ msgid "Pruners usually require `model` and `config_list` as input arguments."
#~ msgstr ""

#~ msgid ""
#~ "Speedup the original model with masks,"
#~ " note that `ModelSpeedup` requires an "
#~ "unwrapped model. The model becomes "
#~ "smaller after speedup, and reaches a "
#~ "higher sparsity ratio because `ModelSpeedup`"
#~ " will propagate the masks across "
#~ "layers."
#~ msgstr ""

#~ msgid "the model will become real smaller after speedup"
#~ msgstr ""

#~ msgid "Fine-tuning Compacted Model"
#~ msgstr ""

#~ msgid ""
#~ "Note that if the model has been"
#~ " sped up, you need to re-"
#~ "initialize a new optimizer for fine-"
#~ "tuning. Because speedup will replace the"
#~ " masked big layers with dense small"
#~ " ones."
#~ msgstr ""

#~ msgid "**Total running time of the script:** ( 1 minutes  0.810 seconds)"
#~ msgstr ""

#~ msgid ""
#~ ":download:`Download Python source code: "
#~ "pruning_quick_start_mnist.py <pruning_quick_start_mnist.py>`"
#~ msgstr ""

#~ msgid ""
#~ ":download:`Download Jupyter notebook: "
#~ "pruning_quick_start_mnist.ipynb "
#~ "<pruning_quick_start_mnist.ipynb>`"
#~ msgstr ""

