{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Speed Up Quantized Model with TensorRT\n\nQuantization algorithms quantize a deep learning model usually in a simulated way. That is, to simulate the effect of low-bit computation with float32 operators, the tensors are quantized to the targeted bit number and dequantized back to float32. Such a quantized model does not have any latency reduction. Thus, there should be a speedup stage to make the quantized model really accelerated with low-bit operators. \nThis tutorial demonstrates how to accelerate a quantized model with [TensorRT](https://developer.nvidia.com/tensorrt) as the inference engine in NNI. More inference engines will be supported in future release.\n\nThe process of speeding up a quantized model in NNI is that 1) the model with quantized weights and configuration is converted into onnx format, 2) the onnx model is fed into TensorRT to generate an inference engine. The engine is used for low latency model inference.\n\nThere are two modes of the speedup: 1) leveraging post-training quantization of TensorRT, 2) using TensorRT as a pure acceleration backend. The two modes will be explained in the usage section below.\n\n## Prerequisite\nWhen using TensorRT to speed up a quantized model, you are highly recommended to use the PyTorch docker image provided by NVIDIA.\nUsers can refer to [this web page](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch)_ for detailed usage of the docker image.\nThe docker image \"nvcr.io/nvidia/pytorch:22.09-py3\" has been tested for the quantization speedup in NNI.\n\nAn example command to launch the docker container is `nvidia-docker run -it nvcr.io/nvidia/pytorch:22.09-py3`.\nIn the docker image, users should install nni>=3.0, pytorch_lightning, pycuda.\n\n## Usage\n\n### Mode #1: Leveraging post-training quantization of TensorRT\n\nAs TensorRT has supported post-training quantization, directly leveraging this functionality is a natural way to use TensorRT. This mode is called \"with calibration data\". In this mode, the quantization-aware training algorithms (e.g., [QAT](https://nni.readthedocs.io/en/stable/reference/compression/quantizer.html#qat-quantizer), [LSQ](https://nni.readthedocs.io/en/stable/reference/compression/quantizer.html#lsq-quantizer)) only take charge of adjusting model weights to be more quantization friendly, and leave the last-step quantization to the post-training quantization of TensorRT.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare the calibration data with 128 samples\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchvision\nimport torchvision.transforms as transforms\n\n\nskip_exec = True\n\nif not skip_exec:\n\n    def prepare_data_loaders(data_path, batch_size):\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                        std=[0.229, 0.224, 0.225])\n        dataset = torchvision.datasets.ImageNet(\n            data_path, split=\"train\",\n            transform=transforms.Compose([\n                transforms.Resize(256),\n                transforms.CenterCrop(224),\n                transforms.ToTensor(),\n                normalize,\n            ]))\n\n        sampler = torch.utils.data.SequentialSampler(dataset)\n        data_loader = torch.utils.data.DataLoader(\n            dataset, batch_size=batch_size,\n            sampler=sampler)\n        return data_loader\n\n    data_path = '/data'  # replace it with your path of ImageNet dataset\n    data_loader = prepare_data_loaders(data_path, batch_size=128)\n    calib_data = None\n    for image, target in data_loader:\n        calib_data = image.numpy()\n        break\n\n    from nni.compression.quantization_speedup.calibrator import Calibrator\n    # TensorRT processes the calibration data in the batch size of 64\n    calib = Calibrator(calib_data, 'data/calib_cache_file.cache', batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare the float32 model MobileNetV2\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not skip_exec:\n    from nni_assets.compression.mobilenetv2 import MobileNetV2\n    model = MobileNetV2()\n    # a checkpoint of MobileNetV2 can be found here\n    # https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\n    float_model_file = 'mobilenet_pretrained_float.pth'\n    state_dict = torch.load(float_model_file)\n    model.load_state_dict(state_dict)\n    model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Speed up the model with TensorRT\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not skip_exec:\n    from nni.compression.quantization_speedup import ModelSpeedupTensorRT\n    # input shape is used for converting to onnx\n    engine = ModelSpeedupTensorRT(model, input_shape=(64, 3, 224, 224))\n    engine.compress_with_calibrator(calib)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the accuracy of the accelerated model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not skip_exec:\n    from nni_assets.compression.mobilenetv2 import AverageMeter, accuracy\n    import time\n\n    def test_accelerated_model(engine, data_loader, neval_batches):\n        top1 = AverageMeter('Acc@1', ':6.2f')\n        top5 = AverageMeter('Acc@5', ':6.2f')\n        cnt = 0\n        total_time = 0\n        for image, target in data_loader:\n            start_time = time.time()\n            output, time_span = engine.inference(image)\n            infer_time = time.time() - start_time\n            print('time: ', time_span, infer_time)\n            total_time += time_span\n\n            start_time = time.time()\n            output = output.view(-1, 1000)\n            cnt += 1\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n            top1.update(acc1[0], image.size(0))\n            top5.update(acc5[0], image.size(0))\n            rest_time = time.time() - start_time\n            print('rest time: ', rest_time)\n            if cnt >= neval_batches:\n                break\n        print('inference time: ', total_time / neval_batches)\n        return top1, top5\n\n    data_loader = prepare_data_loaders(data_path, batch_size=64)\n    top1, top5 = test_accelerated_model(engine, data_loader, neval_batches=32)\n    print('Accuracy of mode #1: ', top1, top5)\n\n\"\"\"\n\nMode #2: Using TensorRT as a pure acceleration backend\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn this mode, the post-training quantization within TensorRT is not used, instead, the quantization bit-width and the range of tensor values are fed into TensorRT for speedup (i.e., with `trt.BuilderFlag.PREFER_PRECISION_CONSTRAINTS` configured).\n\n\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "re-instantiate the MobileNetV2 model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not skip_exec:\n    model = MobileNetV2()\n    state_dict = torch.load(float_model_file)\n    model.load_state_dict(state_dict)\n    model.eval()\n    device = torch.device('cuda')\n    model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare Evaluator for PtqQuantizer\nPtqQuantizer uses eval_for_calibration to collect calibration data \nin the current setting, it handles 128 samples\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not skip_exec:\n    from nni_assets.compression.mobilenetv2 import evaluate\n    from nni.compression.utils import TorchEvaluator\n    data_loader = prepare_data_loaders(data_path, batch_size=128)\n\n    def eval_for_calibration(model):\n        evaluate(model, data_loader, neval_batches=1, device=device)\n\n    dummy_input = torch.Tensor(64, 3, 224, 224).to(device)\n    predict_func = TorchEvaluator(predicting_func=eval_for_calibration, dummy_input=dummy_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use PtqQuantizer to quantize the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nni.compression.quantization import PtqQuantizer\nif not skip_exec:\n    config_list = [{\n        'quant_types': ['input', 'weight', 'output'],\n        'quant_bits': {'input': 8, 'weight': 8, 'output': 8},\n        'quant_dtype': 'int',\n        'quant_scheme': 'per_tensor_symmetric',\n        'op_types': ['default']\n    }]\n    quantizer = PtqQuantizer(model, config_list, predict_func, True)\n    quantizer.compress()\n    calibration_config = quantizer.export_model()\n    print('quant result config: ', calibration_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Speed up the quantized model following the generated calibration_config\nre-instantiate the MobileNetV2 model, because the calibration config is obtained\nafter applying bn folding. bn folding changes the models structure and weights.\nAs TensorRT does bn folding by itself, we should input an original model to it.\nFor simplicity, we re-instantiate a new model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not skip_exec:\n    model = MobileNetV2()\n    state_dict = torch.load(float_model_file)\n    model.load_state_dict(state_dict)\n    model.eval()\n\n    engine = ModelSpeedupTensorRT(model, input_shape=(64, 3, 224, 224), config=calibration_config)\n    engine.compress()\n    data_loader = prepare_data_loaders(data_path, batch_size=64)\n    top1, top5 = test_accelerated_model(engine, data_loader, neval_batches=32)\n    print('Accuracy of mode #2: ', top1, top5)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}