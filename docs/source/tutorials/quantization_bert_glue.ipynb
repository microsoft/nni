{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Quantize BERT on Task GLUE\n\nHere we show an effective transformer simulated quantization process that NNI team has tried, and users can use NNI to discover better process.\n\nWe use the BERT model and the trainer pipeline in the Transformers to do some experiments.\nThe entire quantization process can be divided into the following steps:\n\n1. Use the BERT-base-uncased model and the trainer pipeline in the transformers to fine-tune the model on the downstream task GLUE.\n   From our experience, the final performance of quantization on the finetuned model is\n   better than quantization directly on the pre-trained model.\n2. Use a specific quantizer to quantize the finetuned model on the GLUE.\n   Here we apply QAT, LSQ and PTQ quantizers to quantize the BERT model so that \n   we can compare their performance of the quantized BERT on the GLUE.\n   Among them, LSQ and QAT are quantization aware training methods, and PTQ is a post-training quantization method.\n\nDuring the process of quantizing BERT:\n\n* we use the BERT model and the trainer pipeline in the Transformers to do some experiments.\n* we use int8 to quantize Linear layers in the BERT.encoder.\n\n## Experiment\n\n### Preparation\n\nThis section is mainly for fine-tuning model on the downstream task GLUE.\nIf you are familiar with how to finetune BERT on GLUE dataset, you can skip this section.\n\n1. Load the tokenizer and BERT model from Huggingface transformers.\n2. Create a trainer instance to fine-tune the BERT model.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Please set ``dev_mode`` to ``False`` to run this tutorial. Here ``dev_mode`` is ``True`` by default is for generating documents.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\nimport argparse\n\nimport numpy as np\n\nimport torch\nfrom torch.utils.data import ConcatDataset\n\nimport nni\n\nfrom datasets import load_dataset, load_metric\nfrom transformers import BertTokenizerFast, DataCollatorWithPadding, BertForSequenceClassification, EvalPrediction\nfrom transformers.trainer import Trainer\nfrom transformers.training_args import TrainingArguments\n\n\ntask_name = 'qnli'\nfinetune_lr = 4e-5\nquant_lr = 1e-5\nquant_method = 'lsq'\ndev_mode = True\n\nif dev_mode:\n    quant_max_epochs = 1\n    finetune_max_epochs = 1\nelse:\n    quant_max_epochs = 10\n    finetune_max_epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the pre-trained model from the transformers\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def build_model(pretrained_model_name_or_path: str, task_name: str):\n    is_regression = task_name == 'stsb'\n    num_labels = 1 if is_regression else (3 if task_name == 'mnli' else 2)\n    model = BertForSequenceClassification.from_pretrained(pretrained_model_name_or_path, num_labels=num_labels)\n    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create datasets on the specific task GLUE\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def prepare_datasets(task_name: str, tokenizer: BertTokenizerFast, cache_dir: str):\n    task_to_keys = {\n        'cola': ('sentence', None),\n        'mnli': ('premise', 'hypothesis'),\n        'mrpc': ('sentence1', 'sentence2'),\n        'qnli': ('question', 'sentence'),\n        'qqp': ('question1', 'question2'),\n        'rte': ('sentence1', 'sentence2'),\n        'sst2': ('sentence', None),\n        'stsb': ('sentence1', 'sentence2'),\n        'wnli': ('sentence1', 'sentence2'),\n    }\n    sentence1_key, sentence2_key = task_to_keys[task_name]\n\n    # used to preprocess the raw data\n    def preprocess_function(examples):\n        # Tokenize the texts\n        args = (\n            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n        )\n        result = tokenizer(*args, padding=False, max_length=128, truncation=True)\n\n        if 'label' in examples:\n            # In all cases, rename the column to labels because the model will expect that.\n            result['labels'] = examples['label']\n        return result\n\n    raw_datasets = load_dataset('glue', task_name, cache_dir=cache_dir)\n    for key in list(raw_datasets.keys()):\n        if 'test' in key:\n            raw_datasets.pop(key)\n\n    processed_datasets = raw_datasets.map(preprocess_function, batched=True,\n                                          remove_columns=raw_datasets['train'].column_names)\n\n    train_dataset = processed_datasets['train']\n    if task_name == 'mnli':\n        validation_datasets = {\n            'validation_matched': processed_datasets['validation_matched'],\n            'validation_mismatched': processed_datasets['validation_mismatched']\n        }\n    else:\n        validation_datasets = {\n            'validation': processed_datasets['validation']\n        }\n\n    return train_dataset, validation_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a trainer instance\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Please set ``is_quant`` to ``False`` to fine-tune the BERT model and set ``is_quant`` to ``True``\n    , when you need to create a traced trainer and use ``quant_lr`` for model quantization.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def prepare_traced_trainer(model, load_best_model_at_end=False, is_quant=False):\n    is_regression = task_name == 'stsb'\n    metric = load_metric('glue', task_name)\n\n    def compute_metrics(p: EvalPrediction):\n        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n        preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n        result = metric.compute(predictions=preds, references=p.label_ids)\n        result['default'] = result.get('f1', result.get('accuracy', 0.))\n        return result\n\n    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n    train_dataset, validation_datasets = prepare_datasets(task_name, tokenizer, '')\n    merged_validation_dataset = ConcatDataset([d for d in validation_datasets.values()]) # type: ignore\n    data_collator = DataCollatorWithPadding(tokenizer)\n    training_args = TrainingArguments(output_dir='./output/trainer',\n                                      do_train=True,\n                                      do_eval=True,\n                                      evaluation_strategy='steps',\n                                      per_device_train_batch_size=128, #128,\n                                      per_device_eval_batch_size=128, #128,\n                                      num_train_epochs=finetune_max_epochs,\n                                      dataloader_num_workers=12,\n                                      save_strategy='steps',\n                                      save_total_limit=1,\n                                      metric_for_best_model='default',\n                                      greater_is_better=True,\n                                      seed=1024,\n                                      load_best_model_at_end=load_best_model_at_end,)\n    if is_quant:\n        training_args.learning_rate = quant_lr\n    else:\n        training_args.learning_rate = finetune_lr\n    trainer = nni.trace(Trainer)(model=model,\n                        args=training_args,\n                        data_collator=data_collator,\n                        train_dataset=train_dataset,\n                        eval_dataset=merged_validation_dataset,\n                        tokenizer=tokenizer,\n                        compute_metrics=compute_metrics,\n                        )\n\n    return trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the finetuned model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def build_finetuning_model(state_dict_path: str, is_quant=False):\n    model = build_model('bert-base-uncased', task_name)\n    if Path(state_dict_path).exists():\n        model.load_state_dict(torch.load(state_dict_path))\n    else:\n        trainer = prepare_traced_trainer(model, True, is_quant)\n        trainer.train()\n        torch.save(model.state_dict(), state_dict_path)\n    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quantization\nAfter fine-tuning the BERT model on the specific task GLUE, a specific quantizer instsance can be created\nto process quantization aware training or post-training quantization with BERT on the GLUE.\n\nThe entire quantization process can be devided into the following steps:\n\n1. Call ``build_finetuning_model`` to load or fine-tune the BERT model on a specific task GLUE\n2. Call ``prepare_traced_trainer`` and set ``is_quant`` to ``True`` to create a traced trainer instance for model quantization\n3. Call the TransformersEvaluator to create an evaluator instance\n4. Use the defined config_list and evaluator to create a quantizer instance\n5. Define ``max_steps`` or ``max_epochs``. Note that ``max_steps`` and ``max_epochs`` cannot be None at the same time.\n6. Call ``quantizer.compress(max_steps, max_epochs)`` to execute the simulated quantization process\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import nni\nfrom nni.compression.quantization import QATQuantizer, LsqQuantizer, PtqQuantizer\nfrom nni.compression.utils import TransformersEvaluator\n\ndef fake_quantize():\n    config_list = [{\n        'op_types': ['Linear'],\n        'op_names_re': ['bert.encoder.layer.{}'.format(i) for i in range(12)],\n        'target_names': ['weight', '_output_'],\n        'quant_dtype': 'int8',\n        'quant_scheme': 'affine',\n        'granularity': 'default',\n    }]\n\n    # create a finetune model\n    Path('./output/bert_finetuned/').mkdir(parents=True, exist_ok=True)\n    model: torch.nn.Module = build_finetuning_model(f'./output/bert_finetuned/{task_name}.bin', is_quant=False)  # type: ignore\n    traced_trainer = prepare_traced_trainer(model, is_quant=False)\n    evaluator = TransformersEvaluator(traced_trainer)\n    if quant_method == 'lsq':\n        quantizer = LsqQuantizer(model, config_list, evaluator)\n        model, calibration_config = quantizer.compress(max_steps=None, max_epochs=quant_max_epochs)\n    elif quant_method == 'qat':\n        quantizer = QATQuantizer(model, config_list, evaluator, 1000)\n        model, calibration_config = quantizer.compress(max_steps=None, max_epochs=quant_max_epochs)\n    elif quant_method == 'ptq':\n        quantizer = PtqQuantizer(model, config_list, evaluator)\n        model, calibration_config = quantizer.compress(max_steps=1, max_epochs=None)\n    else:\n        raise ValueError(f\"quantization method {quant_method} is not supported\")\n    print(calibration_config)\n    # evaluate the performance of the fake quantize model\n    quantizer.evaluator.bind_model(model, quantizer._get_param_names_map())\n    print(quantizer.evaluator.evaluate())\n\ndef evaluate():\n    model = build_finetuning_model(f'./output/bert_finetuned/{task_name}.bin', is_quant=False)\n    trainer = prepare_traced_trainer(model, is_quant=False)\n    metrics = trainer.evaluate()\n    print(f\"Evaluate metrics={metrics}\")\n\n\nskip_exec = True\nif not skip_exec:\n    fake_quantize()\n    evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Result\nWe experimented with PTQ, LSQ, and QAT algorithms on the MNLI, QNLI, QQP and  MRPC datasets respectively on an A100, and the experimental results are as follows.\n\n.. list-table:: Quantize Bert-base-uncased on MNLI, QNLI, MRPC and QQP\n    :header-rows: 1\n    :widths: auto\n\n    * - Quant Method\n      - MNLI\n      - QNLI\n      - MRPC\n      - QQP\n    * - Metrics\n      - ACC\n      - ACC\n      - F1\n      - F1\n    * - Baseline\n      - 85.04%\n      - 91.67%\n      - 87.69%\n      - 88.42%\n    * - LSQ\n      - 84.34%\n      - 91.69%\n      - 89.9%\n      - 88.16%\n    * - QAT\n      - 83.68%\n      - 90.52%\n      - 89.16%\n      - 87.62%\n    * - PTQ\n      - 76.37%\n      - 67.67%\n      - 74.79%\n      - 84.42%\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}