# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, Microsoft
# This file is distributed under the same license as the NNI package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: NNI \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-01-29 17:43+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../Compression/AutoCompression.rst:2
msgid "Auto Compression with NNI Experiment"
msgstr ""

#: ../../Compression/AutoCompression.rst:4
msgid ""
"If you want to compress your model, but don't know what compression "
"algorithm to choose, or don't know what sparsity is suitable for your "
"model, or just want to try more possibilities, auto compression may help "
"you. Users can choose different compression algorithms and define the "
"algorithms' search space, then auto compression will launch an NNI "
"experiment and try different compression algorithms with varying sparsity"
" automatically. Of course, in addition to the sparsity rate, users can "
"also introduce other related parameters into the search space. If you "
"don't know what is search space or how to write search space, `this "
"<./Tutorial/SearchSpaceSpec.rst>`__ is for your reference. Auto "
"compression using experience is similar to the NNI experiment in python. "
"The main differences are as follows:"
msgstr ""

#: ../../Compression/AutoCompression.rst:11
msgid "Use a generator to help generate search space object."
msgstr ""

#: ../../Compression/AutoCompression.rst:12
msgid ""
"Need to provide the model to be compressed, and the model should have "
"already been pre-trained."
msgstr ""

#: ../../Compression/AutoCompression.rst:13
msgid ""
"No need to set ``trial_command``, additional need to set "
"``auto_compress_module`` as ``AutoCompressionExperiment`` input."
msgstr ""

#: ../../Compression/AutoCompression.rst:16
msgid ""
"Auto compression only supports TPE Tuner, Random Search Tuner, Anneal "
"Tuner, Evolution Tuner right now."
msgstr ""

#: ../../Compression/AutoCompression.rst:19
msgid "Generate search space"
msgstr ""

#: ../../Compression/AutoCompression.rst:21
msgid ""
"Due to the extensive use of nested search space, we recommend a using "
"generator to configure search space. The following is an example. Using "
"``add_config()`` add subconfig, then ``dumps()`` search space dict."
msgstr ""

#: ../../Compression/AutoCompression.rst:50
msgid "Now we support the following pruners and quantizers:"
msgstr ""

#: ../../Compression/AutoCompression.rst:73
msgid "Provide user model for compression"
msgstr ""

#: ../../Compression/AutoCompression.rst:75
msgid ""
"Users need to inherit ``AbstractAutoCompressionModule`` and override the "
"abstract class function."
msgstr ""

#: ../../Compression/AutoCompression.rst:92
msgid ""
"Users need to implement at least ``model()`` and ``evaluator()``. If you "
"use iterative pruner, you need to additional implement "
"``optimizer_factory()``, ``criterion()`` and ``sparsifying_trainer()``. "
"If you want to finetune the model after compression, you need to "
"implement ``optimizer_factory()``, ``criterion()``, "
"``post_compress_finetuning_trainer()`` and "
"``post_compress_finetuning_epochs()``. The ``optimizer_factory()`` should"
" return a factory function, the input is an iterable variable, i.e. your "
"``model.parameters()``, and the output is an optimizer instance. The two "
"kinds of ``trainer()`` should return a trainer with input ``model, "
"optimizer, criterion, current_epoch``. The full abstract interface refers"
" to :githublink:`interface.py "
"<nni/algorithms/compression/pytorch/auto_compress/interface.py>`. An "
"example of ``AutoCompressionModule`` implementation refers to "
":githublink:`auto_compress_module.py "
"<examples/model_compress/auto_compress/torch/auto_compress_module.py>`."
msgstr ""

#: ../../Compression/AutoCompression.rst:101
msgid "Launch NNI experiment"
msgstr ""

#: ../../Compression/AutoCompression.rst:103
msgid ""
"Similar to launch from python, the difference is no need to set "
"``trial_command`` and put the user-provided ``AutoCompressionModule`` as "
"``AutoCompressionExperiment`` input."
msgstr ""

#: ../../Compression/CompressionReference.rst:2
msgid "Model Compression API Reference"
msgstr ""

#: ../../Compression/CompressionReference.rst:4
#: ../../Compression/CompressionUtils.rst:4
#: ../../Compression/CustomizeCompressor.rst:4
#: ../../Compression/Framework.rst:4 ../../Compression/Overview.rst:4
#: ../../Compression/Tutorial.rst:4
msgid "Contents"
msgstr ""

#: ../../Compression/CompressionReference.rst:7
msgid "Compressors"
msgstr ""

#: ../../Compression/CompressionReference.rst:10
#: ../../Compression/Framework.rst:17
msgid "Compressor"
msgstr ""

#: nni.compression.pytorch.compressor.Compressor:1 of
msgid "Abstract base PyTorch compressor"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner.compress:1
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.IterativePruner.compress:1
#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.compress:1
#: nni.compression.pytorch.compressor.Compressor.compress:1
#: nni.compression.pytorch.compressor.Pruner.compress:1 of
msgid "Compress the model with algorithm implemented by subclass."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner.compress:3
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.IterativePruner.compress:3
#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.compress:3
#: nni.compression.pytorch.compressor.Compressor.compress:3
#: nni.compression.pytorch.compressor.Pruner.compress:3 of
msgid ""
"The model will be instrumented and user should never edit it after "
"calling this method. `self.modules_to_compress` records all the to-be-"
"compressed layers"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.compress
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ADMMPruner.compress
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner.calc_mask
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner.compress
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner.compute_target_sparsity
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.IterativePruner.compress
#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner.calc_mask
#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner.get_prune_iterations
#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner.compress
#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.create_cfg
#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.normalize
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner.compress
#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.get_channel_sum
#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.get_mask
#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.compress
#: nni.algorithms.compression.pytorch.pruning.weight_masker.WeightMasker.calc_mask
#: nni.algorithms.compression.pytorch.quantization.bnn_quantizer.BNNQuantizer.export_model
#: nni.algorithms.compression.pytorch.quantization.dorefa_quantizer.DoReFaQuantizer.export_model
#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.export_model
#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer.export_model
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.export_model
#: nni.compression.pytorch.compressor.Compressor.compress
#: nni.compression.pytorch.compressor.Compressor.get_modules_to_compress
#: nni.compression.pytorch.compressor.Compressor.get_modules_wrapper
#: nni.compression.pytorch.compressor.Compressor.select_config
#: nni.compression.pytorch.compressor.Pruner.compress
#: nni.compression.pytorch.compressor.Quantizer.export_model
#: nni.compression.pytorch.compressor.Quantizer.fold_bn
#: nni.compression.pytorch.quantization_speedup.backend.BaseModelSpeedup.inference
#: nni.compression.pytorch.quantization_speedup.calibrator.Calibrator.get_batch
#: nni.compression.pytorch.quantization_speedup.calibrator.Calibrator.read_calibration_cache
#: nni.compression.pytorch.utils.counter.count_flops_params
#: nni.compression.pytorch.utils.sensitivity_analysis.SensitivityAnalysis.analysis
#: nni.compression.pytorch.utils.shape_dependency.ChannelDependency.dependency_sets
#: nni.compression.pytorch.utils.shape_dependency.GroupDependency.build_dependency
#: of
msgid "Returns"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.compress:3
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ADMMPruner.compress:3
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner.compress:6
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.IterativePruner.compress:6
#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner.compress:3
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner.compress:3
#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.compress:6
#: nni.compression.pytorch.compressor.Compressor.compress:6
#: nni.compression.pytorch.compressor.Pruner.compress:6 of
msgid "model with specified modules compressed."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.compress
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ADMMPruner.compress
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner.calc_mask
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner.compress
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner.compute_target_sparsity
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.IterativePruner.compress
#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner.calc_mask
#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner.get_prune_iterations
#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner.compress
#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.create_cfg
#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.normalize
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner.compress
#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.get_channel_sum
#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.get_mask
#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.compress
#: nni.algorithms.compression.pytorch.pruning.weight_masker.WeightMasker.calc_mask
#: nni.algorithms.compression.pytorch.quantization.bnn_quantizer.BNNQuantizer.export_model
#: nni.algorithms.compression.pytorch.quantization.dorefa_quantizer.DoReFaQuantizer.export_model
#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.export_model
#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer.export_model
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.export_model
#: nni.compression.pytorch.compressor.Compressor.compress
#: nni.compression.pytorch.compressor.Compressor.get_modules_to_compress
#: nni.compression.pytorch.compressor.Compressor.get_modules_wrapper
#: nni.compression.pytorch.compressor.Compressor.select_config
#: nni.compression.pytorch.compressor.Pruner.compress
#: nni.compression.pytorch.compressor.Quantizer.export_model
#: nni.compression.pytorch.compressor.Quantizer.fold_bn
#: nni.compression.pytorch.quantization_speedup.calibrator.Calibrator.get_batch
#: nni.compression.pytorch.quantization_speedup.calibrator.Calibrator.read_calibration_cache
#: nni.compression.pytorch.utils.counter.count_flops_params
#: nni.compression.pytorch.utils.sensitivity_analysis.SensitivityAnalysis.analysis
#: nni.compression.pytorch.utils.shape_dependency.ChannelDependency.dependency_sets
#: nni.compression.pytorch.utils.shape_dependency.GroupDependency.build_dependency
#: of
msgid "Return type"
msgstr ""

#: nni.compression.pytorch.compressor.Compressor.get_modules_to_compress:1 of
msgid "To obtain all the to-be-compressed modules."
msgstr ""

#: nni.compression.pytorch.compressor.Compressor.get_modules_to_compress:3 of
msgid ""
"a list of the layers, each of which is a tuple (`layer`, `config`), "
"`layer` is `LayerInfo`, `config` is a `dict`"
msgstr ""

#: nni.compression.pytorch.compressor.Compressor.get_modules_wrapper:1 of
msgid "To obtain all the wrapped modules."
msgstr ""

#: nni.compression.pytorch.compressor.Compressor.get_modules_wrapper:3 of
msgid "a list of the wrapped modules"
msgstr ""

#: nni.compression.pytorch.compressor.Compressor.reset:1 of
msgid "reset model state dict and model wrapper"
msgstr ""

#: nni.compression.pytorch.compressor.Compressor.select_config:1 of
msgid "Find the configuration for `layer` by parsing `self.config_list`"
msgstr ""

#: ../../Compression/Pruner.rst:98 ../../Compression/Pruner.rst:203
#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner
#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner
#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.calc_mask
#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.export_model
#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.validate_config
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ADMMPruner
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ADMMPruner.validate_config
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner.validate_config
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationAPoZRankFilterPruner
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationMeanRankFilterPruner
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.SlimPruner
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.TaylorFOWeightFilterPruner
#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner
#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner.calc_mask
#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner.validate_config
#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner
#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner.calc_mask
#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner.validate_config
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.FPGMPruner
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.L1FilterPruner
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.L2FilterPruner
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.LevelPruner
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.OneshotPruner.validate_config
#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner
#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.calc_mask
#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.compress
#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.create_cfg
#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.normalize
#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.validate_config
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner.calc_mask
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner.validate_config
#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker
#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.calc_mask
#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.get_mask
#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner
#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.calc_mask
#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.validate_config
#: nni.algorithms.compression.pytorch.quantization.bnn_quantizer.BNNQuantizer.export_model
#: nni.algorithms.compression.pytorch.quantization.bnn_quantizer.BNNQuantizer.validate_config
#: nni.algorithms.compression.pytorch.quantization.dorefa_quantizer.DoReFaQuantizer.export_model
#: nni.algorithms.compression.pytorch.quantization.dorefa_quantizer.DoReFaQuantizer.validate_config
#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.export_model
#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.load_calibration_config
#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer.export_model
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.export_model
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.load_calibration_config
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.validate_config
#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner
#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.FPGMPruner
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L1NormPruner
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L2NormPruner
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.LevelPruner
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.AGPPruner
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LinearPruner
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LotteryTicketPruner
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.SimulatedAnnealingPruner
#: nni.algorithms.compression.v2.pytorch.pruning.movement_pruner.MovementPruner
#: nni.compression.pytorch.compressor.Compressor.select_config
#: nni.compression.pytorch.compressor.Compressor.set_wrappers_attribute
#: nni.compression.pytorch.compressor.Compressor.update_epoch
#: nni.compression.pytorch.compressor.Pruner.calc_mask
#: nni.compression.pytorch.compressor.Pruner.export_model
#: nni.compression.pytorch.compressor.Pruner.get_pruned_weights
#: nni.compression.pytorch.compressor.Pruner.load_model_state_dict
#: nni.compression.pytorch.compressor.Quantizer.export_model
#: nni.compression.pytorch.compressor.Quantizer.export_model_save
#: nni.compression.pytorch.compressor.Quantizer.find_conv_bn_patterns
#: nni.compression.pytorch.compressor.Quantizer.fold_bn
#: nni.compression.pytorch.compressor.Quantizer.load_calibration_config
#: nni.compression.pytorch.compressor.Quantizer.record_shape
#: nni.compression.pytorch.quantization_speedup.backend.BaseModelSpeedup.inference
#: nni.compression.pytorch.quantization_speedup.calibrator.Calibrator.get_batch
#: nni.compression.pytorch.quantization_speedup.calibrator.Calibrator.write_calibration_cache
#: nni.compression.pytorch.quantization_speedup.integrated_tensorrt.ModelSpeedupTensorRT.export_quantized_model
#: nni.compression.pytorch.quantization_speedup.integrated_tensorrt.ModelSpeedupTensorRT.inference
#: nni.compression.pytorch.quantization_speedup.integrated_tensorrt.ModelSpeedupTensorRT.load_quantized_model
#: nni.compression.pytorch.speedup.compressor.ModelSpeedup
#: nni.compression.pytorch.utils.counter.count_flops_params
#: nni.compression.pytorch.utils.sensitivity_analysis.SensitivityAnalysis.analysis
#: nni.compression.pytorch.utils.sensitivity_analysis.SensitivityAnalysis.export
#: of
msgid "Parameters"
msgstr ""

#: nni.compression.pytorch.compressor.Compressor.select_config:3 of
msgid "one layer"
msgstr ""

#: nni.compression.pytorch.compressor.Compressor.select_config:6 of
msgid ""
"the retrieved configuration for this layer, if None, this layer should "
"not be compressed"
msgstr ""

#: nni.compression.pytorch.compressor.Compressor.set_wrappers_attribute:1 of
msgid ""
"To register attributes used in wrapped module's forward method. If the "
"type of the value is Torch.tensor, then this value is registered as a "
"buffer in wrapper, which will be saved by model.state_dict. Otherwise, "
"this value is just a regular variable in wrapper."
msgstr ""

#: nni.compression.pytorch.compressor.Compressor.set_wrappers_attribute:5 of
msgid "name of the variable"
msgstr ""

#: nni.compression.pytorch.compressor.Compressor.set_wrappers_attribute:7 of
msgid "value of the variable"
msgstr ""

#: nni.compression.pytorch.compressor.Compressor.update_epoch:1 of
msgid ""
"If user want to update model every epoch, user can override this method. "
"This method should be called at the beginning of each epoch"
msgstr ""

#: nni.compression.pytorch.compressor.Compressor.update_epoch:4 of
msgid "the current epoch number"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.SlimPruner.validate_config:1
#: nni.algorithms.compression.pytorch.quantization.native_quantizer.NaiveQuantizer.validate_config:1
#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer.validate_config:1
#: nni.compression.pytorch.compressor.Compressor.validate_config:1 of
msgid ""
"subclass can optionally implement this method to check if config_list if "
"valid"
msgstr ""

#: nni.compression.pytorch.compressor.Pruner:1 of
msgid "Prune to an exact pruning level specification"
msgstr ""

#: nni.compression.pytorch.compressor.Pruner:5 of
msgid ""
"Dictionary for saving masks, `key` should be layer name and `value` "
"should be a tensor which has the same shape with layer's weight"
msgstr ""

#: nni.compression.pytorch.compressor.Pruner of
msgid "type"
msgstr ""

#: nni.compression.pytorch.compressor.Pruner:8 of
msgid "dict"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.calc_mask:1
#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner.calc_mask:1
#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.calc_mask:1
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner.calc_mask:1
#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.calc_mask:1
#: nni.compression.pytorch.compressor.Pruner.calc_mask:1 of
msgid ""
"Pruners should overload this method to provide mask for weight tensors. "
"The mask must have the same shape and type comparing to the weight. It "
"will be applied with `mul()` operation on the weight. This method is "
"effectively hooked to `forward()` method of the model."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.calc_mask:6
#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner.calc_mask:6
#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.calc_mask:6
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner.calc_mask:6
#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.calc_mask:6
#: nni.compression.pytorch.compressor.Pruner.calc_mask:6 of
msgid "calculate mask for `wrapper.module`'s weight"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.export_model:1
#: nni.compression.pytorch.compressor.Pruner.export_model:1 of
msgid "Export pruned model weights, masks and onnx model(optional)"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.export_model:3
#: nni.compression.pytorch.compressor.Pruner.export_model:3 of
msgid "path to save pruned model state_dict"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.export_model:5
#: nni.compression.pytorch.compressor.Pruner.export_model:5 of
msgid "(optional) path to save mask dict"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.export_model:7
#: nni.algorithms.compression.pytorch.quantization.bnn_quantizer.BNNQuantizer.export_model:7
#: nni.algorithms.compression.pytorch.quantization.dorefa_quantizer.DoReFaQuantizer.export_model:7
#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.export_model:7
#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer.export_model:7
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.export_model:7
#: nni.compression.pytorch.compressor.Pruner.export_model:7
#: nni.compression.pytorch.compressor.Quantizer.export_model:7
#: nni.compression.pytorch.compressor.Quantizer.export_model_save:11 of
msgid "(optional) path to save onnx model"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.export_model:9
#: nni.compression.pytorch.compressor.Pruner.export_model:9 of
msgid ""
"input shape to onnx model, used for creating a dummy input tensor for "
"torch.onnx.export if the input has a complex structure (e.g., a tuple), "
"please directly create the input and pass it to dummy_input instead note:"
" this argument is deprecated and will be removed; please use dummy_input "
"instead"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.export_model:14
#: nni.compression.pytorch.compressor.Pruner.export_model:14 of
msgid ""
"device of the model, where to place the dummy input tensor for exporting "
"onnx file; the tensor is placed on cpu if ```device``` is None only "
"useful when both onnx_path and input_shape are passed note: this argument"
" is deprecated and will be removed; please use dummy_input instead"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.export_model:19
#: nni.compression.pytorch.compressor.Pruner.export_model:19 of
msgid ""
"dummy input to the onnx model; used when input_shape is not enough to "
"specify dummy input user should ensure that the dummy_input is on the "
"same device as the model"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.export_model:22
#: nni.compression.pytorch.compressor.Pruner.export_model:22 of
msgid ""
"opset_version parameter for torch.onnx.export; only useful when onnx_path"
" is not None if not passed, torch.onnx.export will use its default "
"opset_version"
msgstr ""

#: nni.compression.pytorch.compressor.Pruner.get_pruned_weights:1 of
msgid "Log the simulated prune sparsity."
msgstr ""

#: nni.compression.pytorch.compressor.Pruner.get_pruned_weights:3 of
msgid "the pruned dim."
msgstr ""

#: nni.compression.pytorch.compressor.Pruner.load_model_state_dict:1 of
msgid "Load the state dict saved from unwrapped model."
msgstr ""

#: nni.compression.pytorch.compressor.Pruner.load_model_state_dict:3 of
msgid "state dict saved from unwrapped model"
msgstr ""

#: nni.compression.pytorch.compressor.Quantizer:1 of
msgid "Base quantizer for pytorch quantizer"
msgstr ""

#: nni.compression.pytorch.compressor.Quantizer.export_model:1 of
msgid "Export quantized model weights and calibration parameters"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.bnn_quantizer.BNNQuantizer.export_model:3
#: nni.algorithms.compression.pytorch.quantization.dorefa_quantizer.DoReFaQuantizer.export_model:3
#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.export_model:3
#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer.export_model:3
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.export_model:3
#: nni.compression.pytorch.compressor.Quantizer.export_model:3 of
msgid "path to save quantized model weight"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.bnn_quantizer.BNNQuantizer.export_model:5
#: nni.algorithms.compression.pytorch.quantization.dorefa_quantizer.DoReFaQuantizer.export_model:5
#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.export_model:5
#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer.export_model:5
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.export_model:5
#: nni.compression.pytorch.compressor.Quantizer.export_model:5
#: nni.compression.pytorch.compressor.Quantizer.export_model_save:9 of
msgid "(optional) path to save quantize parameters after calibration"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.bnn_quantizer.BNNQuantizer.export_model:9
#: nni.algorithms.compression.pytorch.quantization.dorefa_quantizer.DoReFaQuantizer.export_model:9
#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.export_model:9
#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer.export_model:9
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.export_model:9
#: nni.compression.pytorch.compressor.Quantizer.export_model:9
#: nni.compression.pytorch.compressor.Quantizer.export_model_save:13 of
msgid "input shape to onnx model"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.bnn_quantizer.BNNQuantizer.export_model:11
#: nni.algorithms.compression.pytorch.quantization.dorefa_quantizer.DoReFaQuantizer.export_model:11
#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.export_model:11
#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer.export_model:11
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.export_model:11
#: nni.compression.pytorch.compressor.Quantizer.export_model:11
#: nni.compression.pytorch.compressor.Quantizer.export_model_save:15 of
msgid ""
"device of the model, used to place the dummy input tensor for exporting "
"onnx file. the tensor is placed on cpu if ```device``` is None"
msgstr ""

#: nni.compression.pytorch.compressor.Quantizer.export_model_save:1 of
msgid ""
"This method helps save pytorch model, calibration config, onnx model in "
"quantizer."
msgstr ""

#: nni.compression.pytorch.compressor.Quantizer.export_model_save:3 of
msgid "pytorch model to be saved"
msgstr ""

#: nni.compression.pytorch.compressor.Quantizer.export_model_save:5 of
msgid "path to save pytorch"
msgstr ""

#: nni.compression.pytorch.compressor.Quantizer.export_model_save:7 of
msgid "(optional) config of calibration parameters"
msgstr ""

#: nni.compression.pytorch.compressor.Quantizer.find_conv_bn_patterns:1 of
msgid "Find all Conv-BN patterns, used for batch normalization folding"
msgstr ""

#: nni.compression.pytorch.compressor.Quantizer.find_conv_bn_patterns:3 of
msgid "model to be analyzed."
msgstr ""

#: nni.compression.pytorch.compressor.Quantizer.find_conv_bn_patterns:5 of
msgid "inputs to the model, used for generating the torchscript"
msgstr ""

#: nni.compression.pytorch.compressor.Quantizer.fold_bn:1 of
msgid ""
"Simulate batch normalization folding in the training graph. Folded weight"
" and bias are returned for the following operations."
msgstr ""

#: nni.compression.pytorch.compressor.Quantizer.fold_bn:4 of
msgid "inputs for the module"
msgstr ""

#: nni.compression.pytorch.compressor.Quantizer.fold_bn:6 of
msgid "the wrapper for origin module"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.load_calibration_config:1
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.load_calibration_config:1
#: nni.compression.pytorch.compressor.Quantizer.load_calibration_config:1 of
msgid ""
"This function aims to help quantizer set quantization parameters by "
"loading from a calibration_config which is exported by other quantizer or"
" itself. The main usage of this function is helping quantize aware "
"training quantizer set appropriate initial parameters so that the "
"training process will be much more flexible and converges quickly. What's"
" more, it can also enable quantizer resume quantization model by loading "
"parameters from config."
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.load_calibration_config:8
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.load_calibration_config:8
#: nni.compression.pytorch.compressor.Quantizer.load_calibration_config:8 of
msgid ""
"dict which saves quantization parameters, quantizer can export itself "
"calibration config. eg, calibration_config = "
"quantizer.export_model(model_path, calibration_path)"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.quantize_input:1
#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer.quantize_input:1
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.quantize_input:1
#: nni.compression.pytorch.compressor.Quantizer.quantize_input:1 of
msgid ""
"quantize should overload this method to quantize input. This method is "
"effectively hooked to :meth:`forward` of the model. :param inputs: inputs"
" that needs to be quantized :type inputs: Tensor :param wrapper: the "
"wrapper for origin module :type wrapper: QuantizerModuleWrapper"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.bnn_quantizer.BNNQuantizer.quantize_output:1
#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.quantize_output:1
#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer.quantize_output:1
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.quantize_output:1
#: nni.compression.pytorch.compressor.Quantizer.quantize_output:1 of
msgid ""
"quantize should overload this method to quantize output. This method is "
"effectively hooked to :meth:`forward` of the model. :param output: output"
" that needs to be quantized :type output: Tensor :param wrapper: the "
"wrapper for origin module :type wrapper: QuantizerModuleWrapper"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.bnn_quantizer.BNNQuantizer.quantize_weight:1
#: nni.algorithms.compression.pytorch.quantization.dorefa_quantizer.DoReFaQuantizer.quantize_weight:1
#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.quantize_weight:1
#: nni.algorithms.compression.pytorch.quantization.native_quantizer.NaiveQuantizer.quantize_weight:1
#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer.quantize_weight:1
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.quantize_weight:1
#: nni.compression.pytorch.compressor.Quantizer.quantize_weight:1 of
msgid ""
"quantize should overload this method to quantize weight. This method is "
"effectively hooked to :meth:`forward` of the model. :param wrapper: the "
"wrapper for origin module :type wrapper: QuantizerModuleWrapper"
msgstr ""

#: nni.compression.pytorch.compressor.Quantizer.record_shape:1 of
msgid "Record input/output's shapes of each module to be quantized"
msgstr ""

#: nni.compression.pytorch.compressor.Quantizer.record_shape:3 of
msgid "model to be recorded."
msgstr ""

#: nni.compression.pytorch.compressor.Quantizer.record_shape:5 of
msgid "inputs to the model."
msgstr ""

#: ../../Compression/CompressionReference.rst:23
msgid "Module Wrapper"
msgstr ""

#: nni.compression.pytorch.compressor.PrunerModuleWrapper.forward:1
#: nni.compression.pytorch.compressor.QuantizerModuleWrapper.forward:1 of
msgid "Defines the computation performed at every call."
msgstr ""

#: nni.compression.pytorch.compressor.PrunerModuleWrapper.forward:3
#: nni.compression.pytorch.compressor.QuantizerModuleWrapper.forward:3 of
msgid "Should be overridden by all subclasses."
msgstr ""

#: nni.compression.pytorch.compressor.PrunerModuleWrapper.forward:6
#: nni.compression.pytorch.compressor.QuantizerModuleWrapper.forward:6 of
msgid ""
"Although the recipe for forward pass needs to be defined within this "
"function, one should call the :class:`Module` instance afterwards instead"
" of this since the former takes care of running the registered hooks "
"while the latter silently ignores them."
msgstr ""

#: ../../Compression/CompressionReference.rst:33
msgid "Weight Masker"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.weight_masker.WeightMasker.calc_mask:1
#: of
msgid ""
"Calculate the mask of given layer. :param sparsity: pruning ratio,  "
"preserved weight ratio is `1 - sparsity` :type sparsity: float :param "
"wrapper: layer wrapper of this layer :type wrapper: PrunerModuleWrapper "
":param wrapper_idx: index of this wrapper in pruner's all wrappers :type "
"wrapper_idx: int"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.weight_masker.WeightMasker.calc_mask:9
#: of
msgid ""
"dictionary for storing masks, keys of the dict: 'weight_mask':  weight "
"mask tensor 'bias_mask': bias mask tensor (optional)"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker:1
#: of
msgid ""
"A structured pruning masker base class that prunes convolutional layer "
"filters."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker:3
#: of
msgid "model to be pruned"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker:5
#: of
msgid "A Pruner instance used to prune the model"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker:7
#: of
msgid ""
"after pruning, preserve filters/channels round to `preserve_round`, for "
"example: for a Conv2d layer, output channel is 32, sparsity is 0.2, if "
"preserve_round is 1 (no preserve round), then there will be int(32 * 0.2)"
" = 6 filters pruned, and 32 - 6 = 26 filters are preserved. If "
"preserve_round is 4, preserved filters will be round up to 28 (which can "
"be divided by 4) and only 4 filters are pruned."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.calc_mask:1
#: of
msgid "calculate the mask for `wrapper`."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.calc_mask:3
#: of
msgid ""
"The target sparsity of the wrapper. If we calculate the mask in the "
"normal way, then sparsity is a float number. In contrast, if we calculate"
" the mask in the dependency-aware way, sparsity is a list of float "
"numbers, each float number corressponds to a sparsity of a layer."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.calc_mask:9
#: of
msgid ""
"The wrapper of the target layer. If we calculate the mask in the normal "
"way, then `wrapper` is an instance of PrunerModuleWrapper, else `wrapper`"
" is a list of PrunerModuleWrapper."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.calc_mask:13
#: of
msgid "The index of the wrapper."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.calc_mask:15
#: of
msgid "The kw_args for the dependency-aware mode."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.get_channel_sum:1
#: of
msgid ""
"Calculate the importance weight for each channel. If want to support the "
"dependency-aware mode for this one-shot pruner, this function must be "
"implemented. :param wrapper: layer wrapper of this layer :type wrapper: "
"PrunerModuleWrapper :param wrapper_idx: index of this wrapper in pruner's"
" all wrappers :type wrapper_idx: int"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.get_channel_sum:9
#: of
msgid "Tensor that indicates the importance of each channel"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.get_mask:1
#: of
msgid "Calculate the mask of given layer."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.get_mask:3
#: of
msgid ""
"The basic mask with the same shape of weight, all item in the basic mask "
"is 1."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.get_mask:5
#: of
msgid "the module weight to be pruned"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.get_mask:7
#: of
msgid "Num of filters to prune"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.get_mask:9
#: of
msgid "layer wrapper of this layer"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.get_mask:11
#: of
msgid "index of this wrapper in pruner's all wrappers"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.get_mask:13
#: of
msgid ""
"If mask some channels for this layer in advance. In the dependency-aware "
"mode, before calculating the masks for each layer, we will calculate a "
"common mask for all the layers in the dependency set. For the pruners "
"that doesnot support dependency-aware mode, they can just ignore this "
"parameter."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.structured_pruning_masker.StructuredWeightMasker.get_mask:19
#: of
msgid "dictionary for storing masks"
msgstr ""

#: ../../Compression/CompressionReference.rst:42
#: ../../Compression/pruning.rst:20
msgid "Pruners"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner:1
#: of
msgid "This function prune the model based on the sensitivity for each layer."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner:4
#: of
msgid "model to be compressed"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner:6
#: of
msgid ""
"validation function for the model. This function should return the "
"accuracy of the validation dataset. The input parameters of evaluator can"
" be specified in the parameter `eval_args` and 'eval_kwargs' of the "
"compress function if needed. Example: >>> def evaluator(model): >>>     "
"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
" >>>     val_loader = ... >>>     model.eval() >>>     correct = 0 >>>"
"     with torch.no_grad(): >>>         for data, target in val_loader: "
">>>             data, target = data.to(device), target.to(device) >>>"
"             output = model(data) >>>             # get the index of the "
"max log-probability >>>             pred = output.argmax(dim=1, "
"keepdim=True) >>>             correct += "
"pred.eq(target.view_as(pred)).sum().item() >>>     accuracy = correct / "
"len(val_loader.dataset) >>>     return accuracy"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner:25
#: of
msgid ""
"finetune function for the model. This parameter is not essential, if is "
"not None, the sensitivity pruner will finetune the model after pruning in"
" each iteration. The input parameters of finetuner can be specified in "
"the parameter of compress called `finetune_args` and `finetune_kwargs` if"
" needed. Example: >>> def finetuner(model, epoch=3): >>>     device = "
"torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") >>>     "
"train_loader = ... >>>     criterion = torch.nn.CrossEntropyLoss() >>>"
"     optimizer = torch.optim.SGD(model.parameters(), lr=0.01) >>>     "
"model.train() >>>     for _ in range(epoch): >>>         for _, (data, "
"target) in enumerate(train_loader): >>>             data, target = "
"data.to(device), target.to(device) >>>             optimizer.zero_grad() "
">>>             output = model(data) >>>             loss = "
"criterion(output, target) >>>             loss.backward() >>>"
"             optimizer.step()"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner:45
#: of
msgid "base pruning algorithm. `level`, `l1`, `l2` or `fpgm`, by default `l1`."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner:47
#: of
msgid ""
"This function generate the sparsity proportion between the conv layers "
"according to the sensitivity analysis results. We provide a default "
"function to quantify the sparsity proportion according to the sensitivity"
" analysis results. Users can also customize this function according to "
"their needs. The input of this function is a dict, for example : {'conv1'"
" : {0.1: 0.9, 0.2 : 0.8}, 'conv2' : {0.1: 0.9, 0.2 : 0.8}}, in which, "
"'conv1' and is the name of the conv layer, and 0.1:0.9 means when the "
"sparsity of conv1 is 0.1 (10%), the model's val accuracy equals to 0.9."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner:55
#: of
msgid "The sparsity of the model that the pruner try to prune in each iteration."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner:57
#: of
msgid "The hyperparameter used to quantifiy the sensitivity for each layer."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner:59
#: of
msgid "The dir path to save the checkpoints during the pruning."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.compress:1
#: of
msgid ""
"This function iteratively prune the model according to the results of the"
" sensitivity analysis."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.compress:6
#: of
msgid ""
"Parameters for the val_funtion, the val_function will be called like "
"evaluator(\\*eval_args, \\*\\*eval_kwargs)"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.compress:11
#: of
msgid "Parameters for the finetuner function if needed."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.compress:13
#: of
msgid "resume the sensitivity results from this file."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.create_cfg:1
#: of
msgid "Generate the cfg_list for the pruner according to the prune ratios."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.create_cfg:3
#: of
msgid "For example: {'conv1' : 0.2}"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.create_cfg:5
#: of
msgid ""
"For example: [{'sparsity':0.2, 'op_names':['conv1'], "
"'op_types':['Conv2d']}]"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.current_sparsity:1
#: of
msgid "The sparsity of the weight."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.load_sensitivity:1
#: of
msgid "load the sensitivity results exported by the sensitivity analyzer"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.normalize:1
#: of
msgid ""
"Normalize the prune ratio of each layer according to the total already "
"pruned ratio and the final target total pruning ratio"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.normalize:5
#: of
msgid "Dict object that save the prune ratio for each layer"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.normalize:6
#: of
msgid "The amount of the weights expected to be pruned in this iteration"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.normalize:9
#: of
msgid "return the normalized prune ratios for each layer."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.validate_config:1
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ADMMPruner.validate_config:1
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner.validate_config:1
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationAPoZRankFilterPruner:1
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationMeanRankFilterPruner:1
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.SlimPruner:1
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.TaylorFOWeightFilterPruner:1
#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner.validate_config:1
#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner.validate_config:1
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.FPGMPruner:1
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.L1FilterPruner:1
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.L2FilterPruner:1
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.LevelPruner:1
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.OneshotPruner.validate_config:1
#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.validate_config:1
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner.validate_config:1
#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.validate_config:1
#: nni.algorithms.compression.pytorch.quantization.bnn_quantizer.BNNQuantizer.validate_config:1
#: nni.algorithms.compression.pytorch.quantization.dorefa_quantizer.DoReFaQuantizer.validate_config:1
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.validate_config:1
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.FPGMPruner:1
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L1NormPruner:1
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L2NormPruner:1
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.LevelPruner:1
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:1
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:1
#: of
msgid "Model to be pruned"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.validate_config:3
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ADMMPruner.validate_config:3
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner.validate_config:3
#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner.validate_config:3
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.OneshotPruner.validate_config:3
#: nni.algorithms.compression.pytorch.pruning.sensitivity_pruner.SensitivityPruner.validate_config:3
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner.validate_config:3
#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.validate_config:3
#: of
msgid "List on pruning configs"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.OneshotPruner:1
#: of
msgid "Prune model to an exact pruning level for one time."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.LevelPruner:3 of
msgid ""
"Supported keys:     - sparsity : This is to specify the sparsity "
"operations to be compressed to.     - op_types : Operation types to "
"prune."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:7
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner:5
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationAPoZRankFilterPruner:5
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationMeanRankFilterPruner:5
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.SlimPruner:5
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.TaylorFOWeightFilterPruner:5
#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner:5
#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner.validate_config:5
#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner:7
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.FPGMPruner:5
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.L1FilterPruner:5
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.L2FilterPruner:5
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.LevelPruner:5
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner:7
#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:9
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:17
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.FPGMPruner:9
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L1NormPruner:9
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L2NormPruner:9
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.LevelPruner:9
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:11
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:11
#: nni.algorithms.compression.v2.pytorch.pruning.movement_pruner.MovementPruner:9
#: of
msgid "Supported keys:"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner:5
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.SlimPruner:5
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.FPGMPruner:5
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.L1FilterPruner:5
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.L2FilterPruner:5
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.LevelPruner:5
#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:8
#: of
msgid "sparsity : This is to specify the sparsity operations to be compressed to."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.LevelPruner:6 of
msgid "op_types : Operation types to prune."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.L1FilterPruner:3
#: of
msgid ""
"Supported keys:     - sparsity : This is to specify the sparsity "
"operations to be compressed to.     - op_types : Only Conv2d is supported"
" in L1FilterPruner."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.L1FilterPruner:6
#: of
msgid "op_types : Only Conv2d is supported in L1FilterPruner."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationAPoZRankFilterPruner:21
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationMeanRankFilterPruner:21
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.SlimPruner:21
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.TaylorFOWeightFilterPruner:19
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.FPGMPruner:8
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.L1FilterPruner:8
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.L2FilterPruner:8
#: of
msgid ""
"If prune the model in a dependency-aware way. If it is `True`, this "
"pruner will prune the model according to the l2-norm of weights and the "
"channel-dependency or group-dependency of the model. In this way, the "
"pruner will force the conv layers that have dependencies to prune the "
"same channels, so the speedup module can better harvest the speed benefit"
" from the pruned model. Note that, if this flag is set True , the "
"dummy_input cannot be None, because the pruner needs a dummy input to "
"trace the dependency between the conv layers."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationAPoZRankFilterPruner:29
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationMeanRankFilterPruner:29
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.SlimPruner:29
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.TaylorFOWeightFilterPruner:27
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.FPGMPruner:16
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.L1FilterPruner:16
#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.L2FilterPruner:16
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.FPGMPruner:21
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L1NormPruner:21
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L2NormPruner:21
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:54
#: of
msgid ""
"The dummy input to analyze the topology constraints. Note that, the "
"dummy_input should on the same device with the model."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.L2FilterPruner:3
#: of
msgid ""
"Supported keys:     - sparsity : This is to specify the sparsity "
"operations to be compressed to.     - op_types : Only Conv2d is supported"
" in L2FilterPruner."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.L2FilterPruner:6
#: of
msgid "op_types : Only Conv2d is supported in L2FilterPruner."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.FPGMPruner:3 of
msgid ""
"Supported keys:     - sparsity : This is to specify the sparsity "
"operations to be compressed to.     - op_types : Only Conv2d is supported"
" in FPGM Pruner."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.one_shot_pruner.FPGMPruner:6 of
msgid "op_types : Only Conv2d is supported in FPGM Pruner."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.IterativePruner:1
#: of
msgid "Prune model during the training process."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.SlimPruner:3 of
msgid ""
"Supported keys:     - sparsity : This is to specify the sparsity "
"operations to be compressed to.     - op_types : Only BatchNorm2d is "
"supported in Slim Pruner."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.SlimPruner:6 of
msgid "op_types : Only BatchNorm2d is supported in Slim Pruner."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationAPoZRankFilterPruner:8
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.SlimPruner:8
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.TaylorFOWeightFilterPruner:8
#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:39
#: of
msgid "Optimizer used to train model"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.SlimPruner:10
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.TaylorFOWeightFilterPruner:10
#: of
msgid ""
"Function used to sparsify BatchNorm2d scaling factors. Users should write"
" this function as a normal function to train the Pytorch model and "
"include `model, optimizer, criterion, epoch` as function arguments."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner:12
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationAPoZRankFilterPruner:14
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationMeanRankFilterPruner:14
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.SlimPruner:14
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.TaylorFOWeightFilterPruner:14
#: of
msgid ""
"Function used to calculate the loss between the target and the output. "
"For example, you can use ``torch.nn.CrossEntropyLoss()`` as input."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.SlimPruner:17 of
msgid ""
"The number of channel sparsity regularization training epochs before "
"pruning."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.SlimPruner:19 of
msgid "Penalty parameters for sparsification."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.TaylorFOWeightFilterPruner:3
#: of
msgid ""
"Supported keys:     - sparsity : How much percentage of convolutional "
"filters are to be pruned.     - op_types : Currently only Conv2d is "
"supported in TaylorFOWeightFilterPruner."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationAPoZRankFilterPruner:5
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationMeanRankFilterPruner:5
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.TaylorFOWeightFilterPruner:5
#: of
msgid "sparsity : How much percentage of convolutional filters are to be pruned."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.TaylorFOWeightFilterPruner:6
#: of
msgid ""
"op_types : Currently only Conv2d is supported in "
"TaylorFOWeightFilterPruner."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationAPoZRankFilterPruner:19
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationMeanRankFilterPruner:19
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.TaylorFOWeightFilterPruner:17
#: of
msgid ""
"The number of batches to collect the contributions. Note that the number "
"need to be less than the maximum batch number in one epoch."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.TaylorFOWeightFilterPruner:30
#: of
msgid ""
"Only support TaylorFOWeightFilterPruner currently. If prune the model in "
"a global-sort way. If it is `True`, this pruner will prune the model "
"according to the global contributions information which means channel "
"contributions will be sorted globally and whether specific channel will "
"be pruned depends on global information."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationAPoZRankFilterPruner:3
#: of
msgid ""
"Supported keys:     - sparsity : How much percentage of convolutional "
"filters are to be pruned.     - op_types : Only Conv2d is supported in "
"ActivationAPoZRankFilterPruner."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationAPoZRankFilterPruner:6
#: of
msgid "op_types : Only Conv2d is supported in ActivationAPoZRankFilterPruner."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationAPoZRankFilterPruner:10
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationMeanRankFilterPruner:10
#: of
msgid ""
"Function used to train the model. Users should write this function as a "
"normal function to train the Pytorch model and include `model, optimizer,"
" criterion, epoch` as function arguments."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationAPoZRankFilterPruner:17
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationMeanRankFilterPruner:17
#: of
msgid "The activation type."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationMeanRankFilterPruner:3
#: of
msgid ""
"Supported keys:     - sparsity : How much percentage of convolutional "
"filters are to be pruned.     - op_types : Only Conv2d is supported in "
"ActivationMeanRankFilterPruner."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationMeanRankFilterPruner:6
#: of
msgid "op_types : Only Conv2d is supported in ActivationMeanRankFilterPruner."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner:8
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ActivationMeanRankFilterPruner:8
#: of
msgid "Optimizer used to train model."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ADMMPruner:3
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner:1
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:8
#: nni.algorithms.compression.v2.pytorch.pruning.movement_pruner.MovementPruner:1
#: of
msgid "Model to be pruned."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner:3 of
msgid ""
"Supported keys:     - sparsity : This is to specify the sparsity "
"operations to be compressed to.     - op_types : See supported type in "
"your specific pruning algorithm."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner:6 of
msgid "op_types : See supported type in your specific pruning algorithm."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner:10 of
msgid "Function to train the model"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner:15 of
msgid ""
"Total number of iterations in pruning process. We will calculate mask at "
"the end of an iteration."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner:17 of
msgid "The number of training epochs for each iteration."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner:19 of
msgid ""
"Algorithms being used to prune model, choose from `['level', 'slim', "
"'l1', 'l2', 'fpgm', 'taylorfo', 'apoz', 'mean_activation']`, by default "
"`level`"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner.calc_mask:1
#: of
msgid ""
"Calculate the mask of given layer. Scale factors with the smallest "
"absolute value in the BN layer are masked. :param wrapper: the layer to "
"instrument the compression operation :type wrapper: Module :param "
"wrapper_idx: index of this wrapper in pruner's all wrappers :type "
"wrapper_idx: int"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner.calc_mask:8
#: of
msgid ""
"Dictionary for storing masks, keys of the dict: 'weight_mask':  weight "
"mask tensor 'bias_mask': bias mask tensor (optional)"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner.compute_target_sparsity:1
#: of
msgid ""
"Calculate the sparsity for pruning :param config: Layer's pruning config "
":type config: dict"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner.compute_target_sparsity:5
#: of
msgid "Target sparsity to be pruned"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.AGPPruner.update_epoch:1
#: of
msgid "Update epoch :param epoch: current training epoch :type epoch: int"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ADMMPruner:1 of
msgid "A Pytorch implementation of ADMM Pruner algorithm."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ADMMPruner:5 of
msgid "List on pruning configs."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ADMMPruner:7 of
msgid ""
"Function used for the first subproblem. Users should write this function "
"as a normal function to train the Pytorch model and include `model, "
"optimizer, criterion, epoch` as function arguments."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ADMMPruner:11 of
msgid ""
"Function used to calculate the loss between the target and the output. By"
" default, we use CrossEntropyLoss in ADMMPruner. For example, you can use"
" ``torch.nn.CrossEntropyLoss()`` as input."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ADMMPruner:14 of
msgid ""
"Total number of iterations in pruning process. We will calculate mask "
"after we finish all iterations in ADMMPruner."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ADMMPruner:16 of
msgid "Training epochs of the first subproblem."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:57
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ADMMPruner:18 of
msgid "Penalty parameters for ADMM training."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:42
#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ADMMPruner:20
#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner:51
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner:31
#: of
msgid ""
"Base pruning algorithm. `level`, `l1`, `l2` or `fpgm`, by default `l1`. "
"Given the sparsity distribution among the ops, the assigned `base_algo` "
"is used to decide which filters/channels/weights to prune."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.iterative_pruner.ADMMPruner.compress:1
#: of
msgid "Compress the model with ADMM."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:1
#: of
msgid "A Pytorch implementation of AutoCompress pruning algorithm."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:3
#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner:3
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner:3
#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:8 of
msgid "The model to be pruned."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:5
#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner:5
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner:5
#: of
msgid ""
"Supported keys:     - sparsity : The target overall sparsity.     - "
"op_types : The operation type to prune."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:7
#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner:7
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner:7
#: of
msgid "sparsity : The target overall sparsity."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:8
#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner:8
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner:8
#: of
msgid "op_types : The operation type to prune."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:10
#: of
msgid ""
"Function used for the first subproblem of ADMM Pruner. Users should write"
" this function as a normal function to train the Pytorch model and "
"include `model, optimizer, criterion, epoch` as function arguments."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:14
#: of
msgid ""
"Function used to calculate the loss between the target and the output. By"
" default, we use CrossEntropyLoss. For example, you can use "
"``torch.nn.CrossEntropyLoss()`` as input."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:17
#: of
msgid ""
"function to evaluate the pruned model. This function should include "
"`model` as the only parameter, and returns a scalar value. Example::"
"      def evaluator(model):         device = torch.device(\"cuda\" if "
"torch.cuda.is_available() else \"cpu\")         val_loader = ...         "
"model.eval()         correct = 0         with torch.no_grad():"
"             for data, target in val_loader:                 data, target"
" = data.to(device), target.to(device)                 output = "
"model(data)                 # get the index of the max log-probability"
"                 pred = output.argmax(dim=1, keepdim=True)"
"                 correct += pred.eq(target.view_as(pred)).sum().item()"
"         accuracy = correct / len(val_loader.dataset)         return "
"accuracy"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:17
#: of
msgid ""
"function to evaluate the pruned model. This function should include "
"`model` as the only parameter, and returns a scalar value. Example::"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:36
#: of
msgid ""
"The dummy input for ```jit.trace```, users should put it on right device "
"before pass in."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:38
#: of
msgid "Number of overall iterations."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:40
#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner:49
#: of
msgid "optimize mode, `maximize` or `minimize`, by default `maximize`."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:45
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner:34
#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:29
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.SimulatedAnnealingPruner:7
#: of
msgid "Start temperature of the simulated annealing process."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:47
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner:36
#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:31
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.SimulatedAnnealingPruner:9
#: of
msgid "Stop temperature of the simulated annealing process."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:49
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner:38
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.SimulatedAnnealingPruner:11
#: of
msgid "Cool down rate of the temperature."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:51
#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner:40
#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:35
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.SimulatedAnnealingPruner:13
#: of
msgid ""
"Initial perturbation magnitude to the sparsities. The magnitude decreases"
" with current temperature."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:53
#: of
msgid "Number of iterations of ADMM Pruner."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:55
#: of
msgid "Training epochs of the first optimization subproblem of ADMMPruner."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:59
#: of
msgid "PATH to store temporary experiment data."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.auto_compress_pruner.AutoCompressPruner.compress:1
#: of
msgid "Compress the model with AutoCompress."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner:1
#: of
msgid "A Pytorch implementation of NetAdapt compression algorithm."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner:10
#: of
msgid ""
"function to short-term fine tune the masked model. This function should "
"include `model` as the only parameter, and fine tune the model for a "
"short term after each pruning iteration. Example::      def "
"short_term_fine_tuner(model, epoch=3):         device = "
"torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")         "
"train_loader = ...         criterion = torch.nn.CrossEntropyLoss()"
"         optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
"         model.train()         for _ in range(epoch):             for "
"batch_idx, (data, target) in enumerate(train_loader):                 "
"data, target = data.to(device), target.to(device)                 "
"optimizer.zero_grad()                 output = model(data)"
"                 loss = criterion(output, target)                 "
"loss.backward()                 optimizer.step()"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner:10
#: of
msgid ""
"function to short-term fine tune the masked model. This function should "
"include `model` as the only parameter, and fine tune the model for a "
"short term after each pruning iteration. Example::"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner:30
#: of
msgid ""
"function to evaluate the masked model. This function should include "
"`model` as the only parameter, and returns a scalar value. Example::"
"      def evaluator(model):         device = torch.device(\"cuda\" if "
"torch.cuda.is_available() else \"cpu\")         val_loader = ...         "
"model.eval()         correct = 0         with torch.no_grad():"
"             for data, target in val_loader:                 data, target"
" = data.to(device), target.to(device)                 output = "
"model(data)                 # get the index of the max log-probability"
"                 pred = output.argmax(dim=1, keepdim=True)"
"                 correct += pred.eq(target.view_as(pred)).sum().item()"
"         accuracy = correct / len(val_loader.dataset)         return "
"accuracy"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner:30
#: of
msgid ""
"function to evaluate the masked model. This function should include "
"`model` as the only parameter, and returns a scalar value. Example::"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner:54
#: of
msgid "sparsity to prune in each iteration."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner:56
#: of
msgid ""
"PATH to save experiment data, including the config_list generated for the"
" base pruning algorithm and the performance of the pruned model."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.net_adapt_pruner.NetAdaptPruner.compress:1
#: of
msgid "Compress the model."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner:1
#: of
msgid "A Pytorch implementation of Simulated Annealing compression algorithm."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner:10
#: of
msgid ""
"Function to evaluate the pruned model. This function should include "
"`model` as the only parameter, and returns a scalar value. Example::"
"      def evaluator(model):         device = torch.device(\"cuda\" if "
"torch.cuda.is_available() else \"cpu\")         val_loader = ...         "
"model.eval()         correct = 0         with torch.no_grad():"
"             for data, target in val_loader:                 data, target"
" = data.to(device), target.to(device)                 output = "
"model(data)                 # get the index of the max log-probability"
"                 pred = output.argmax(dim=1, keepdim=True)"
"                 correct += pred.eq(target.view_as(pred)).sum().item()"
"         accuracy = correct / len(val_loader.dataset)         return "
"accuracy"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner:10
#: of
msgid ""
"Function to evaluate the pruned model. This function should include "
"`model` as the only parameter, and returns a scalar value. Example::"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner:29
#: of
msgid "Optimize mode, `maximize` or `minimize`, by default `maximize`."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner:42
#: of
msgid ""
"PATH to save experiment data, including the config_list generated for the"
" base pruning algorithm, the performance of the pruned model and the "
"pruning history."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.simulated_annealing_pruner.SimulatedAnnealingPruner.compress:1
#: of
msgid "Compress the model with Simulated Annealing."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner:1
#: of
msgid "The model to be pruned"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner:3
#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner.validate_config:3
#: of
msgid ""
"Supported keys:     - prune_iterations : The number of rounds for the "
"iterative pruning.     - sparsity : The final sparsity when the "
"compression is done."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner:5
#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner.validate_config:5
#: of
msgid "prune_iterations : The number of rounds for the iterative pruning."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner:6
#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner.validate_config:6
#: of
msgid "sparsity : The final sparsity when the compression is done."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner:8
#: of
msgid "The optimizer for the model"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner:10
#: of
msgid "The lr scheduler for the model if used"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner:12
#: of
msgid "Whether reset weights and optimizer at the beginning of each round."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner.calc_mask:1
#: of
msgid "Generate mask for the given ``weight``."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner.calc_mask:3
#: of
msgid "The layer to be pruned"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner.calc_mask:6
#: of
msgid ""
"The mask for this weight, it is ```None``` because this pruner calculates"
" and assigns masks in ```prune_iteration_start```, no need to do anything"
" in this function."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner.get_prune_iterations:1
#: of
msgid ""
"Return the range for iterations. In the first prune iteration, masks are "
"all one, thus, add one more iteration"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner.get_prune_iterations:4
#: of
msgid "A list for pruning iterations"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.lottery_ticket.LotteryTicketPruner.prune_iteration_start:1
#: of
msgid ""
"Control the pruning procedure on updated epoch number. Should be called "
"at the beginning of the epoch."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:1
#: of
msgid ""
"A pruner specialized for pruning attention heads in models belong to the "
"transformer family."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:3
#: of
msgid ""
"Model to be pruned. Expect a model from transformers library (e.g., "
"BertModel). This pruner can work with other customized transformer "
"models, but some ranking modes might fail."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:6
#: of
msgid ""
"Supported keys:     - sparsity : This is to specify the sparsity "
"operations to be compressed to.     - op_types : Optional. Operation "
"types to prune. (Should be 'Linear' for this pruner.)     - op_names : "
"Optional. Operation names to prune."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:9
#: of
msgid ""
"op_types : Optional. Operation types to prune. (Should be 'Linear' for "
"this pruner.)"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:10
#: of
msgid "op_names : Optional. Operation names to prune."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:12
#: of
msgid ""
"Dimension of the hidden dimension of each attention head. (e.g., 64 for "
"BERT) We assume that this head_hidden_dim is constant across the entire "
"model."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:15
#: of
msgid ""
"List of groups of names for weights of each attention layer. Each element"
" should be a four-element list, with the first three corresponding to "
"Q_proj, K_proj, V_proj (in any order) and the last one being output_proj."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:18
#: of
msgid ""
"Input to model's forward method, used to infer module grouping if "
"attention_name_groups is not specified. This tensor is used by the "
"underlying torch.jit.trace to infer the module graph."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:21
#: of
msgid ""
"The criterion for ranking attention heads. Currently we support:     - "
"l1_weight: l1 norm of Q_proj, K_proj, and V_proj     - l2_weight: l2 norm"
" of Q_proj, K_proj, and V_proj     - l1_activation: l1 norm of the output"
" of attention computation     - l2_activation: l2 norm of the output of "
"attention computation     - taylorfo: l1 norm of the output of attention "
"computation * gradient for this output                 (check more "
"details in the masker documentation)"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:27
#: of
msgid "The criterion for ranking attention heads. Currently we support:"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:23
#: of
msgid "l1_weight: l1 norm of Q_proj, K_proj, and V_proj"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:24
#: of
msgid "l2_weight: l2 norm of Q_proj, K_proj, and V_proj"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:25
#: of
msgid "l1_activation: l1 norm of the output of attention computation"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:26
#: of
msgid "l2_activation: l2 norm of the output of attention computation"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:27
#: of
msgid ""
"taylorfo: l1 norm of the output of attention computation * gradient for "
"this output"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:28
#: of
msgid "(check more details in the masker documentation)"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:30
#: of
msgid "Whether rank the heads globally or locally before deciding heads to prune."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:32
#: of
msgid ""
"Number of pruning iterations. Defaults to 1 (ont-shot pruning). If "
"num_iterations > 1, the pruner will split the sparsity specified in "
"config_list uniformly and assign a fraction to each pruning iteration."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:35
#: of
msgid ""
"Number of finetuning epochs before the next pruning iteration. Only used "
"when num_iterations > 1. If num_iterations is 1, then no finetuning is "
"performed by the pruner after pruning."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:41
#: of
msgid ""
"Function used to finetune the model between pruning iterations. Only used"
" when  num_iterations > 1 or ranking_criterion is 'taylorfo'. Users "
"should write this function as a normal function to train the PyTorch "
"model and include `model, optimizer, criterion, epoch` as function "
"arguments. Note that the trainer is also used for collecting gradients "
"for pruning if ranking_criterion is 'taylorfo'. In that case, "
"``epoch=None`` will be passed."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:47
#: of
msgid ""
"Function used to calculate the loss between the target and the output. "
"Only used when  num_iterations > 1 or ranking_criterion is 'taylorfo'. "
"For example, you can use ``torch.nn.CrossEntropyLoss()`` as input."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner:51
#: of
msgid ""
"Function used to perform a \"dry run\" on the model on the entire "
"train/validation dataset in order to collect data for pruning required by"
" the criteria 'l1_activation' or 'l2_activation'. Only used when "
"ranking_criterion is 'l1_activation' or 'l2_activation'. Users should "
"write this function as a normal function that accepts a PyTorch model and"
" runs forward on the model using the entire train/validation dataset. "
"This function is not expected to perform any backpropagation or parameter"
" updates."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.group_weight_names_by_graph:1
#: of
msgid ""
"Populate self.attention_name_groups by running inference on the module "
"graph. Currently, the group inferred AttentionWeightDependency is limited"
" to a set of four weights, with the first three corresponding to Q_proj, "
"K_proj, V_proj (in any order) and the last one being output_proj."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.group_weights_by_name:1
#: of
msgid ""
"Populate self.masking_groups using the groups specified by user in "
"attention_name_groups."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.remove_ungrouped_modules:1
#: of
msgid ""
"Remove non-attention weights that might be mistakenly captured by a "
"simplified config_list. Also update the corresponding list of layer "
"information (self.modules_to_compress)"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.update_mask:1
#: of
msgid ""
"Calculate and update masks for each masking group. If global_sort is set,"
" the masks for all groups are calculated altogether, and then the groups "
"are updated individually."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.validate_weight_groups:6
#: of
msgid "Sanity checks:"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.validate_weight_groups:2
#: of
msgid "Q, K, V projection weights in each groups must have the same shape"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.validate_weight_groups:3
#: of
msgid ""
"output projection weight shape must match total hidden dimension "
"(inferred from Q, K, V projection)"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.validate_weight_groups:4
#: of
msgid "Four weights in a group must have the same sparsity in their config"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.validate_weight_groups:5
#: of
msgid "If global_sort is specified, all weights must have the same sparsity"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.transformer_pruner.TransformerHeadPruner.validate_weight_groups:6
#: of
msgid ""
"head_hidden_dim must be a divisor of the output dimension of the "
"projection weights (i.e., the resulting head number must be an integer)"
msgstr ""

#: ../../Compression/CompressionReference.rst:98
#: ../../Compression/quantization.rst:14
msgid "Quantizers"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.native_quantizer.NaiveQuantizer:1
#: of
msgid "quantize weight to 8 bits"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer:1
#: of
msgid ""
"Quantizer defined in: Quantization and Training of Neural Networks for "
"Efficient Integer-Arithmetic-Only Inference "
"http://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.bnn_quantizer.BNNQuantizer.export_model:1
#: nni.algorithms.compression.pytorch.quantization.dorefa_quantizer.DoReFaQuantizer.export_model:1
#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.export_model:1
#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer.export_model:1
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.export_model:1
#: of
msgid "Export quantized model weights and calibration parameters(optional)"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.step_with_optimizer:1
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.step_with_optimizer:1
#: of
msgid ""
"override `compressor` `step` method, quantization only happens after "
"certain number of steps"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.bnn_quantizer.BNNQuantizer.validate_config:3
#: nni.algorithms.compression.pytorch.quantization.dorefa_quantizer.DoReFaQuantizer.validate_config:3
#: nni.algorithms.compression.pytorch.quantization.qat_quantizer.QAT_Quantizer.validate_config:3
#: of
msgid "List of configurations"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.dorefa_quantizer.DoReFaQuantizer:1
#: of
msgid ""
"Quantizer using the DoReFa scheme, as defined in: Zhou et al., DoReFa-"
"Net: Training Low Bitwidth Convolutional Neural Networks with Low "
"Bitwidth Gradients (https://arxiv.org/abs/1606.06160)"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.bnn_quantizer.BNNQuantizer:1
#: of
msgid ""
"Binarized Neural Networks, as defined in: Binarized Neural Networks: "
"Training Deep Neural Networks with Weights and Outputs Constrained to +1 "
"or -1 (https://arxiv.org/abs/1602.02830)"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer:1
#: of
msgid ""
"Quantizer defined in: Learned Step Size Quantization (ICLR 2020) "
"https://arxiv.org/pdf/1902.08153.pdf"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.grad_scale:1
#: of
msgid ""
"Used to scale the gradient. Give tensor `x`, we have `y=grad_scale(x, "
"scale)=x` in the forward pass, which means that this function will not "
"change the value of `x`. In the backward pass, we have:"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.grad_scale:7
#: of
msgid ""
"rac{lpha_L}{lpha_x}= rac{lpha_L}{lpha_y}* "
"rac{lpha_y}{lpha_x}=sclae* rac{lpha_L}{lpha_x}"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.grad_scale:12
#: of
msgid ""
"This means that the origin gradient of x is scaled by a factor of "
"`scale`. Applying this function to a nn.Parameter will scale the gradient"
" of it without changing its value."
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.lsq_quantizer.LsqQuantizer.round_pass:1
#: of
msgid "A simple way to achieve STE operation."
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer:1
#: of
msgid ""
"This quantizer uses observers to record weight/output statistics to get "
"quantization information. The whole process can be divided into three "
"steps:"
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer:4
#: of
msgid ""
"It will register observers to the place where quantization would happen "
"(just like registering hooks)."
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer:5
#: of
msgid "The observers would record tensors' statistics during calibration."
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer:6
#: of
msgid "Scale & zero point would be obtained after calibration."
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer:8
#: of
msgid ""
"Note that the observer type, tensor dtype and quantization qscheme are "
"hard coded for now. Their customization are under development and will be"
" ready soon."
msgstr ""

#: nni.algorithms.compression.pytorch.quantization.observer_quantizer.ObserverQuantizer.compress:1
#: of
msgid ""
"Calculate quantization information of each tensor. Note that the "
"inference of the compressed model will no longer update the "
"corresponding. Instead, the quantization process will be simulated, which"
" is used to test the accuracy of the quantization."
msgstr ""

#: ../../Compression/CompressionReference.rst:118
#: ../../Compression/Overview.rst:108 ../../Compression/pruning.rst:20
msgid "Model Speedup"
msgstr ""

#: ../../Compression/CompressionReference.rst:121
#: ../../Compression/quantization.rst:14
msgid "Quantization Speedup"
msgstr ""

#: nni.compression.pytorch.quantization_speedup.backend.BaseModelSpeedup:1 of
msgid "Base speedup class for backend engine"
msgstr ""

#: nni.compression.pytorch.quantization_speedup.backend.BaseModelSpeedup.compress:1
#: nni.compression.pytorch.quantization_speedup.backend.BaseModelSpeedup.export_quantized_model:1
#: of
msgid ""
"This function should be overrided by subclass to build inference engine "
"which will be used to process input data"
msgstr ""

#: nni.compression.pytorch.quantization_speedup.backend.BaseModelSpeedup.inference:1
#: of
msgid ""
"This function should be overrided by subclass to provide inference "
"ability, which should return output and inference time."
msgstr ""

#: nni.compression.pytorch.quantization_speedup.backend.BaseModelSpeedup.inference:4
#: of
msgid "test data given to the inference engine"
msgstr ""

#: nni.compression.pytorch.quantization_speedup.backend.BaseModelSpeedup.inference:7
#: of
msgid ""
"* *numpy data* -- output data will be generated after inference * *float*"
" -- latency of such inference process"
msgstr ""

#: nni.compression.pytorch.quantization_speedup.backend.BaseModelSpeedup.inference:7
#: of
msgid "*numpy data* -- output data will be generated after inference"
msgstr ""

#: nni.compression.pytorch.quantization_speedup.backend.BaseModelSpeedup.inference:8
#: of
msgid "*float* -- latency of such inference process"
msgstr ""

#: nni.compression.pytorch.quantization_speedup.integrated_tensorrt.ModelSpeedupTensorRT.compress:1
#: of
msgid "Get onnx config and build tensorrt engine."
msgstr ""

#: nni.compression.pytorch.quantization_speedup.integrated_tensorrt.ModelSpeedupTensorRT.export_quantized_model:1
#: of
msgid ""
"Export TensorRT quantized model engine which only can be loaded by "
"TensorRT deserialize API."
msgstr ""

#: nni.compression.pytorch.quantization_speedup.integrated_tensorrt.ModelSpeedupTensorRT.export_quantized_model:3
#: nni.compression.pytorch.quantization_speedup.integrated_tensorrt.ModelSpeedupTensorRT.load_quantized_model:3
#: of
msgid "The path of export model"
msgstr ""

#: nni.compression.pytorch.quantization_speedup.integrated_tensorrt.ModelSpeedupTensorRT.inference:1
#: of
msgid "Do inference by tensorrt builded engine."
msgstr ""

#: nni.compression.pytorch.quantization_speedup.integrated_tensorrt.ModelSpeedupTensorRT.inference:3
#: of
msgid "Model input tensor"
msgstr ""

#: nni.compression.pytorch.quantization_speedup.integrated_tensorrt.ModelSpeedupTensorRT.load_quantized_model:1
#: of
msgid "Load TensorRT quantized model engine from specific path."
msgstr ""

#: nni.compression.pytorch.quantization_speedup.calibrator.Calibrator.get_batch:1
#: of
msgid ""
"This function is used to define the way of feeding calibrating data each "
"batch."
msgstr ""

#: nni.compression.pytorch.quantization_speedup.calibrator.Calibrator.get_batch:3
#: of
msgid "The names of the network inputs for each object in the bindings array"
msgstr ""

#: nni.compression.pytorch.quantization_speedup.calibrator.Calibrator.get_batch:6
#: of
msgid ""
"A list of device memory pointers set to the memory containing each "
"network input data, or an empty list if there are no more batches for "
"calibration. You can allocate these device buffers with pycuda, for "
"example, and then cast them to int to retrieve the pointer"
msgstr ""

#: nni.compression.pytorch.quantization_speedup.calibrator.Calibrator.read_calibration_cache:1
#: of
msgid ""
"If there is a cache, use it instead of calibrating again. Otherwise, "
"implicitly return None."
msgstr ""

#: nni.compression.pytorch.quantization_speedup.calibrator.Calibrator.read_calibration_cache:3
#: of
msgid "A cache object which contains calibration parameters for quantization"
msgstr ""

#: nni.compression.pytorch.quantization_speedup.calibrator.Calibrator.write_calibration_cache:1
#: of
msgid "Write calibration cache to specific path."
msgstr ""

#: nni.compression.pytorch.quantization_speedup.calibrator.Calibrator.write_calibration_cache:3
#: of
msgid "The calibration cache to write"
msgstr ""

#: ../../Compression/CompressionReference.rst:134
#: ../../Compression/Overview.rst:114
msgid "Compression Utilities"
msgstr ""

#: ../../Compression/CompressionReference.rst:137
msgid "Sensitivity Utilities"
msgstr ""

#: nni.compression.pytorch.utils.sensitivity_analysis.SensitivityAnalysis.analysis:1
#: of
msgid ""
"This function analyze the sensitivity to pruning for each conv layer in "
"the target model. If start and end are not set, we analyze all the conv "
"layers by default. Users can specify several layers to analyze or "
"parallelize the analysis process easily through the start and end "
"parameter."
msgstr ""

#: nni.compression.pytorch.utils.sensitivity_analysis.SensitivityAnalysis.analysis:8
#: of
msgid "args for the val_function"
msgstr ""

#: nni.compression.pytorch.utils.sensitivity_analysis.SensitivityAnalysis.analysis:10
#: of
msgid "kwargs for the val_funtion"
msgstr ""

#: nni.compression.pytorch.utils.sensitivity_analysis.SensitivityAnalysis.analysis:12
#: of
msgid ""
"list of layer names to analyze sensitivity. If this variable is set, then"
" only analyze the conv layers that specified in the list. User can also "
"use this option to parallelize the sensitivity analysis easily."
msgstr ""

#: nni.compression.pytorch.utils.sensitivity_analysis.SensitivityAnalysis.analysis:19
#: of
msgid ""
"**sensitivities** -- dict object that stores the trajectory of the "
"accuracy/loss when the prune ratio changes"
msgstr ""

#: nni.compression.pytorch.utils.sensitivity_analysis.SensitivityAnalysis.export:1
#: of
msgid ""
"Export the results of the sensitivity analysis to a csv file. The "
"firstline of the csv file describe the content structure. The first line "
"is constructed by 'layername' and sparsity list. Each line below records "
"the validation metric returned by val_func when this layer is under "
"different sparsities. Note that, due to the early_stop option, some "
"layers may not have the metrics under all sparsities."
msgstr ""

#: nni.compression.pytorch.utils.sensitivity_analysis.SensitivityAnalysis.export:8
#: of
msgid "layername, 0.25, 0.5, 0.75 conv1, 0.6, 0.55 conv2, 0.61, 0.57, 0.56"
msgstr ""

#: nni.compression.pytorch.utils.sensitivity_analysis.SensitivityAnalysis.export:12
#: of
msgid "Path of the output file"
msgstr ""

#: nni.compression.pytorch.utils.sensitivity_analysis.SensitivityAnalysis.load_state_dict:1
#: of
msgid "Update the weight of the model"
msgstr ""

#: nni.compression.pytorch.utils.sensitivity_analysis.SensitivityAnalysis.update_already_pruned:1
#: of
msgid "Set the already pruned ratio for the target layer."
msgstr ""

#: ../../Compression/CompressionReference.rst:143
msgid "Topology Utilities"
msgstr ""

#: nni.compression.pytorch.utils.shape_dependency.ChannelDependency.build_dependency:1
#: of
msgid "Build the channel dependency for the conv layers in the model."
msgstr ""

#: nni.compression.pytorch.utils.shape_dependency.ChannelDependency.dependency_sets:1
#: of
msgid "Get the list of the dependency set."
msgstr ""

#: nni.compression.pytorch.utils.shape_dependency.ChannelDependency.dependency_sets:3
#: of
msgid ""
"**dependency_sets** -- list of the dependency sets. For example, "
"[set(['conv1', 'conv2']), set(['conv3', 'conv4'])]"
msgstr ""

#: nni.compression.pytorch.utils.shape_dependency.ChannelDependency.export:1 of
msgid ""
"export the channel dependencies as a csv file. The layers at the same "
"line have output channel dependencies with each other. For example, "
"layer1.1.conv2, conv1, and layer1.0.conv2 have output channel "
"dependencies with each other, which means the output channel(filters) "
"numbers of these three layers should be same with each other, otherwise "
"the model may has shape conflict. Output example: Dependency "
"Set,Convolutional Layers Set 1,layer1.1.conv2,layer1.0.conv2,conv1 Set "
"2,layer1.0.conv1 Set 3,layer1.1.conv1"
msgstr ""

#: nni.compression.pytorch.utils.shape_dependency.GroupDependency.build_dependency:1
#: of
msgid ""
"Build the channel dependency for the conv layers in the model. This "
"function return the group number of each conv layers. Note that, here, "
"the group count of conv layers may be larger than their originl groups. "
"This is because that the input channel will also be grouped for the group"
" conv layers. To make this clear, assume we have two group conv layers: "
"conv1(group=2), conv2(group=4). conv2 takes the output features of conv1 "
"as input. Then we have to the filters of conv1 can still be divided into "
"4 groups after filter pruning, because the input channels of conv2 should"
" be divided into 4 groups."
msgstr ""

#: nni.compression.pytorch.utils.shape_dependency.GroupDependency.build_dependency:14
#: of
msgid ""
"**self.dependency** -- key: the name of conv layers, value: the minimum "
"value that the number of filters should be divisible to."
msgstr ""

#: nni.compression.pytorch.utils.shape_dependency.GroupDependency.export:1 of
msgid ""
"export the group dependency to a csv file. Each line describes a "
"convolution layer, the first part of each line is the Pytorch module name"
" of the conv layer. The second part of each line is the group count of "
"the filters in this layer. Note that, the group count may be larger than "
"this layers original group number. output example: Conv layer, Groups "
"Conv1, 1 Conv2, 2 Conv3, 4"
msgstr ""

#: nni.compression.pytorch.utils.mask_conflict.GroupMaskConflict.fix_mask:1 of
msgid ""
"Fix the mask conflict before the mask inference for the layers that has "
"group dependencies. This function should be called before the mask "
"inference of the 'speedup' module."
msgstr ""

#: nni.compression.pytorch.utils.mask_conflict.ChannelMaskConflict.fix_mask:1
#: of
msgid ""
"Fix the mask conflict before the mask inference for the layers that has "
"shape dependencies. This function should be called before the mask "
"inference of the 'speedup' module. Only structured pruning masks are "
"supported."
msgstr ""

#: ../../Compression/CompressionReference.rst:158
#: ../../Compression/CompressionUtils.rst:164
msgid "Model FLOPs/Parameters Counter"
msgstr ""

#: nni.compression.pytorch.utils.counter.count_flops_params:1 of
msgid ""
"Count FLOPs and Params of the given model. This function would identify "
"the mask on the module and take the pruned shape into consideration. Note"
" that, for sturctured pruning, we only identify the remained filters "
"according to its mask, and do not take the pruned input channels into "
"consideration, so the calculated FLOPs will be larger than real number."
msgstr ""

#: nni.compression.pytorch.utils.counter.count_flops_params:7 of
msgid ""
"The FLOPs is counted \"per sample\", which means that input has a batch "
"size larger than 1, the calculated FLOPs should not differ from batch "
"size of 1."
msgstr ""

#: nni.compression.pytorch.utils.counter.count_flops_params:10 of
msgid "Target model."
msgstr ""

#: nni.compression.pytorch.utils.counter.count_flops_params:12 of
msgid ""
"The input shape of data (a tuple), a tensor or a tuple of tensor as input"
" data."
msgstr ""

#: nni.compression.pytorch.utils.counter.count_flops_params:14 of
msgid ""
"A mapping of (module -> torch.nn.Module : custom operation) the custom "
"operation is a callback funtion to calculate the module flops and "
"parameters, it will overwrite the default operation. for reference, "
"please see ``ops`` in ``ModelProfiler``."
msgstr ""

#: nni.compression.pytorch.utils.counter.count_flops_params:19 of
msgid "If False, mute detail information about modules. Default is True."
msgstr ""

#: nni.compression.pytorch.utils.counter.count_flops_params:21 of
msgid ""
"the mode of how to collect information. If the mode is set to "
"``default``, only the information of convolution and linear will be "
"collected. If the mode is set to ``full``, other operations will also be "
"collected."
msgstr ""

#: nni.compression.pytorch.utils.counter.count_flops_params:26 of
msgid ""
"Representing total FLOPs, total parameters, and a detailed list of "
"results respectively. The list of results are a list of dict, each of "
"which contains (name, module_type, weight_shape, flops, params, "
"input_size, output_size) as its keys."
msgstr ""

#: ../../Compression/CompressionUtils.rst:2
msgid "Analysis Utils for Model Compression"
msgstr ""

#: ../../Compression/CompressionUtils.rst:6
msgid ""
"We provide several easy-to-use tools for users to analyze their model "
"during model compression."
msgstr ""

#: ../../Compression/CompressionUtils.rst:9
msgid "Sensitivity Analysis"
msgstr ""

#: ../../Compression/CompressionUtils.rst:11
msgid ""
"First, we provide a sensitivity analysis tool (\\ "
"**SensitivityAnalysis**\\ ) for users to analyze the sensitivity of each "
"convolutional layer in their model. Specifically, the SensitiviyAnalysis "
"gradually prune each layer of the model, and test the accuracy of the "
"model at the same time. Note that, SensitivityAnalysis only prunes a "
"layer once a time, and the other layers are set to their original "
"weights. According to the accuracies of different convolutional layers "
"under different sparsities, we can easily find out which layers the model"
" accuracy is more sensitive to."
msgstr ""

#: ../../Compression/CompressionUtils.rst:14
#: ../../Compression/CompressionUtils.rst:107
#: ../../Compression/CompressionUtils.rst:171
#: ../../Compression/DependencyAware.rst:39
#: ../../Compression/ModelSpeedup.rst:25 ../../Compression/Pruner.rst:41
#: ../../Compression/Pruner.rst:69 ../../Compression/Pruner.rst:128
#: ../../Compression/Pruner.rst:174 ../../Compression/Pruner.rst:231
#: ../../Compression/Pruner.rst:264 ../../Compression/Pruner.rst:299
#: ../../Compression/Pruner.rst:341 ../../Compression/Pruner.rst:375
#: ../../Compression/Pruner.rst:432 ../../Compression/Pruner.rst:473
#: ../../Compression/Pruner.rst:511 ../../Compression/Pruner.rst:548
#: ../../Compression/Pruner.rst:604 ../../Compression/Pruner.rst:651
#: ../../Compression/Pruner.rst:707 ../../Compression/Pruner.rst:764
#: ../../Compression/QuantizationSpeedup.rst:58
#: ../../Compression/Quantizer.rst:20 ../../Compression/Quantizer.rst:45
#: ../../Compression/Quantizer.rst:177 ../../Compression/Quantizer.rst:227
#: ../../Compression/Quantizer.rst:266 ../../Compression/Quantizer.rst:325
#: ../../Compression/v2_pruning_algo.rst:41
#: ../../Compression/v2_pruning_algo.rst:72
#: ../../Compression/v2_pruning_algo.rst:98
#: ../../Compression/v2_pruning_algo.rst:127
#: ../../Compression/v2_pruning_algo.rst:154
#: ../../Compression/v2_pruning_algo.rst:191
#: ../../Compression/v2_pruning_algo.rst:224
#: ../../Compression/v2_pruning_algo.rst:261
#: ../../Compression/v2_pruning_algo.rst:299
#: ../../Compression/v2_pruning_algo.rst:340
#: ../../Compression/v2_pruning_algo.rst:386
#: ../../Compression/v2_pruning_algo.rst:416
#: ../../Compression/v2_pruning_algo.rst:457
#: ../../Compression/v2_pruning_algo.rst:493
#: ../../Compression/v2_pruning_algo.rst:525
#: ../../Compression/v2_pruning_algo.rst:569
msgid "Usage"
msgstr ""

#: ../../Compression/CompressionUtils.rst:16
msgid "The following codes show the basic usage of the SensitivityAnalysis."
msgstr ""

#: ../../Compression/CompressionUtils.rst:40
msgid ""
"Two key parameters of SensitivityAnalysis are ``model``\\ , and "
"``val_func``. ``model`` is the neural network that to be analyzed and the"
" ``val_func`` is the validation function that returns the model "
"accuracy/loss/ or other metrics on the validation dataset. Due to "
"different scenarios may have different ways to calculate the "
"loss/accuracy, so users should prepare a function that returns the model "
"accuracy/loss on the dataset and pass it to SensitivityAnalysis. "
"SensitivityAnalysis can export the sensitivity results as a csv file "
"usage is shown in the example above."
msgstr ""

#: ../../Compression/CompressionUtils.rst:43
msgid ""
"Futhermore, users can specify the sparsities values used to prune for "
"each layer by optional parameter ``sparsities``."
msgstr ""

#: ../../Compression/CompressionUtils.rst:49
#, python-format
msgid ""
"the SensitivityAnalysis will prune 25% 50% 75% weights gradually for each"
" layer, and record the model's accuracy at the same time "
"(SensitivityAnalysis only prune a layer once a time, the other layers are"
" set to their original weights). If the sparsities is not set, "
"SensitivityAnalysis will use the numpy.arange(0.1, 1.0, 0.1) as the "
"default sparsity values."
msgstr ""

#: ../../Compression/CompressionUtils.rst:51
msgid ""
"Users can also speed up the progress of sensitivity analysis by the "
"early_stop_mode and early_stop_value option. By default, the "
"SensitivityAnalysis will test the accuracy under all sparsities for each "
"layer. In contrast, when the early_stop_mode and early_stop_value are "
"set, the sensitivity analysis for a layer will stop, when the "
"accuracy/loss has already met the threshold set by early_stop_value. We "
"support four early stop modes:  minimize, maximize, dropped, raised."
msgstr ""

#: ../../Compression/CompressionUtils.rst:53
msgid ""
"minimize: The analysis stops when the validation metric return by the "
"val_func lower than ``early_stop_value``."
msgstr ""

#: ../../Compression/CompressionUtils.rst:55
msgid ""
"maximize: The analysis stops when the validation metric return by the "
"val_func larger than ``early_stop_value``."
msgstr ""

#: ../../Compression/CompressionUtils.rst:57
msgid ""
"dropped: The analysis stops when the validation metric has dropped by "
"``early_stop_value``."
msgstr ""

#: ../../Compression/CompressionUtils.rst:59
msgid ""
"raised: The analysis stops when the validation metric has raised by "
"``early_stop_value``."
msgstr ""

#: ../../Compression/CompressionUtils.rst:65
msgid ""
"If users only want to analyze several specified convolutional layers, "
"users can specify the target conv layers by the ``specified_layers`` in "
"analysis function. ``specified_layers`` is a list that consists of the "
"Pytorch module names of the conv layers. For example"
msgstr ""

#: ../../Compression/CompressionUtils.rst:71
msgid ""
"In this example, only the ``Conv1`` layer is analyzed. In addtion, users "
"can quickly and easily achieve the analysis parallelization by launching "
"multiple processes and assigning different conv layers of the same model "
"to each process."
msgstr ""

#: ../../Compression/CompressionUtils.rst:74
msgid "Output example"
msgstr ""

#: ../../Compression/CompressionUtils.rst:76
msgid ""
"The following lines are the example csv file exported from "
"SensitivityAnalysis. The first line is constructed by 'layername' and "
"sparsity list. Here the sparsity value means how much weight "
"SensitivityAnalysis prune for each layer. Each line below records the "
"model accuracy when this layer is under different sparsities. Note that, "
"due to the early_stop option, some layers may not have model "
"accuracies/losses under all sparsities, for example, its accuracy drop "
"has already exceeded the threshold set by the user."
msgstr ""

#: ../../Compression/CompressionUtils.rst:89
msgid "Topology Analysis"
msgstr ""

#: ../../Compression/CompressionUtils.rst:91
msgid ""
"We also provide several tools for the topology analysis during the model "
"compression. These tools are to help users compress their model better. "
"Because of the complex topology of the network, when compressing the "
"model, users often need to spend a lot of effort to check whether the "
"compression configuration is reasonable. So we provide these tools for "
"topology analysis to reduce the burden on users."
msgstr ""

#: ../../Compression/CompressionUtils.rst:94
msgid "ChannelDependency"
msgstr ""

#: ../../Compression/CompressionUtils.rst:96
msgid ""
"Complicated models may have residual connection/concat operations in "
"their models. When the user prunes these models, they need to be careful "
"about the channel-count dependencies between the convolution layers in "
"the model. Taking the following residual block in the resnet18 as an "
"example. The output features of the ``layer2.0.conv2`` and "
"``layer2.0.downsample.0`` are added together, so the number of the output"
" channels of ``layer2.0.conv2`` and ``layer2.0.downsample.0`` should be "
"the same, or there may be a tensor shape conflict."
msgstr ""

#: ../../Compression/CompressionUtils.rst:104
msgid ""
"If the layers have channel dependency are assigned with different "
"sparsities (here we only discuss the structured pruning by "
"L1FilterPruner/L2FilterPruner), then there will be a shape conflict "
"during these layers. Even the pruned model with mask works fine, the "
"pruned model cannot be speedup to the final model directly that runs on "
"the devices, because there will be a shape conflict when the model tries "
"to add/concat the outputs of these layers. This tool is to find the "
"layers that have channel count dependencies to help users better prune "
"their model."
msgstr ""

#: ../../Compression/CompressionUtils.rst:117
msgid "Output Example"
msgstr ""

#: ../../Compression/CompressionUtils.rst:119
msgid ""
"The following lines are the output example of torchvision.models.resnet18"
" exported by ChannelDependency. The layers at the same line have output "
"channel dependencies with each other. For example, layer1.1.conv2, conv1,"
" and layer1.0.conv2 have output channel dependencies with each other, "
"which means the output channel(filters) numbers of these three layers "
"should be same with each other, otherwise, the model may have shape "
"conflict."
msgstr ""

#: ../../Compression/CompressionUtils.rst:138
msgid "MaskConflict"
msgstr ""

#: ../../Compression/CompressionUtils.rst:140
msgid ""
"When the masks of different layers in a model have conflict (for example,"
" assigning different sparsities for the layers that have channel "
"dependency), we can fix the mask conflict by MaskConflict. Specifically, "
"the MaskConflict loads the masks exported by the pruners(L1FilterPruner, "
"etc), and check if there is mask conflict, if so, MaskConflict sets the "
"conflicting masks to the same value."
msgstr ""

#: ../../Compression/CompressionUtils.rst:148
msgid "not_safe_to_prune"
msgstr ""

#: ../../Compression/CompressionUtils.rst:150
msgid ""
"If we try to prune a layer whose output tensor is taken as the input by a"
" shape-constraint OP(for example, view, reshape), then such pruning maybe"
" not be safe. For example, we have a convolutional layer followed by a "
"view function."
msgstr ""

#: ../../Compression/CompressionUtils.rst:157
msgid ""
"If the output shape of the pruned conv layer is not divisible by 1024(for"
" example(batch, 500, 3, 3)), we may meet a shape error. We cannot replace"
" such a function that directly operates on the Tensor. Therefore, we need"
" to be careful when pruning such layers. The function not_safe_to_prune "
"finds all the layers followed by a shape-constraint function. Here is an "
"example for usage. If you meet a shape error when running the forward "
"inference on the speeduped model, you can exclude the layers returned by "
"not_safe_to_prune and try again."
msgstr ""

#: ../../Compression/CompressionUtils.rst:166
msgid ""
"We provide a model counter for calculating the model FLOPs and "
"parameters. This counter supports calculating FLOPs/parameters of a "
"normal model without masks, it can also calculates FLOPs/parameters of a "
"model with mask wrappers, which helps users easily check model complexity"
" during model compression on NNI. Note that, for sturctured pruning, we "
"only identify the remained filters according to its mask, which not "
"taking the pruned input channels into consideration, so the calculated "
"FLOPs will be larger than real number (i.e., the number calculated after "
"Model Speedup)."
msgstr ""

#: ../../Compression/CompressionUtils.rst:168
msgid ""
"We support two modes to collect information of modules. The first mode is"
" ``default``\\ , which only collect the information of convolution and "
"linear. The second mode is ``full``\\ , which also collect the "
"information of other operations. Users can easily use our collected "
"``results`` for futher analysis."
msgstr ""

#: ../../Compression/CustomizeCompressor.rst:2
msgid "Customize New Compression Algorithm"
msgstr ""

#: ../../Compression/CustomizeCompressor.rst:6
msgid ""
"In order to simplify the process of writing new compression algorithms, "
"we have designed simple and flexible programming interface, which covers "
"pruning and quantization. Below, we first demonstrate how to customize a "
"new pruning algorithm and then demonstrate how to customize a new "
"quantization algorithm."
msgstr ""

#: ../../Compression/CustomizeCompressor.rst:8
msgid ""
"**Important Note** To better understand how to customize new "
"pruning/quantization algorithms, users should first understand the "
"framework that supports various pruning algorithms in NNI. Reference "
"`Framework overview of model compression "
"<../Compression/Framework.rst>`__"
msgstr ""

#: ../../Compression/CustomizeCompressor.rst:11
msgid "Customize a new pruning algorithm"
msgstr ""

#: ../../Compression/CustomizeCompressor.rst:13
msgid ""
"Implementing a new pruning algorithm requires implementing a ``weight "
"masker`` class which shoud be a subclass of ``WeightMasker``\\ , and a "
"``pruner`` class, which should be a subclass ``Pruner``."
msgstr ""

#: ../../Compression/CustomizeCompressor.rst:15
msgid "An implementation of ``weight masker`` may look like this:"
msgstr ""

#: ../../Compression/CustomizeCompressor.rst:31
msgid ""
"You can reference nni provided :githublink:`weight masker "
"<nni/algorithms/compression/pytorch/pruning/structured_pruning_masker.py>`"
" implementations to implement your own weight masker."
msgstr ""

#: ../../Compression/CustomizeCompressor.rst:33
msgid "A basic ``pruner`` looks likes this:"
msgstr ""

#: ../../Compression/CustomizeCompressor.rst:55
msgid ""
"Reference nni provided :githublink:`pruner "
"<nni/algorithms/compression/pytorch/pruning/one_shot_pruner.py>` "
"implementations to implement your own pruner class."
msgstr ""

#: ../../Compression/CustomizeCompressor.rst:60
msgid "Customize a new quantization algorithm"
msgstr ""

#: ../../Compression/CustomizeCompressor.rst:62
msgid ""
"To write a new quantization algorithm, you can write a class that "
"inherits ``nni.compression.pytorch.Quantizer``. Then, override the member"
" functions with the logic of your algorithm. The member function to "
"override is ``quantize_weight``. ``quantize_weight`` directly returns the"
" quantized weights rather than mask, because for quantization the "
"quantized weights cannot be obtained by applying mask."
msgstr ""

#: ../../Compression/CustomizeCompressor.rst:137
msgid "Customize backward function"
msgstr ""

#: ../../Compression/CustomizeCompressor.rst:139
msgid ""
"Sometimes it's necessary for a quantization operation to have a "
"customized backward function, such as `Straight-Through Estimator "
"<https://stackoverflow.com/questions/38361314/the-concept-of-straight-"
"through-estimator-ste>`__\\ , user can customize a backward function as "
"follow:"
msgstr ""

#: ../../Compression/CustomizeCompressor.rst:178
msgid ""
"If you do not customize ``QuantGrad``\\ , the default backward is "
"Straight-Through Estimator. *Coming Soon* ..."
msgstr ""

#: ../../Compression/DependencyAware.rst:2
msgid "Dependency-aware Mode for Filter Pruning"
msgstr ""

#: ../../Compression/DependencyAware.rst:4
msgid ""
"Currently, we have several filter pruning algorithm for the convolutional"
" layers: FPGM Pruner, L1Filter Pruner, L2Filter Pruner, Activation APoZ "
"Rank Filter Pruner, Activation Mean Rank Filter Pruner, Taylor FO On "
"Weight Pruner. In these filter pruning algorithms, the pruner will prune "
"each convolutional layer separately. While pruning a convolution layer, "
"the algorithm will quantify the importance of each filter based on some "
"specific rules(such as l1-norm), and prune the less important filters."
msgstr ""

#: ../../Compression/DependencyAware.rst:6
msgid ""
"As `dependency analysis utils <./CompressionUtils.rst>`__ shows, if the "
"output channels of two convolutional layers(conv1, conv2) are added "
"together, then these two conv layers have channel dependency with each "
"other(more details please see `Compression Utils "
"<./CompressionUtils.rst>`__\\ ). Take the following figure as an example."
msgstr ""

#: ../../Compression/DependencyAware.rst:14
#, python-format
msgid ""
"If we prune the first 50% of output channels(filters) for conv1, and "
"prune the last 50% of output channels for conv2. Although both layers "
"have pruned 50% of the filters, the speedup module still needs to add "
"zeros to align the output channels. In this case, we cannot harvest the "
"speed benefit from the model pruning."
msgstr ""

#: ../../Compression/DependencyAware.rst:16
msgid ""
"To better gain the speed benefit of the model pruning, we add a "
"dependency-aware mode for the Filter Pruner. In the dependency-aware "
"mode, the pruner prunes the model not only based on the l1 norm of each "
"filter, but also the topology of the whole network architecture."
msgstr ""

#: ../../Compression/DependencyAware.rst:18
msgid ""
"In the dependency-aware mode(\\ ``dependency_aware`` is set ``True``\\ ),"
" the pruner will try to prune the same output channels for the layers "
"that have the channel dependencies with each other, as shown in the "
"following figure."
msgstr ""

#: ../../Compression/DependencyAware.rst:26
msgid ""
"Take the dependency-aware mode of L1Filter Pruner as an example. "
"Specifically, the pruner will calculate the L1 norm (for example) sum of "
"all the layers in the dependency set for each channel. Obviously, the "
"number of channels that can actually be pruned of this dependency set in "
"the end is determined by the minimum sparsity of layers in this "
"dependency set(denoted by ``min_sparsity``\\ ). According to the L1 norm "
"sum of each channel, the pruner will prune the same ``min_sparsity`` "
"channels for all the layers. Next, the pruner will additionally prune "
"``sparsity`` - ``min_sparsity`` channels for each convolutional layer "
"based on its own L1 norm of each channel. For example, suppose the output"
" channels of ``conv1`` , ``conv2`` are added together and the configured "
"sparsities of ``conv1`` and ``conv2`` are 0.3, 0.2 respectively. In this "
"case, the ``dependency-aware pruner`` will"
msgstr ""

#: ../../Compression/DependencyAware.rst:34
msgid ""
"In addition, for the convolutional layers that have more than one filter "
"group, ``dependency-aware pruner`` will also try to prune the same number"
" of the channels for each filter group. Overall, this pruner will prune "
"the model according to the L1 norm of each filter and try to meet the "
"topological constrains(channel dependency, etc) to improve the final "
"speed gain after the speedup process."
msgstr ""

#: ../../Compression/DependencyAware.rst:36
msgid ""
"In the dependency-aware mode, the pruner will provide a better speed gain"
" from the model pruning."
msgstr ""

#: ../../Compression/DependencyAware.rst:41
msgid ""
"In this section, we will show how to enable the dependency-aware mode for"
" the filter pruner. Currently, only the one-shot pruners such as FPGM "
"Pruner, L1Filter Pruner, L2Filter Pruner, Activation APoZ Rank Filter "
"Pruner, Activation Mean Rank Filter Pruner, Taylor FO On Weight Pruner, "
"support the dependency-aware mode."
msgstr ""

#: ../../Compression/DependencyAware.rst:43
msgid "To enable the dependency-aware mode for ``L1FilterPruner``\\ :"
msgstr ""

#: ../../Compression/DependencyAware.rst:66
msgid "Evaluation"
msgstr ""

#: ../../Compression/DependencyAware.rst:68
msgid ""
"In order to compare the performance of the pruner with or without the "
"dependency-aware mode, we use L1FilterPruner to prune the Mobilenet_v2 "
"separately when the dependency-aware mode is turned on and off. To "
"simplify the experiment, we use the uniform pruning which means we "
"allocate the same sparsity for all convolutional layers in the model. We "
"trained a Mobilenet_v2 model on the cifar10 dataset and prune the model "
"based on this pretrained checkpoint. The following figure shows the "
"accuracy and FLOPs of the model pruned by different pruners."
msgstr ""

#: ../../Compression/DependencyAware.rst:77
msgid ""
"In the figure, the ``Dependency-aware`` represents the L1FilterPruner "
"with dependency-aware mode enabled. ``L1 Filter`` is the normal "
"``L1FilterPruner`` without the dependency-aware mode, and the ``No-"
"Dependency`` means  pruner only prunes the layers that has no channel "
"dependency with other layers. As we can see in the figure, when the "
"dependency-aware mode enabled, the pruner can bring higher accuracy under"
" the same Flops."
msgstr ""

#: ../../Compression/Framework.rst:2
msgid "Framework overview of model compression"
msgstr ""

#: ../../Compression/Framework.rst:6
msgid ""
"Below picture shows the components overview of model compression "
"framework."
msgstr ""

#: ../../Compression/Framework.rst:14
msgid ""
"There are 3 major components/classes in NNI model compression framework: "
"``Compressor``\\ , ``Pruner`` and ``Quantizer``. Let's look at them in "
"detail one by one:"
msgstr ""

#: ../../Compression/Framework.rst:19
msgid ""
"Compressor is the base class for pruner and quntizer, it provides a "
"unified interface for pruner and quantizer for end users, so that pruner "
"and quantizer can be used in the same way. For example, to use a pruner:"
msgstr ""

#: ../../Compression/Framework.rst:38
msgid "To use a quantizer:"
msgstr ""

#: ../../Compression/Framework.rst:55
msgid ""
"View :githublink:`example code <examples/model_compress>` for more "
"information."
msgstr ""

#: ../../Compression/Framework.rst:57
msgid "``Compressor`` class provides some utility methods for subclass and users:"
msgstr ""

#: ../../Compression/Framework.rst:60
msgid "Set wrapper attribute"
msgstr ""

#: ../../Compression/Framework.rst:62
msgid ""
"Sometimes ``calc_mask`` must save some state data, therefore users can "
"use ``set_wrappers_attribute`` API to register attribute just like how "
"buffers are registered in PyTorch modules. These buffers will be "
"registered to ``module wrapper``. Users can access these buffers through "
"``module wrapper``. In above example, we use ``set_wrappers_attribute`` "
"to set a buffer ``if_calculated`` which is used as flag indicating if the"
" mask of a layer is already calculated."
msgstr ""

#: ../../Compression/Framework.rst:66
msgid "Collect data during forward"
msgstr ""

#: ../../Compression/Framework.rst:68
msgid ""
"Sometimes users want to collect some data during the modules' forward "
"method, for example, the mean value of the activation. This can be done "
"by adding a customized collector to module."
msgstr ""

#: ../../Compression/Framework.rst:87
msgid "The collector function will be called each time the forward method runs."
msgstr ""

#: ../../Compression/Framework.rst:89
msgid "Users can also remove this collector like this:"
msgstr ""

#: ../../Compression/Framework.rst:103
msgid "Pruner"
msgstr ""

#: ../../Compression/Framework.rst:105
msgid ""
"A pruner receives ``model`` , ``config_list`` as arguments. Some pruners "
"like ``TaylorFOWeightFilter Pruner`` prune the model per the "
"``config_list`` during training loop by adding a hook on "
"``optimizer.step()``."
msgstr ""

#: ../../Compression/Framework.rst:108
msgid ""
"Pruner class is a subclass of Compressor, so it contains everything in "
"the Compressor class and some additional components only for pruning, it "
"contains:"
msgstr ""

#: ../../Compression/Framework.rst:111
msgid "Weight masker"
msgstr ""

#: ../../Compression/Framework.rst:113
msgid ""
"A ``weight masker`` is the implementation of pruning algorithms, it can "
"prune a specified layer wrapped by ``module wrapper`` with specified "
"sparsity."
msgstr ""

#: ../../Compression/Framework.rst:116
msgid "Pruning module wrapper"
msgstr ""

#: ../../Compression/Framework.rst:118
msgid "A ``pruning module wrapper`` is a module containing:"
msgstr ""

#: ../../Compression/Framework.rst:121
msgid "the origin module"
msgstr ""

#: ../../Compression/Framework.rst:122
msgid "some buffers used by ``calc_mask``"
msgstr ""

#: ../../Compression/Framework.rst:123
msgid ""
"a new forward method that applies masks before running the original "
"forward method."
msgstr ""

#: ../../Compression/Framework.rst:125
msgid "the reasons to use ``module wrapper``\\ :"
msgstr ""

#: ../../Compression/Framework.rst:128
msgid ""
"some buffers are needed by ``calc_mask`` to calculate masks and these "
"buffers should be registered in ``module wrapper`` so that the original "
"modules are not contaminated."
msgstr ""

#: ../../Compression/Framework.rst:129
msgid ""
"a new ``forward`` method is needed to apply masks to weight before "
"calling the real ``forward`` method."
msgstr ""

#: ../../Compression/Framework.rst:132
msgid "Pruning hook"
msgstr ""

#: ../../Compression/Framework.rst:134
msgid ""
"A pruning hook is installed on a pruner when the pruner is constructed, "
"it is used to call pruner's calc_mask method at ``optimizer.step()`` is "
"invoked."
msgstr ""

#: ../../Compression/Framework.rst:139
msgid "Quantizer"
msgstr ""

#: ../../Compression/Framework.rst:141
msgid ""
"Quantizer class is also a subclass of ``Compressor``\\ , it is used to "
"compress models by reducing the number of bits required to represent "
"weights or activations, which can reduce the computations and the "
"inference time. It contains:"
msgstr ""

#: ../../Compression/Framework.rst:144
msgid "Quantization module wrapper"
msgstr ""

#: ../../Compression/Framework.rst:146
msgid ""
"Each module/layer of the model to be quantized is wrapped by a "
"quantization module wrapper, it provides a new ``forward`` method to "
"quantize the original module's weight, input and output."
msgstr ""

#: ../../Compression/Framework.rst:149
msgid "Quantization hook"
msgstr ""

#: ../../Compression/Framework.rst:151
msgid ""
"A quantization hook is installed on a quntizer when it is constructed, it"
" is call at ``optimizer.step()``."
msgstr ""

#: ../../Compression/Framework.rst:154
msgid "Quantization methods"
msgstr ""

#: ../../Compression/Framework.rst:156
msgid ""
"``Quantizer`` class provides following methods for subclass to implement "
"quantization algorithms:"
msgstr ""

#: ../../Compression/Framework.rst:206
msgid "Multi-GPU support"
msgstr ""

#: ../../Compression/Framework.rst:208
msgid ""
"On multi-GPU training, buffers and parameters are copied to multiple GPU "
"every time the ``forward`` method runs on multiple GPU. If buffers and "
"parameters are updated in the ``forward`` method, an ``in-place`` update "
"is needed to ensure the update is effective. Since ``calc_mask`` is "
"called in the ``optimizer.step`` method, which happens after the "
"``forward`` method and happens only on one GPU, it supports multi-GPU "
"naturally."
msgstr ""

#: ../../Compression/ModelSpeedup.rst:2
msgid "Speed up Masked Model"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:4
msgid "*This feature is in Beta version.*"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:7
#: ../../Compression/QuantizationSpeedup.rst:6
msgid "Introduction"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:9
msgid ""
"Pruning algorithms usually use weight masks to simulate the real pruning."
" Masks can be used to check model performance of a specific pruning (or "
"sparsity), but there is no real speedup. Since model speedup is the "
"ultimate goal of model pruning, we try to provide a tool to users to "
"convert a model to a smaller one based on user provided masks (the masks "
"come from the pruning algorithms)."
msgstr ""

#: ../../Compression/ModelSpeedup.rst:15
msgid ""
"There are two types of pruning. One is fine-grained pruning, it does not "
"change the shape of weights, and input/output tensors. Sparse kernel is "
"required to speed up a fine-grained pruned layer. The other is coarse-"
"grained pruning (e.g., channels), shape of weights and input/output "
"tensors usually change due to such pruning. To speed up this kind of "
"pruning, there is no need to use sparse kernel, just replace the pruned "
"layer with smaller one. Since the support of sparse kernels in community "
"is limited, we only support the speedup of coarse-grained pruning and "
"leave the support of fine-grained pruning in future."
msgstr ""

#: ../../Compression/ModelSpeedup.rst:18
#: ../../Compression/QuantizationSpeedup.rst:21
msgid "Design and Implementation"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:20
msgid ""
"To speed up a model, the pruned layers should be replaced, either "
"replaced with smaller layer for coarse-grained mask, or replaced with "
"sparse kernel for fine-grained mask. Coarse-grained mask usually changes "
"the shape of weights or input/output tensors, thus, we should do shape "
"inference to check are there other unpruned layers should be replaced as "
"well due to shape change. Therefore, in our design, there are two main "
"steps: first, do shape inference to find out all the modules that should "
"be replaced; second, replace the modules. The first step requires "
"topology (i.e., connections) of the model, we use ``jit.trace`` to obtain"
" the model graph for PyTorch."
msgstr ""

#: ../../Compression/ModelSpeedup.rst:22
msgid ""
"For each module, we should prepare four functions, three for shape "
"inference and one for module replacement. The three shape inference "
"functions are: given weight shape infer input/output shape, given input "
"shape infer weight/output shape, given output shape infer weight/input "
"shape. The module replacement function returns a newly created module "
"which is smaller."
msgstr ""

#: ../../Compression/ModelSpeedup.rst:40
msgid ""
"For complete examples please refer to :githublink:`the code "
"<examples/model_compress/pruning/speedup/model_speedup.py>`"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:42
msgid "NOTE: The current implementation supports PyTorch 1.3.1 or newer."
msgstr ""

#: ../../Compression/ModelSpeedup.rst:45
msgid "Limitations"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:47
msgid ""
"Since every module requires four functions for shape inference and module"
" replacement, this is a large amount of work, we only implemented the "
"ones that are required by the examples. If you want to speed up your own "
"model which cannot supported by the current implementation, you are "
"welcome to contribute."
msgstr ""

#: ../../Compression/ModelSpeedup.rst:49
msgid ""
"For PyTorch we can only replace modules, if functions in ``forward`` "
"should be replaced, our current implementation does not work. One "
"workaround is make the function a PyTorch module."
msgstr ""

#: ../../Compression/ModelSpeedup.rst:52
msgid "Speedup Results of Examples"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:54
msgid ""
"The code of these experiments can be found :githublink:`here "
"<examples/model_compress/pruning/speedup/model_speedup.py>`."
msgstr ""

#: ../../Compression/ModelSpeedup.rst:57
msgid "slim pruner example"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:59 ../../Compression/ModelSpeedup.rst:129
#: ../../Compression/ModelSpeedup.rst:162
msgid "on one V100 GPU, input tensor: ``torch.randn(64, 3, 32, 32)``"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:66 ../../Compression/ModelSpeedup.rst:100
#: ../../Compression/ModelSpeedup.rst:136
#: ../../Compression/ModelSpeedup.rst:169
msgid "Times"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:67 ../../Compression/ModelSpeedup.rst:101
#: ../../Compression/ModelSpeedup.rst:137
#: ../../Compression/ModelSpeedup.rst:170
msgid "Mask Latency"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:68 ../../Compression/ModelSpeedup.rst:102
#: ../../Compression/ModelSpeedup.rst:138
#: ../../Compression/ModelSpeedup.rst:171
msgid "Speedup Latency"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:69 ../../Compression/ModelSpeedup.rst:103
#: ../../Compression/ModelSpeedup.rst:139
#: ../../Compression/ModelSpeedup.rst:172
msgid "1"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:70
msgid "0.01197"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:71
msgid "0.005107"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:72 ../../Compression/ModelSpeedup.rst:106
#: ../../Compression/ModelSpeedup.rst:142
#: ../../Compression/ModelSpeedup.rst:175
msgid "2"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:73
msgid "0.02019"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:74
msgid "0.008769"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:75 ../../Compression/ModelSpeedup.rst:109
#: ../../Compression/ModelSpeedup.rst:145
#: ../../Compression/ModelSpeedup.rst:178
msgid "4"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:76
msgid "0.02733"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:77
msgid "0.014809"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:78 ../../Compression/ModelSpeedup.rst:148
#: ../../Compression/ModelSpeedup.rst:181
msgid "8"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:79
msgid "0.04310"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:80
msgid "0.027441"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:81 ../../Compression/ModelSpeedup.rst:151
#: ../../Compression/ModelSpeedup.rst:184
msgid "16"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:82
msgid "0.07731"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:83
msgid "0.05008"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:84 ../../Compression/ModelSpeedup.rst:154
#: ../../Compression/ModelSpeedup.rst:187
msgid "32"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:85
msgid "0.14464"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:86
msgid "0.10027"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:90
msgid "fpgm pruner example"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:92
msgid ""
"on cpu, input tensor: ``torch.randn(64, 1, 28, 28)``\\ , too large "
"variance"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:104
msgid "0.01383"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:105
msgid "0.01839"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:107
msgid "0.01167"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:108
msgid "0.003558"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:110
msgid "0.01636"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:111
msgid "0.01088"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:112
#: ../../Compression/ModelSpeedup.rst:115
#: ../../Compression/ModelSpeedup.rst:118
msgid "40"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:113
msgid "0.14412"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:114
msgid "0.08268"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:116
msgid "1.29385"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:117
msgid "0.14408"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:119
msgid "0.41035"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:120
msgid "0.46162"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:121
msgid "400"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:122
msgid "6.29020"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:123
msgid "5.82143"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:127
msgid "l1filter pruner example"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:140
msgid "0.01026"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:141
msgid "0.003677"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:143
msgid "0.01657"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:144
msgid "0.008161"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:146
msgid "0.02458"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:147
msgid "0.020018"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:149
msgid "0.03498"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:150
msgid "0.025504"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:152
msgid "0.06757"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:153
msgid "0.047523"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:155
msgid "0.10487"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:156
msgid "0.086442"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:160
msgid "APoZ pruner example"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:173
msgid "0.01389"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:174
msgid "0.004208"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:176
msgid "0.01628"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:177
msgid "0.008310"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:179
msgid "0.02521"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:180
msgid "0.014008"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:182
msgid "0.03386"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:183
msgid "0.023923"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:185
msgid "0.06042"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:186
msgid "0.046183"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:188
msgid "0.12421"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:189
msgid "0.087113"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:193
msgid "SimulatedAnnealing pruner example"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:195
msgid ""
"In this experiment, we use SimulatedAnnealing pruner to prune the "
"resnet18 on the cifar10 dataset. We measure the latencies and accuracies "
"of the pruned model under different sparsity ratios, as shown in the "
"following figure. The latency is measured on one V100 GPU and the input "
"tensor is  ``torch.randn(128, 3, 32, 32)``."
msgstr ""

#: ../../Compression/ModelSpeedup.rst:204
msgid "User configuration for ModelSpeedup"
msgstr ""

#: ../../Compression/ModelSpeedup.rst:206 ../../Compression/Pruner.rst:55
#: ../../Compression/Pruner.rst:83 ../../Compression/Pruner.rst:145
#: ../../Compression/Pruner.rst:188 ../../Compression/Pruner.rst:245
#: ../../Compression/Pruner.rst:285 ../../Compression/Pruner.rst:320
#: ../../Compression/Pruner.rst:358 ../../Compression/Pruner.rst:416
#: ../../Compression/Pruner.rst:451 ../../Compression/Pruner.rst:492
#: ../../Compression/Pruner.rst:533 ../../Compression/Pruner.rst:566
#: ../../Compression/Pruner.rst:628 ../../Compression/Pruner.rst:676
#: ../../Compression/Pruner.rst:725 ../../Compression/Pruner.rst:822
#: ../../Compression/v2_pruning_algo.rst:55
#: ../../Compression/v2_pruning_algo.rst:86
#: ../../Compression/v2_pruning_algo.rst:112
#: ../../Compression/v2_pruning_algo.rst:141
#: ../../Compression/v2_pruning_algo.rst:173
#: ../../Compression/v2_pruning_algo.rst:210
#: ../../Compression/v2_pruning_algo.rst:243
#: ../../Compression/v2_pruning_algo.rst:280
#: ../../Compression/v2_pruning_algo.rst:318
#: ../../Compression/v2_pruning_algo.rst:357
#: ../../Compression/v2_pruning_algo.rst:401
#: ../../Compression/v2_pruning_algo.rst:431
#: ../../Compression/v2_pruning_algo.rst:472
#: ../../Compression/v2_pruning_algo.rst:508
#: ../../Compression/v2_pruning_algo.rst:555
#: ../../Compression/v2_pruning_algo.rst:585
msgid "**PyTorch**"
msgstr ""

#: nni.compression.pytorch.speedup.compressor.ModelSpeedup:1 of
msgid "This class is to speedup the model with provided weight mask."
msgstr ""

#: nni.compression.pytorch.speedup.compressor.ModelSpeedup:3 of
msgid "The model user wants to speed up"
msgstr ""

#: nni.compression.pytorch.speedup.compressor.ModelSpeedup:5 of
msgid ""
"Note: The first dimension of the dummy_input should be the batchsize. The"
" dummy input for ```jit.trace```, users should put it on the right "
"device."
msgstr ""

#: nni.compression.pytorch.speedup.compressor.ModelSpeedup:9 of
msgid "The path of user provided mask file, or the mask object"
msgstr ""

#: nni.compression.pytorch.speedup.compressor.ModelSpeedup:11 of
msgid ""
"the device on which masks are placed, same to map_location in "
"```torch.load```"
msgstr ""

#: nni.compression.pytorch.speedup.compressor.ModelSpeedup:13 of
msgid "the index of batch dimension in the dummy_input"
msgstr ""

#: nni.compression.pytorch.speedup.compressor.ModelSpeedup:15 of
msgid "actually used as the batchsize of the dummy_input."
msgstr ""

#: ../../Compression/Overview.rst:2
msgid "Model Compression with NNI"
msgstr ""

#: ../../Compression/Overview.rst:6
msgid ""
"As larger neural networks with more layers and nodes are considered, "
"reducing their storage and computational cost becomes critical, "
"especially for some real-time applications. Model compression can be used"
" to address this problem."
msgstr ""

#: ../../Compression/Overview.rst:8
msgid ""
"NNI provides a model compression toolkit to help user compress and speed "
"up their model with state-of-the-art compression algorithms and "
"strategies. There are several core features supported by NNI model "
"compression:"
msgstr ""

#: ../../Compression/Overview.rst:11
msgid "Support many popular pruning and quantization algorithms."
msgstr ""

#: ../../Compression/Overview.rst:12
msgid ""
"Automate model pruning and quantization process with state-of-the-art "
"strategies and NNI's auto tuning power."
msgstr ""

#: ../../Compression/Overview.rst:13
msgid ""
"Speed up a compressed model to make it have lower inference latency and "
"also make it become smaller."
msgstr ""

#: ../../Compression/Overview.rst:14
msgid ""
"Provide friendly and easy-to-use compression utilities for users to dive "
"into the compression process and results."
msgstr ""

#: ../../Compression/Overview.rst:15
msgid "Concise interface for users to customize their own compression algorithms."
msgstr ""

#: ../../Compression/Overview.rst:19
msgid "Compression Pipeline"
msgstr ""

#: ../../Compression/Overview.rst:25
msgid ""
"The overall compression pipeline in NNI. For compressing a pretrained "
"model, pruning and quantization can be used alone or in combination."
msgstr ""

#: ../../Compression/Overview.rst:28
msgid ""
"Since NNI compression algorithms are not meant to compress model while "
"NNI speedup tool can truly compress model and reduce latency. To obtain a"
" truly compact model, users should conduct `model speedup "
"<./ModelSpeedup.rst>`__. The interface and APIs are unified for both "
"PyTorch and TensorFlow, currently only PyTorch version has been "
"supported, TensorFlow version will be supported in future."
msgstr ""

#: ../../Compression/Overview.rst:31
msgid "Supported Algorithms"
msgstr ""

#: ../../Compression/Overview.rst:33
msgid "The algorithms include pruning algorithms and quantization algorithms."
msgstr ""

#: ../../Compression/Overview.rst:36 ../../Compression/v2_pruning.rst:21
msgid "Pruning Algorithms"
msgstr ""

#: ../../Compression/Overview.rst:38
msgid ""
"Pruning algorithms compress the original network by removing redundant "
"weights or channels of layers, which can reduce model complexity and "
"mitigate the over-fitting issue."
msgstr ""

#: ../../Compression/Overview.rst:44 ../../Compression/Overview.rst:91
msgid "Name"
msgstr ""

#: ../../Compression/Overview.rst:45 ../../Compression/Overview.rst:92
msgid "Brief Introduction of Algorithm"
msgstr ""

#: ../../Compression/Overview.rst:46
msgid "`Level Pruner <Pruner.rst#level-pruner>`__"
msgstr ""

#: ../../Compression/Overview.rst:47
msgid ""
"Pruning the specified ratio on each weight based on absolute values of "
"weights"
msgstr ""

#: ../../Compression/Overview.rst:48
msgid "`AGP Pruner <../Compression/Pruner.rst#agp-pruner>`__"
msgstr ""

#: ../../Compression/Overview.rst:49
msgid ""
"Automated gradual pruning (To prune, or not to prune: exploring the "
"efficacy of pruning for model compression) `Reference Paper "
"<https://arxiv.org/abs/1710.01878>`__"
msgstr ""

#: ../../Compression/Overview.rst:50
msgid ""
"`Lottery Ticket Pruner <../Compression/Pruner.rst#lottery-ticket-"
"hypothesis>`__"
msgstr ""

#: ../../Compression/Overview.rst:51
msgid ""
"The pruning process used by \"The Lottery Ticket Hypothesis: Finding "
"Sparse, Trainable Neural Networks\". It prunes a model iteratively. "
"`Reference Paper <https://arxiv.org/abs/1803.03635>`__"
msgstr ""

#: ../../Compression/Overview.rst:52
msgid "`FPGM Pruner <../Compression/Pruner.rst#fpgm-pruner>`__"
msgstr ""

#: ../../Compression/Overview.rst:53
msgid ""
"Filter Pruning via Geometric Median for Deep Convolutional Neural "
"Networks Acceleration `Reference Paper "
"<https://arxiv.org/pdf/1811.00250.pdf>`__"
msgstr ""

#: ../../Compression/Overview.rst:54
msgid "`L1Filter Pruner <../Compression/Pruner.rst#l1filter-pruner>`__"
msgstr ""

#: ../../Compression/Overview.rst:55
msgid ""
"Pruning filters with the smallest L1 norm of weights in convolution "
"layers (Pruning Filters for Efficient Convnets) `Reference Paper "
"<https://arxiv.org/abs/1608.08710>`__"
msgstr ""

#: ../../Compression/Overview.rst:56
msgid "`L2Filter Pruner <../Compression/Pruner.rst#l2filter-pruner>`__"
msgstr ""

#: ../../Compression/Overview.rst:57
msgid "Pruning filters with the smallest L2 norm of weights in convolution layers"
msgstr ""

#: ../../Compression/Overview.rst:58
msgid ""
"`ActivationAPoZRankFilterPruner <../Compression/Pruner.rst"
"#activationapozrankfilter-pruner>`__"
msgstr ""

#: ../../Compression/Overview.rst:59
msgid ""
"Pruning filters based on the metric APoZ (average percentage of zeros) "
"which measures the percentage of zeros in activations of (convolutional) "
"layers. `Reference Paper <https://arxiv.org/abs/1607.03250>`__"
msgstr ""

#: ../../Compression/Overview.rst:60
msgid ""
"`ActivationMeanRankFilterPruner <../Compression/Pruner.rst"
"#activationmeanrankfilter-pruner>`__"
msgstr ""

#: ../../Compression/Overview.rst:61
msgid ""
"Pruning filters based on the metric that calculates the smallest mean "
"value of output activations"
msgstr ""

#: ../../Compression/Overview.rst:62
msgid "`Slim Pruner <../Compression/Pruner.rst#slim-pruner>`__"
msgstr ""

#: ../../Compression/Overview.rst:63
msgid ""
"Pruning channels in convolution layers by pruning scaling factors in BN "
"layers(Learning Efficient Convolutional Networks through Network "
"Slimming) `Reference Paper <https://arxiv.org/abs/1708.06519>`__"
msgstr ""

#: ../../Compression/Overview.rst:64
msgid ""
"`TaylorFO Pruner <../Compression/Pruner.rst#taylorfoweightfilter-"
"pruner>`__"
msgstr ""

#: ../../Compression/Overview.rst:65
msgid ""
"Pruning filters based on the first order taylor expansion on "
"weights(Importance Estimation for Neural Network Pruning) `Reference "
"Paper "
"<http://jankautz.com/publications/Importance4NNPruning_CVPR19.pdf>`__"
msgstr ""

#: ../../Compression/Overview.rst:66
msgid "`ADMM Pruner <../Compression/Pruner.rst#admm-pruner>`__"
msgstr ""

#: ../../Compression/Overview.rst:67
msgid ""
"Pruning based on ADMM optimization technique `Reference Paper "
"<https://arxiv.org/abs/1804.03294>`__"
msgstr ""

#: ../../Compression/Overview.rst:68
msgid "`NetAdapt Pruner <../Compression/Pruner.rst#netadapt-pruner>`__"
msgstr ""

#: ../../Compression/Overview.rst:69
msgid ""
"Automatically simplify a pretrained network to meet the resource budget "
"by iterative pruning  `Reference Paper "
"<https://arxiv.org/abs/1804.03230>`__"
msgstr ""

#: ../../Compression/Overview.rst:70
msgid ""
"`SimulatedAnnealing Pruner <../Compression/Pruner.rst#simulatedannealing-"
"pruner>`__"
msgstr ""

#: ../../Compression/Overview.rst:71
msgid ""
"Automatic pruning with a guided heuristic search method, Simulated "
"Annealing algorithm `Reference Paper "
"<https://arxiv.org/abs/1907.03141>`__"
msgstr ""

#: ../../Compression/Overview.rst:72
msgid "`AutoCompress Pruner <../Compression/Pruner.rst#autocompress-pruner>`__"
msgstr ""

#: ../../Compression/Overview.rst:73
msgid ""
"Automatic pruning by iteratively call SimulatedAnnealing Pruner and ADMM "
"Pruner `Reference Paper <https://arxiv.org/abs/1907.03141>`__"
msgstr ""

#: ../../Compression/Overview.rst:74
msgid "`AMC Pruner <../Compression/Pruner.rst#amc-pruner>`__"
msgstr ""

#: ../../Compression/Overview.rst:75
msgid ""
"AMC: AutoML for Model Compression and Acceleration on Mobile Devices "
"`Reference Paper <https://arxiv.org/pdf/1802.03494.pdf>`__"
msgstr ""

#: ../../Compression/Overview.rst:76
msgid ""
"`Transformer Head Pruner <../Compression/Pruner.rst#transformer-head-"
"pruner>`__"
msgstr ""

#: ../../Compression/Overview.rst:77
msgid ""
"Pruning attention heads from transformer models either in one shot or "
"iteratively."
msgstr ""

#: ../../Compression/Overview.rst:80
msgid ""
"You can refer to this `benchmark "
"<../CommunitySharings/ModelCompressionComparison.rst>`__ for the "
"performance of these pruners on some benchmark problems."
msgstr ""

#: ../../Compression/Overview.rst:83
msgid "Quantization Algorithms"
msgstr ""

#: ../../Compression/Overview.rst:85
msgid ""
"Quantization algorithms compress the original network by reducing the "
"number of bits required to represent weights or activations, which can "
"reduce the computations and the inference time."
msgstr ""

#: ../../Compression/Overview.rst:93
msgid "`Naive Quantizer <../Compression/Quantizer.rst#naive-quantizer>`__"
msgstr ""

#: ../../Compression/Overview.rst:94
msgid "Quantize weights to default 8 bits"
msgstr ""

#: ../../Compression/Overview.rst:95
msgid "`QAT Quantizer <../Compression/Quantizer.rst#qat-quantizer>`__"
msgstr ""

#: ../../Compression/Overview.rst:96
msgid ""
"Quantization and Training of Neural Networks for Efficient Integer-"
"Arithmetic-Only Inference. `Reference Paper "
"<http://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf>`__"
msgstr ""

#: ../../Compression/Overview.rst:97
msgid "`DoReFa Quantizer <../Compression/Quantizer.rst#dorefa-quantizer>`__"
msgstr ""

#: ../../Compression/Overview.rst:98
msgid ""
"DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low "
"Bitwidth Gradients. `Reference Paper "
"<https://arxiv.org/abs/1606.06160>`__"
msgstr ""

#: ../../Compression/Overview.rst:99
msgid "`BNN Quantizer <../Compression/Quantizer.rst#bnn-quantizer>`__"
msgstr ""

#: ../../Compression/Overview.rst:100
msgid ""
"Binarized Neural Networks: Training Deep Neural Networks with Weights and"
" Activations Constrained to +1 or -1. `Reference Paper "
"<https://arxiv.org/abs/1602.02830>`__"
msgstr ""

#: ../../Compression/Overview.rst:101
msgid "`LSQ Quantizer <../Compression/Quantizer.rst#lsq-quantizer>`__"
msgstr ""

#: ../../Compression/Overview.rst:102
msgid ""
"Learned step size quantization. `Reference Paper "
"<https://arxiv.org/pdf/1902.08153.pdf>`__"
msgstr ""

#: ../../Compression/Overview.rst:103
msgid "`Observer Quantizer <../Compression/Quantizer.rst#observer-quantizer>`__"
msgstr ""

#: ../../Compression/Overview.rst:104
msgid ""
"Post training quantizaiton. Collect quantization information during "
"calibration with observers."
msgstr ""

#: ../../Compression/Overview.rst:110
msgid ""
"The final goal of model compression is to reduce inference latency and "
"model size. However, existing model compression algorithms mainly use "
"simulation to check the performance (e.g., accuracy) of compressed model,"
" for example, using masks for pruning algorithms, and storing quantized "
"values still in float32 for quantization algorithms. Given the output "
"masks and quantization bits produced by those algorithms, NNI can really "
"speed up the model. The detailed tutorial of Masked Model Speedup can be "
"found `here <./ModelSpeedup.rst>`__, The detailed tutorial of Mixed "
"Precision Quantization Model Speedup can be found `here "
"<./QuantizationSpeedup.rst>`__."
msgstr ""

#: ../../Compression/Overview.rst:116
msgid ""
"Compression utilities include some useful tools for users to understand "
"and analyze the model they want to compress. For example, users could "
"check sensitivity of each layer to pruning. Users could easily calculate "
"the FLOPs and parameter size of a model. Please refer to `here "
"<./CompressionUtils.rst>`__ for a complete list of compression utilities."
msgstr ""

#: ../../Compression/Overview.rst:119 ../../Compression/advanced.rst:2
msgid "Advanced Usage"
msgstr ""

#: ../../Compression/Overview.rst:121
msgid ""
"NNI model compression leaves simple interface for users to customize a "
"new compression algorithm. The design philosophy of the interface is "
"making users focus on the compression logic while hiding framework "
"specific implementation details from users. Users can learn more about "
"our compression framework and customize a new compression algorithm "
"(pruning algorithm or quantization algorithm) based on our framework. "
"Moreover, users could leverage NNI's auto tuning power to automatically "
"compress a model. Please refer to `here <./advanced.rst>`__ for more "
"details."
msgstr ""

#: ../../Compression/Overview.rst:125
msgid "Reference and Feedback"
msgstr ""

#: ../../Compression/Overview.rst:127
msgid ""
"To `report a bug <https://github.com/microsoft/nni/issues/new?template"
"=bug-report.rst>`__ for this feature in GitHub;"
msgstr ""

#: ../../Compression/Overview.rst:128
msgid ""
"To `file a feature or improvement request "
"<https://github.com/microsoft/nni/issues/new?template=enhancement.rst>`__"
" for this feature in GitHub;"
msgstr ""

#: ../../Compression/Overview.rst:129
msgid ""
"To know more about `Feature Engineering with NNI "
"<../FeatureEngineering/Overview.rst>`__\\ ;"
msgstr ""

#: ../../Compression/Overview.rst:130
msgid "To know more about `NAS with NNI <../NAS/Overview.rst>`__\\ ;"
msgstr ""

#: ../../Compression/Overview.rst:131
msgid ""
"To know more about `Hyperparameter Tuning with NNI "
"<../Tuner/BuiltinTuner.rst>`__\\ ;"
msgstr ""

#: ../../Compression/Pruner.rst:2
msgid "Supported Pruning Algorithms on NNI"
msgstr ""

#: ../../Compression/Pruner.rst:4
msgid ""
"We provide several pruning algorithms that support fine-grained weight "
"pruning and structural filter pruning. **Fine-grained Pruning** generally"
" results in  unstructured models, which need specialized hardware or "
"software to speed up the sparse network. **Filter Pruning** achieves "
"acceleration by removing the entire filter. Some pruning algorithms use "
"one-shot method that prune weights at once based on an importance metric "
"(It is necessary to finetune the model to compensate for the loss of "
"accuracy). Other pruning algorithms **iteratively** prune weights during "
"optimization, which control the pruning schedule, including some "
"automatic pruning algorithms."
msgstr ""

#: ../../Compression/Pruner.rst:7
msgid "**One-shot Pruning**"
msgstr ""

#: ../../Compression/Pruner.rst:9
msgid "`Level Pruner <#level-pruner>`__ ((fine-grained pruning))"
msgstr ""

#: ../../Compression/Pruner.rst:10 ../../Compression/v2_pruning_algo.rst:17
msgid "`Slim Pruner <#slim-pruner>`__"
msgstr ""

#: ../../Compression/Pruner.rst:11 ../../Compression/v2_pruning_algo.rst:16
msgid "`FPGM Pruner <#fpgm-pruner>`__"
msgstr ""

#: ../../Compression/Pruner.rst:12
msgid "`L1Filter Pruner <#l1filter-pruner>`__"
msgstr ""

#: ../../Compression/Pruner.rst:13
msgid "`L2Filter Pruner <#l2filter-pruner>`__"
msgstr ""

#: ../../Compression/Pruner.rst:14
msgid "`Activation APoZ Rank Filter Pruner <#activationAPoZRankFilter-pruner>`__"
msgstr ""

#: ../../Compression/Pruner.rst:15
msgid "`Activation Mean Rank Filter Pruner <#activationmeanrankfilter-pruner>`__"
msgstr ""

#: ../../Compression/Pruner.rst:16
msgid "`Taylor FO On Weight Pruner <#taylorfoweightfilter-pruner>`__"
msgstr ""

#: ../../Compression/Pruner.rst:18
msgid "**Iteratively Pruning**"
msgstr ""

#: ../../Compression/Pruner.rst:20 ../../Compression/v2_pruning_algo.rst:27
msgid "`AGP Pruner <#agp-pruner>`__"
msgstr ""

#: ../../Compression/Pruner.rst:21
msgid "`NetAdapt Pruner <#netadapt-pruner>`__"
msgstr ""

#: ../../Compression/Pruner.rst:22
msgid "`SimulatedAnnealing Pruner <#simulatedannealing-pruner>`__"
msgstr ""

#: ../../Compression/Pruner.rst:23
msgid "`AutoCompress Pruner <#autocompress-pruner>`__"
msgstr ""

#: ../../Compression/Pruner.rst:24 ../../Compression/v2_pruning_algo.rst:31
msgid "`AMC Pruner <#amc-pruner>`__"
msgstr ""

#: ../../Compression/Pruner.rst:25
msgid "`Sensitivity Pruner <#sensitivity-pruner>`__"
msgstr ""

#: ../../Compression/Pruner.rst:26 ../../Compression/v2_pruning_algo.rst:21
msgid "`ADMM Pruner <#admm-pruner>`__"
msgstr ""

#: ../../Compression/Pruner.rst:28
msgid "**Others**"
msgstr ""

#: ../../Compression/Pruner.rst:30
msgid "`Lottery Ticket Hypothesis <#lottery-ticket-hypothesis>`__"
msgstr ""

#: ../../Compression/Pruner.rst:31
msgid "`Transformer Head Pruner <#transformer-head-pruner>`__"
msgstr ""

#: ../../Compression/Pruner.rst:34 ../../Compression/v2_pruning_algo.rst:34
msgid "Level Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:36
#, python-format
msgid ""
"This is one basic one-shot pruner: you can set a target sparsity level "
"(expressed as a fraction, 0.6 means we will prune 60% of the weight "
"parameters)."
msgstr ""

#: ../../Compression/Pruner.rst:38
msgid ""
"We first sort the weights in the specified layer by their absolute "
"values. And then mask to zero the smallest magnitude weights until the "
"desired sparsity level is reached."
msgstr ""

#: ../../Compression/Pruner.rst:43 ../../Compression/Pruner.rst:71
#: ../../Compression/Pruner.rst:130 ../../Compression/Pruner.rst:176
#: ../../Compression/Pruner.rst:233 ../../Compression/Pruner.rst:266
#: ../../Compression/Pruner.rst:301 ../../Compression/Pruner.rst:343
#: ../../Compression/Pruner.rst:379 ../../Compression/Pruner.rst:434
#: ../../Compression/Pruner.rst:475 ../../Compression/Pruner.rst:513
#: ../../Compression/Pruner.rst:550 ../../Compression/Pruner.rst:606
#: ../../Compression/Pruner.rst:653 ../../Compression/Pruner.rst:709
#: ../../Compression/Quantizer.rst:49 ../../Compression/Quantizer.rst:186
#: ../../Compression/Quantizer.rst:231 ../../Compression/Quantizer.rst:268
#: ../../Compression/Quantizer.rst:332
#: ../../Compression/v2_pruning_algo.rst:571
msgid "PyTorch code"
msgstr ""

#: ../../Compression/Pruner.rst:53 ../../Compression/v2_pruning_algo.rst:53
msgid "User configuration for Level Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:59
msgid "**TensorFlow**"
msgstr ""

#: ../../Compression/Pruner.rst:65 ../../Compression/v2_pruning_algo.rst:146
msgid "Slim Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:66
msgid ""
"This is an one-shot pruner, which adds sparsity regularization on the "
"scaling factors of batch normalization (BN) layers during training to "
"identify unimportant channels. The channels with small scaling factor "
"values will be pruned. For more details, please refer to `'Learning "
"Efficient Convolutional Networks through Network Slimming' "
"<https://arxiv.org/pdf/1708.06519.pdf>`__\\."
msgstr ""

#: ../../Compression/Pruner.rst:81 ../../Compression/v2_pruning_algo.rst:171
msgid "User configuration for Slim Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:88 ../../Compression/Pruner.rst:193
#: ../../Compression/Pruner.rst:571 ../../Compression/Pruner.rst:681
#: ../../Compression/v2_pruning_algo.rst:362
msgid "Reproduced Experiment"
msgstr ""

#: ../../Compression/Pruner.rst:90
msgid ""
"We implemented one of the experiments in `Learning Efficient "
"Convolutional Networks through Network Slimming "
"<https://arxiv.org/pdf/1708.06519.pdf>`__\\ , we pruned ``70%`` channels "
"in the **VGGNet** for CIFAR-10 in the paper, in which ``88.5%`` "
"parameters are pruned. Our experiments results are as follows:"
msgstr ""

#: ../../Compression/Pruner.rst:96 ../../Compression/Pruner.rst:201
#: ../../Compression/Pruner.rst:579 ../../Compression/Quantizer.rst:308
#: ../../Compression/v2_pruning_algo.rst:368
msgid "Model"
msgstr ""

#: ../../Compression/Pruner.rst:97 ../../Compression/Pruner.rst:202
msgid "Error(paper/ours)"
msgstr ""

#: ../../Compression/Pruner.rst:99 ../../Compression/Pruner.rst:204
msgid "Pruned"
msgstr ""

#: ../../Compression/Pruner.rst:100 ../../Compression/Quantizer.rst:310
msgid "VGGNet"
msgstr ""

#: ../../Compression/Pruner.rst:101
msgid "6.34/6.69"
msgstr ""

#: ../../Compression/Pruner.rst:102
msgid "20.04M"
msgstr ""

#: ../../Compression/Pruner.rst:104
msgid "Pruned-VGGNet"
msgstr ""

#: ../../Compression/Pruner.rst:105
msgid "6.20/6.34"
msgstr ""

#: ../../Compression/Pruner.rst:106
msgid "2.03M"
msgstr ""

#: ../../Compression/Pruner.rst:107
msgid "88.5%"
msgstr ""

#: ../../Compression/Pruner.rst:110 ../../Compression/Pruner.rst:215
msgid ""
"The experiments code can be found at "
":githublink:`examples/model_compress/pruning/basic_pruners_torch.py "
"<examples/model_compress/pruning/basic_pruners_torch.py>`"
msgstr ""

#: ../../Compression/Pruner.rst:120 ../../Compression/v2_pruning_algo.rst:117
msgid "FPGM Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:122
msgid ""
"This is an one-shot pruner, which prunes filters with the smallest "
"geometric median. FPGM chooses the filters with the most replaceable "
"contribution. For more details, please refer to `Filter Pruning via "
"Geometric Median for Deep Convolutional Neural Networks Acceleration "
"<https://arxiv.org/pdf/1811.00250.pdf>`__."
msgstr ""

#: ../../Compression/Pruner.rst:125 ../../Compression/Pruner.rst:228
#: ../../Compression/Pruner.rst:261 ../../Compression/Pruner.rst:296
#: ../../Compression/Pruner.rst:336
msgid ""
"We also provide a dependency-aware mode for this pruner to get better "
"speedup from the pruning. Please reference `dependency-aware "
"<./DependencyAware.rst>`__ for more details."
msgstr ""

#: ../../Compression/Pruner.rst:143 ../../Compression/v2_pruning_algo.rst:139
msgid "User configuration for FPGM Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:150
msgid "L1Filter Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:152
msgid ""
"This is an one-shot pruner, which prunes the filters in the **convolution"
" layers**."
msgstr ""

#: ../../Compression/Pruner.rst:167 ../../Compression/v2_pruning_algo.rst:67
msgid ""
"For more details, please refer to `PRUNING FILTERS FOR EFFICIENT CONVNETS"
" <https://arxiv.org/abs/1608.08710>`__\\."
msgstr ""

#: ../../Compression/Pruner.rst:171
msgid ""
"In addition, we also provide a dependency-aware mode for the "
"L1FilterPruner. For more details about the dependency-aware mode, please "
"reference `dependency-aware mode <./DependencyAware.rst>`__."
msgstr ""

#: ../../Compression/Pruner.rst:186
msgid "User configuration for L1Filter Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:195
msgid ""
"We implemented one of the experiments in `PRUNING FILTERS FOR EFFICIENT "
"CONVNETS <https://arxiv.org/abs/1608.08710>`__ with **L1FilterPruner**\\ "
", we pruned **VGG-16** for CIFAR-10 to **VGG-16-pruned-A** in the paper, "
"in which ``64%`` parameters are pruned. Our experiments results are as "
"follows:"
msgstr ""

#: ../../Compression/Pruner.rst:205
msgid "VGG-16"
msgstr ""

#: ../../Compression/Pruner.rst:206
msgid "6.75/6.49"
msgstr ""

#: ../../Compression/Pruner.rst:207
msgid "1.5x10^7"
msgstr ""

#: ../../Compression/Pruner.rst:209
msgid "VGG-16-pruned-A"
msgstr ""

#: ../../Compression/Pruner.rst:210
msgid "6.60/6.47"
msgstr ""

#: ../../Compression/Pruner.rst:211
msgid "5.4x10^6"
msgstr ""

#: ../../Compression/Pruner.rst:212
msgid "64.0%"
msgstr ""

#: ../../Compression/Pruner.rst:224
msgid "L2Filter Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:226
msgid ""
"This is a structured pruning algorithm that prunes the filters with the "
"smallest L2 norm of the weights. It is implemented as a one-shot pruner."
msgstr ""

#: ../../Compression/Pruner.rst:243
msgid "User configuration for L2Filter Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:252
msgid "ActivationAPoZRankFilter Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:254
msgid ""
"ActivationAPoZRankFilter Pruner is a pruner which prunes the filters with"
" the smallest importance criterion ``APoZ`` calculated from the output "
"activations of convolution layers to achieve a preset level of network "
"sparsity. The pruning criterion ``APoZ`` is explained in the paper "
"`Network Trimming: A Data-Driven Neuron Pruning Approach towards "
"Efficient Deep Architectures <https://arxiv.org/abs/1607.03250>`__."
msgstr ""

#: ../../Compression/Pruner.rst:256 ../../Compression/v2_pruning_algo.rst:184
msgid "The APoZ is defined as:"
msgstr ""

#: ../../Compression/Pruner.rst:258 ../../Compression/v2_pruning_algo.rst:186
msgid ""
":math:`APoZ_{c}^{(i)} = "
"APoZ\\left(O_{c}^{(i)}\\right)=\\frac{\\sum_{k}^{N} \\sum_{j}^{M} "
"f\\left(O_{c, j}^{(i)}(k)=0\\right)}{N \\times M}`"
msgstr ""

#: ../../Compression/Pruner.rst:278
msgid ""
"Note: ActivationAPoZRankFilterPruner is used to prune convolutional "
"layers within deep neural networks, therefore the ``op_types`` field "
"supports only convolutional layers."
msgstr ""

#: ../../Compression/Pruner.rst:280 ../../Compression/Pruner.rst:315
msgid ""
"You can view :githublink:`example "
"<examples/model_compress/pruning/basic_pruners_torch.py>` for more "
"information."
msgstr ""

#: ../../Compression/Pruner.rst:283
msgid "User configuration for ActivationAPoZRankFilter Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:292
msgid "ActivationMeanRankFilter Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:294
msgid ""
"ActivationMeanRankFilterPruner is a pruner which prunes the filters with "
"the smallest importance criterion ``mean activation`` calculated from the"
" output activations of convolution layers to achieve a preset level of "
"network sparsity. The pruning criterion ``mean activation`` is explained "
"in section 2.2 of the paper `Pruning Convolutional Neural Networks for "
"Resource Efficient Inference <https://arxiv.org/abs/1611.06440>`__. Other"
" pruning criteria mentioned in this paper will be supported in future "
"release."
msgstr ""

#: ../../Compression/Pruner.rst:313
msgid ""
"Note: ActivationMeanRankFilterPruner is used to prune convolutional "
"layers within deep neural networks, therefore the ``op_types`` field "
"supports only convolutional layers."
msgstr ""

#: ../../Compression/Pruner.rst:318
msgid "User configuration for ActivationMeanRankFilterPruner"
msgstr ""

#: ../../Compression/Pruner.rst:327
msgid "TaylorFOWeightFilter Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:329
msgid ""
"TaylorFOWeightFilter Pruner is a pruner which prunes convolutional layers"
" based on estimated importance calculated from the first order taylor "
"expansion on weights to achieve a preset level of network sparsity. The "
"estimated importance of filters is defined as the paper `Importance "
"Estimation for Neural Network Pruning "
"<http://jankautz.com/publications/Importance4NNPruning_CVPR19.pdf>`__. "
"Other pruning criteria mentioned in this paper will be supported in "
"future release."
msgstr ""

#: ../../Compression/Pruner.rst:333 ../../Compression/v2_pruning_algo.rst:254
msgid ""
":math:`\\widehat{\\mathcal{I}}_{\\mathcal{S}}^{(1)}(\\mathbf{W}) "
"\\triangleq \\sum_{s \\in \\mathcal{S}} "
"\\mathcal{I}_{s}^{(1)}(\\mathbf{W})=\\sum_{s \\in "
"\\mathcal{S}}\\left(g_{s} w_{s}\\right)^{2}`"
msgstr ""

#: ../../Compression/Pruner.rst:338
msgid ""
"What's more, we provide a global-sort mode for this pruner which is "
"aligned with paper implementation. Please set parameter 'global_sort' to "
"True when instantiate TaylorFOWeightFilterPruner."
msgstr ""

#: ../../Compression/Pruner.rst:356
msgid "User configuration for TaylorFOWeightFilter Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:365 ../../Compression/v2_pruning_algo.rst:406
msgid "AGP Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:367
msgid ""
"This is an iterative pruner, which the sparsity is increased from an "
"initial sparsity value si (usually 0) to a final sparsity value sf over a"
" span of n pruning steps, starting at training step :math:`t_{0}` and "
"with pruning frequency :math:`\\Delta t`:"
msgstr ""

#: ../../Compression/Pruner.rst:369 ../../Compression/v2_pruning_algo.rst:411
msgid ""
":math:`s_{t}=s_{f}+\\left(s_{i}-s_{f}\\right)\\left(1-\\frac{t-t_{0}}{n "
"\\Delta t}\\right)^{3} \\text { for } t \\in\\left\\{t_{0}, t_{0}+\\Delta"
" t, \\ldots, t_{0} + n \\Delta t\\right\\}`"
msgstr ""

#: ../../Compression/Pruner.rst:371 ../../Compression/v2_pruning_algo.rst:413
msgid ""
"For more details please refer to `To prune, or not to prune: exploring "
"the efficacy of pruning for model compression "
"<https://arxiv.org/abs/1710.01878>`__\\."
msgstr ""

#: ../../Compression/Pruner.rst:377
#, python-format
msgid ""
"You can prune all weights from 0% to 80% sparsity in 10 epoch with the "
"code below."
msgstr ""

#: ../../Compression/Pruner.rst:400
msgid ""
"AGP pruner uses ``LevelPruner`` algorithms to prune the weight by "
"default, however you can set ``pruning_algorithm`` parameter to other "
"values to use other pruning algorithms:"
msgstr ""

#: ../../Compression/Pruner.rst:403
msgid "``level``\\ : LevelPruner"
msgstr ""

#: ../../Compression/Pruner.rst:404
msgid "``slim``\\ : SlimPruner"
msgstr ""

#: ../../Compression/Pruner.rst:405
msgid "``l1``\\ : L1FilterPruner"
msgstr ""

#: ../../Compression/Pruner.rst:406
msgid "``l2``\\ : L2FilterPruner"
msgstr ""

#: ../../Compression/Pruner.rst:407
msgid "``fpgm``\\ : FPGMPruner"
msgstr ""

#: ../../Compression/Pruner.rst:408
msgid "``taylorfo``\\ : TaylorFOWeightFilterPruner"
msgstr ""

#: ../../Compression/Pruner.rst:409
msgid "``apoz``\\ : ActivationAPoZRankFilterPruner"
msgstr ""

#: ../../Compression/Pruner.rst:410
msgid "``mean_activation``\\ : ActivationMeanRankFilterPruner"
msgstr ""

#: ../../Compression/Pruner.rst:414 ../../Compression/v2_pruning_algo.rst:429
msgid "User configuration for AGP Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:423
msgid "NetAdapt Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:425
msgid ""
"NetAdapt allows a user to automatically simplify a pretrained network to "
"meet the resource budget. Given the overall sparsity, NetAdapt will "
"automatically generate the sparsities distribution among different layers"
" by iterative pruning."
msgstr ""

#: ../../Compression/Pruner.rst:428
msgid ""
"For more details, please refer to `NetAdapt: Platform-Aware Neural "
"Network Adaptation for Mobile Applications "
"<https://arxiv.org/abs/1804.03230>`__."
msgstr ""

#: ../../Compression/Pruner.rst:446 ../../Compression/Pruner.rst:487
#: ../../Compression/Pruner.rst:528 ../../Compression/Pruner.rst:623
msgid ""
"You can view :githublink:`example "
"<examples/model_compress/pruning/auto_pruners_torch.py>` for more "
"information."
msgstr ""

#: ../../Compression/Pruner.rst:449
msgid "User configuration for NetAdapt Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:456
msgid "SimulatedAnnealing Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:458
msgid ""
"We implement a guided heuristic search method, Simulated Annealing (SA) "
"algorithm, with enhancement on guided search based on prior experience. "
"The enhanced SA technique is based on the observation that a DNN layer "
"with more number of weights often has a higher degree of model "
"compression with less impact on overall accuracy."
msgstr ""

#: ../../Compression/Pruner.rst:462 ../../Compression/v2_pruning_algo.rst:482
msgid "Randomly initialize a pruning rate distribution (sparsities)."
msgstr ""

#: ../../Compression/Pruner.rst:463 ../../Compression/v2_pruning_algo.rst:483
msgid "While current_temperature < stop_temperature:"
msgstr ""

#: ../../Compression/Pruner.rst:465 ../../Compression/v2_pruning_algo.rst:485
msgid "generate a perturbation to current distribution"
msgstr ""

#: ../../Compression/Pruner.rst:466 ../../Compression/v2_pruning_algo.rst:486
msgid "Perform fast evaluation on the perturbated distribution"
msgstr ""

#: ../../Compression/Pruner.rst:467 ../../Compression/v2_pruning_algo.rst:487
msgid ""
"accept the perturbation according to the performance and probability, if "
"not accepted, return to step 1"
msgstr ""

#: ../../Compression/Pruner.rst:468 ../../Compression/v2_pruning_algo.rst:488
msgid "cool down, current_temperature <- current_temperature * cool_down_rate"
msgstr ""

#: ../../Compression/Pruner.rst:470 ../../Compression/Pruner.rst:508
#: ../../Compression/v2_pruning_algo.rst:490
#: ../../Compression/v2_pruning_algo.rst:522
msgid ""
"For more details, please refer to `AutoCompress: An Automatic DNN "
"Structured Pruning Framework for Ultra-High Compression Rates "
"<https://arxiv.org/abs/1907.03141>`__."
msgstr ""

#: ../../Compression/Pruner.rst:490
msgid "User configuration for SimulatedAnnealing Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:497
msgid "AutoCompress Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:499
msgid ""
"For each round, AutoCompressPruner prune the model for the same sparsity "
"to achive the overall sparsity:"
msgstr ""

#: ../../Compression/Pruner.rst:531
msgid "User configuration for AutoCompress Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:538 ../../Compression/v2_pruning_algo.rst:560
msgid "AMC Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:540
msgid ""
"AMC pruner leverages reinforcement learning to provide the model "
"compression policy. This learning-based compression policy outperforms "
"conventional rule-based compression policy by having higher compression "
"ratio, better preserving the accuracy and freeing human labor."
msgstr ""

#: ../../Compression/Pruner.rst:545 ../../Compression/v2_pruning_algo.rst:566
msgid ""
"For more details, please refer to `AMC: AutoML for Model Compression and "
"Acceleration on Mobile Devices <https://arxiv.org/pdf/1802.03494.pdf>`__."
msgstr ""

#: ../../Compression/Pruner.rst:561
msgid ""
"You can view :githublink:`example <examples/model_compress/pruning/amc/>`"
" for more information."
msgstr ""

#: ../../Compression/Pruner.rst:564 ../../Compression/v2_pruning_algo.rst:583
msgid "User configuration for AMC Pruner"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:1 of
msgid ""
"A pytorch implementation of AMC: AutoML for Model Compression and "
"Acceleration on Mobile Devices. (https://arxiv.org/pdf/1802.03494.pdf)"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:4 of
msgid "nn.Module The model to be pruned."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:6 of
msgid ""
"list Configuration list to configure layer pruning. Supported keys: - "
"op_types: operation type to be pruned - op_names: operation name to be "
"pruned"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:11 of
msgid ""
"function function to evaluate the pruned model. The prototype of the "
"function: >>> def evaluator(val_loader, model): >>>     ... >>>     "
"return acc"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:17 of
msgid "torch.utils.data.DataLoader Data loader of validation dataset."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:19 of
msgid "str suffix to help you remember what experiment you ran. Default: None."
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:22 of
msgid ""
"str model type to prune, currently 'mobilenet' and 'mobilenetv2' are "
"supported. Default: mobilenet"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:24 of
msgid "float preserve flops ratio. Default: 0.5"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:26 of
msgid "float minimum weight preserve ratio for each layer. Default: 0.2"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:28 of
msgid "float maximum weight preserve ratio for each layer. Default: 1.0"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:30 of
msgid ""
"function reward function type: - acc_reward: accuracy * 0.01 - "
"acc_flops_reward: - (100 - accuracy) * 0.01 * np.log(flops) Default: "
"acc_reward"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:36 of
msgid "int number of batches to extract layer information. Default: 60"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:38 of
msgid "int number of feature points per layer. Default: 10"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:40 of
msgid "int round channel to multiple of channel_round. Default: 8"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:43 of
msgid "int hidden num of first fully connect layer. Default: 300"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:45 of
msgid "int hidden num of second fully connect layer. Default: 300"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:47 of
msgid "float learning rate for critic. Default: 1e-3"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:49 of
msgid "float learning rate for actor. Default: 1e-4"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:51 of
msgid ""
"int number of episodes without training but only filling the replay "
"memory. During warmup episodes, random actions ares used for pruning. "
"Default: 100"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:54 of
msgid "float next Q value discount for deep Q value target. Default: 0.99"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:56 of
msgid "int minibatch size for training DDPG agent. Default: 64"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:58 of
msgid "int memory size for each layer. Default: 100"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:60 of
msgid "int replay buffer window length. Default: 1"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:62 of
msgid ""
"float moving average for target network being used by soft_update. "
"Default: 0.99"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:65 of
msgid "float initial variance of truncated normal distribution"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:67 of
msgid "float delta decay during exploration"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:70 of
msgid "int maximum episode length"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:72 of
msgid "str output directory to save log files and model files. Default: ./logs"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:74 of
msgid "boolean debug mode"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:76 of
msgid "int train iters each timestep. Default: 800"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:78 of
msgid "int linear decay of exploration policy. Default: 50000"
msgstr ""

#: nni.algorithms.compression.pytorch.pruning.amc.amc_pruner.AMCPruner:80 of
msgid "int random seed to set for reproduce experiment. Default: None"
msgstr ""

#: ../../Compression/Pruner.rst:573
#, python-format
msgid ""
"We implemented one of the experiments in `AMC: AutoML for Model "
"Compression and Acceleration on Mobile Devices "
"<https://arxiv.org/pdf/1802.03494.pdf>`__\\ , we pruned **MobileNet** to "
"50% FLOPS for ImageNet in the paper. Our experiments results are as "
"follows:"
msgstr ""

#: ../../Compression/Pruner.rst:580
msgid "Top 1 acc.(paper/ours)"
msgstr ""

#: ../../Compression/Pruner.rst:581
msgid "Top 5 acc. (paper/ours)"
msgstr ""

#: ../../Compression/Pruner.rst:582
msgid "FLOPS"
msgstr ""

#: ../../Compression/Pruner.rst:583
msgid "MobileNet"
msgstr ""

#: ../../Compression/Pruner.rst:584
msgid "70.5% / 69.9%"
msgstr ""

#: ../../Compression/Pruner.rst:585
msgid "89.3% / 89.1%"
msgstr ""

#: ../../Compression/Pruner.rst:586
msgid "50%"
msgstr ""

#: ../../Compression/Pruner.rst:589
msgid ""
"The experiments code can be found at "
":githublink:`examples/model_compress/pruning/ "
"<examples/model_compress/pruning/amc/>`"
msgstr ""

#: ../../Compression/Pruner.rst:592 ../../Compression/v2_pruning_algo.rst:285
msgid "ADMM Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:594 ../../Compression/v2_pruning_algo.rst:287
msgid ""
"Alternating Direction Method of Multipliers (ADMM) is a mathematical "
"optimization technique, by decomposing the original nonconvex problem "
"into two subproblems that can be solved iteratively. In weight pruning "
"problem, these two subproblems are solved via 1) gradient descent "
"algorithm and 2) Euclidean projection respectively."
msgstr ""

#: ../../Compression/Pruner.rst:597
msgid ""
"During the process of solving these two subproblems, the weights of the "
"original model will be changed. An one-shot pruner will then be applied "
"to prune the model according to the config list given."
msgstr ""

#: ../../Compression/Pruner.rst:599 ../../Compression/v2_pruning_algo.rst:294
msgid ""
"This solution framework applies both to non-structured and different "
"variations of structured pruning schemes."
msgstr ""

#: ../../Compression/Pruner.rst:601 ../../Compression/v2_pruning_algo.rst:296
msgid ""
"For more details, please refer to `A Systematic DNN Weight Pruning "
"Framework using Alternating Direction Method of Multipliers "
"<https://arxiv.org/abs/1804.03294>`__."
msgstr ""

#: ../../Compression/Pruner.rst:626 ../../Compression/v2_pruning_algo.rst:316
msgid "User configuration for ADMM Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:633
msgid "Lottery Ticket Hypothesis"
msgstr ""

#: ../../Compression/Pruner.rst:635 ../../Compression/v2_pruning_algo.rst:438
msgid ""
"`The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"
" <https://arxiv.org/abs/1803.03635>`__\\ , authors Jonathan Frankle and "
"Michael Carbin,provides comprehensive measurement and analysis, and "
"articulate the *lottery ticket hypothesis*\\ : dense, randomly-"
"initialized, feed-forward networks contain subnetworks (*winning "
"tickets*\\ ) that -- when trained in isolation -- reach test accuracy "
"comparable to the original network in a similar number of iterations."
msgstr ""

#: ../../Compression/Pruner.rst:637 ../../Compression/v2_pruning_algo.rst:443
msgid ""
"In this paper, the authors use the following process to prune a model, "
"called *iterative prunning*\\ :"
msgstr ""

#: ../../Compression/Pruner.rst:641 ../../Compression/v2_pruning_algo.rst:447
msgid ""
"Randomly initialize a neural network f(x;theta_0) (where theta\\ *0 "
"follows D*\\ {theta})."
msgstr ""

#: ../../Compression/Pruner.rst:642 ../../Compression/v2_pruning_algo.rst:448
msgid "Train the network for j iterations, arriving at parameters theta_j."
msgstr ""

#: ../../Compression/Pruner.rst:643 ../../Compression/v2_pruning_algo.rst:449
#, python-format
msgid "Prune p% of the parameters in theta_j, creating a mask m."
msgstr ""

#: ../../Compression/Pruner.rst:644 ../../Compression/v2_pruning_algo.rst:450
msgid ""
"Reset the remaining parameters to their values in theta_0, creating the "
"winning ticket f(x;m*theta_0)."
msgstr ""

#: ../../Compression/Pruner.rst:645 ../../Compression/v2_pruning_algo.rst:451
msgid "Repeat step 2, 3, and 4."
msgstr ""

#: ../../Compression/Pruner.rst:648 ../../Compression/v2_pruning_algo.rst:453
msgid ""
"If the configured final sparsity is P (e.g., 0.8) and there are n times "
"iterative pruning, each iterative pruning prunes 1-(1-P)^(1/n) of the "
"weights that survive the previous round."
msgstr ""

#: ../../Compression/Pruner.rst:670
msgid ""
"The above configuration means that there are 5 times of iterative "
"pruning. As the 5 times iterative pruning are executed in the same run, "
"LotteryTicketPruner needs ``model`` and ``optimizer`` (\\ **Note that "
"should add ``lr_scheduler`` if used**\\ ) to reset their states every "
"time a new prune iteration starts. Please use ``get_prune_iterations`` to"
" get the pruning iterations, and invoke ``prune_iteration_start`` at the "
"beginning of each iteration. ``epoch_num`` is better to be large enough "
"for model convergence, because the hypothesis is that the performance "
"(accuracy) got in latter rounds with high sparsity could be comparable "
"with that got in the first round."
msgstr ""

#: ../../Compression/Pruner.rst:674
msgid "User configuration for LotteryTicket Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:683
msgid ""
"We try to reproduce the experiment result of the fully connected network "
"on MNIST using the same configuration as in the paper. The code can be "
"referred :githublink:`here "
"<examples/model_compress/pruning/lottery_torch_mnist_fc.py>`. In this "
"experiment, we prune 10 times, for each pruning we train the pruned model"
" for 50 epochs."
msgstr ""

#: ../../Compression/Pruner.rst:691
msgid ""
"The above figure shows the result of the fully connected network. "
"``round0-sparsity-0.0`` is the performance without pruning. Consistent "
"with the paper, pruning around 80% also obtain similar performance "
"compared to non-pruning, and converges a little faster. If pruning too "
"much, e.g., larger than 94%, the accuracy becomes lower and convergence "
"becomes a little slower. A little different from the paper, the trend of "
"the data in the paper is relatively more clear."
msgstr ""

#: ../../Compression/Pruner.rst:694
msgid "Sensitivity Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:696
msgid ""
"For each round, SensitivityPruner prunes the model based on the "
"sensitivity to the accuracy of each layer until meeting the final "
"configured sparsity of the whole model:"
msgstr ""

#: ../../Compression/Pruner.rst:704
msgid ""
"For more details, please refer to `Learning both Weights and Connections "
"for Efficient Neural Networks  <https://arxiv.org/abs/1506.02626>`__."
msgstr ""

#: ../../Compression/Pruner.rst:723
msgid "User configuration for Sensitivity Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:730
msgid "Transformer Head Pruner"
msgstr ""

#: ../../Compression/Pruner.rst:732
msgid ""
"Transformer Head Pruner is a tool designed for pruning attention heads "
"from the models belonging to the `Transformer family "
"<https://proceedings.neurips.cc/paper/2017/file"
"/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>`__. The following image from"
" `Efficient Transformers: A Survey "
"<https://arxiv.org/pdf/2009.06732.pdf>`__ gives a good overview the "
"general structure of the Transformer."
msgstr ""

#: ../../Compression/Pruner.rst:738
msgid ""
"Typically, each attention layer in the Transformer models consists of "
"four weights: three projection matrices for query, key, value, and an "
"output projection matrix. The outputs of the former three matrices "
"contains the projected results for all heads. Normally, the results are "
"then reshaped so that each head performs that attention computation "
"independently. The final results are concatenated back before fed into "
"the output projection. Therefore, when an attention head is pruned, the "
"same weights corresponding to that heads in the three projection matrices"
" are pruned. Also, the weights in the output projection corresponding to "
"the head's output are pruned. In our implementation, we calculate and "
"apply masks to the four matrices together."
msgstr ""

#: ../../Compression/Pruner.rst:740
msgid ""
"Note: currently, the pruner can only handle models with projection "
"weights written as separate ``Linear`` modules, i.e., it expects four "
"``Linear`` modules corresponding to query, key, value, and an output "
"projections. Therefore, in the ``config_list``, you should either write "
"``['Linear']`` for the ``op_types`` field, or write names corresponding "
"to ``Linear`` modules for the ``op_names`` field. For instance, the "
"`Huggingface transformers "
"<https://huggingface.co/transformers/index.html>`_ are supported, but "
"``torch.nn.Transformer`` is not."
msgstr ""

#: ../../Compression/Pruner.rst:742
msgid "The pruner implements the following algorithm:"
msgstr ""

#: ../../Compression/Pruner.rst:751
msgid "Currently, the following head sorting criteria are supported:"
msgstr ""

#: ../../Compression/Pruner.rst:753
msgid ""
"\"l1_weight\": rank heads by the L1-norm of weights of the query, key, "
"and value projection matrices."
msgstr ""

#: ../../Compression/Pruner.rst:754
msgid ""
"\"l2_weight\": rank heads by the L2-norm of weights of the query, key, "
"and value projection matrices."
msgstr ""

#: ../../Compression/Pruner.rst:755
msgid ""
"\"l1_activation\": rank heads by the L1-norm of their attention "
"computation output."
msgstr ""

#: ../../Compression/Pruner.rst:756
msgid ""
"\"l2_activation\": rank heads by the L2-norm of their attention "
"computation output."
msgstr ""

#: ../../Compression/Pruner.rst:757
msgid ""
"\"taylorfo\": rank heads by l1 norm of the output of attention "
"computation * gradient for this output. Check more details in `this paper"
" <https://arxiv.org/abs/1905.10650>`__ and `this one "
"<https://arxiv.org/abs/1611.06440>`__."
msgstr ""

#: ../../Compression/Pruner.rst:759
msgid ""
"We support local sorting (i.e., sorting heads within a layer) and global "
"sorting (sorting all heads together), and you can control by setting the "
"``global_sort`` parameter. Note that if ``global_sort=True`` is passed, "
"all weights must have the same sparsity in the config list. However, this"
" does not mean that each layer will be prune to the same sparsity as "
"specified. This sparsity value will be interpreted as a global sparsity, "
"and each layer is likely to have different sparsity after pruning by "
"global sort. As a reminder, we found that if global sorting is used, it "
"is usually helpful to use an iterative pruning scheme, interleaving "
"pruning with intermediate finetuning, since global sorting often results "
"in non-uniform sparsity distributions, which makes the model more "
"susceptible to forgetting."
msgstr ""

#: ../../Compression/Pruner.rst:761
msgid ""
"In our implementation, we support two ways to group the four weights in "
"the same layer together. You can either pass a nested list containing the"
" names of these modules as the pruner's initialization parameters (usage "
"below), or simply pass a dummy input instead and the pruner will run "
"``torch.jit.trace`` to group the weights (experimental feature). However,"
" if you would like to assign different sparsity to each layer, you can "
"only use the first option, i.e., passing names of the weights to the "
"pruner (see usage below). Also, note that we require the weights "
"belonging to the same layer to have the same sparsity."
msgstr ""

#: ../../Compression/Pruner.rst:766
msgid ""
"Suppose we want to prune a BERT with Huggingface implementation, which "
"has the following architecture (obtained by calling ``print(model)``). "
"Note that we only show the first layer of the repeated layers in the "
"encoder's ``ModuleList layer``."
msgstr ""

#: ../../Compression/Pruner.rst:772
msgid ""
"**Usage Example: one-shot pruning, assigning sparsity 0.5 to the first "
"six layers and sparsity 0.25 to the last six layers (PyTorch code)**. "
"Note that"
msgstr ""

#: ../../Compression/Pruner.rst:774
msgid ""
"Here we specify ``op_names`` in the config list to assign different "
"sparsity to different layers."
msgstr ""

#: ../../Compression/Pruner.rst:775
msgid ""
"Meanwhile, we pass ``attention_name_groups`` to the pruner so that the "
"pruner may group together the weights belonging to the same attention "
"layer."
msgstr ""

#: ../../Compression/Pruner.rst:776
msgid ""
"Since in this example we want to do one-shot pruning, the "
"``num_iterations`` parameter is set to 1, and the parameter "
"``epochs_per_iteration`` is ignored. If you would like to do iterative "
"pruning instead, you can set the ``num_iterations`` parameter to the "
"number of pruning iterations, and the ``epochs_per_iteration`` parameter "
"to the number of finetuning epochs between two iterations."
msgstr ""

#: ../../Compression/Pruner.rst:777
msgid ""
"The arguments ``trainer`` and ``optimizer`` are only used when we want to"
" do iterative pruning, or the ranking criterion is ``taylorfo``. Here "
"these two parameters are ignored by the pruner."
msgstr ""

#: ../../Compression/Pruner.rst:778
msgid ""
"The argument ``forward_runner`` is only used when the ranking criterion "
"is ``l1_activation`` or ``l2_activation``. Here this parameter is ignored"
" by the pruner."
msgstr ""

#: ../../Compression/Pruner.rst:811
msgid ""
"In addition to this usage guide, we provide a more detailed example of "
"pruning BERT (Huggingface implementation) for transfer learning on the "
"tasks from the `GLUE benchmark <https://gluebenchmark.com/>`_. Please "
"find it in this :githublink:`page "
"<examples/model_compress/pruning/transformers>`. To run the example, "
"first make sure that you install the package ``transformers`` and "
"``datasets``. Then, you may start by running the following command:"
msgstr ""

#: ../../Compression/Pruner.rst:817
#, python-format
msgid ""
"By default, the code will download a pretrained BERT language model, and "
"then finetune for several epochs on the downstream GLUE task. Then, the "
"``TransformerHeadPruner`` will be used to prune out heads from each layer"
" by a certain criterion (by default, the code lets the pruner uses "
"magnitude ranking, and prunes out 50% of the heads in each layer in an "
"one-shot manner). Finally, the pruned model will be finetuned in the "
"downstream task for several epochs. You can check the details of pruning "
"from the logs printed out by the example. You can also experiment with "
"different pruning settings by changing the parameters in ``run.sh``, or "
"directly changing the ``config_list`` in ``transformer_pruning.py``."
msgstr ""

#: ../../Compression/Pruner.rst:820
msgid "User configuration for Transformer Head Pruner"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:2
msgid "Speed up Mixed Precision Quantization Model (experimental)"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:8
msgid ""
"Deep learning network has been computational intensive and memory "
"intensive which increases the difficulty of deploying deep neural network"
" model. Quantization is a fundamental technology which is widely used to "
"reduce memory footprint and speed up inference process. Many frameworks "
"begin to support quantization, but few of them support mixed precision "
"quantization and get real speedup. Frameworks like `HAQ: Hardware-Aware "
"Automated Quantization with Mixed Precision "
"<https://arxiv.org/pdf/1811.08886.pdf>`__\\, only support simulated mixed"
" precision quantization which will not speed up the inference process. To"
" get real speedup of mixed precision quantization and help people get the"
" real feedback from hardware, we design a general framework with simple "
"interface to allow NNI quantization algorithms to connect different DL "
"model optimization backends (e.g., TensorRT, NNFusion), which gives users"
" an end-to-end experience that after quantizing their model with "
"quantization algorithms, the quantized model can be directly speeded up "
"with the connected optimization backend. NNI connects TensorRT at this "
"stage, and will support more backends in the future."
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:23
msgid ""
"To support speeding up mixed precision quantization, we divide framework "
"into two part, frontend and backend. Frontend could be popular training "
"frameworks such as PyTorch, TensorFlow etc. Backend could be inference "
"framework for different hardwares, such as TensorRT. At present, we "
"support PyTorch as frontend and TensorRT as backend. To convert PyTorch "
"model to TensorRT engine, we leverage onnx as intermediate graph "
"representation. In this way, we convert PyTorch model to onnx model, then"
" TensorRT parse onnx model to generate inference engine."
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:31
msgid ""
"Quantization aware training combines NNI quantization algorithm 'QAT' and"
" NNI quantization speedup tool. Users should set config to train "
"quantized model using QAT algorithm(please refer to `NNI Quantization "
"Algorithms "
"<https://nni.readthedocs.io/en/stable/Compression/Quantizer.html>`__\\  "
"). After quantization aware training, users can get new config with "
"calibration parameters and model with quantized weight. By passing new "
"config and model to quantization speedup tool, users can get real mixed "
"precision speedup engine to do inference."
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:36
msgid ""
"After getting mixed precision engine, users can do inference with input "
"data."
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:39
#: ../../Compression/QuantizationSpeedup.rst:53
msgid "Note"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:42
msgid ""
"Recommend using \"cpu\"(host) as data device(for both inference data and "
"calibration data) since data should be on host initially and it will be "
"transposed to device before inference. If data type is not \"cpu\"(host),"
" this tool will transpose it to \"cpu\" which may increases unnecessary "
"overhead."
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:43
msgid ""
"User can also do post-training quantization leveraging TensorRT "
"directly(need to provide calibration dataset)."
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:44
msgid ""
"Not all op types are supported right now. At present, NNI supports Conv, "
"Linear, Relu and MaxPool. More op types will be supported in the "
"following release."
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:48
msgid "Prerequisite"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:49
msgid "CUDA version >= 11.0"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:51
msgid "TensorRT version >= 7.2"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:55
msgid ""
"If you haven't installed TensorRT before or use the old version, please "
"refer to `TensorRT Installation Guide "
"<https://docs.nvidia.com/deeplearning/tensorrt/install-"
"guide/index.html>`__\\"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:59
msgid "quantization aware training:"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:86
msgid ""
"Note that NNI also supports post-training quantization directly, please "
"refer to complete examples for detail."
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:89
msgid ""
"For complete examples please refer to :githublink:`the code "
"<examples/model_compress/quantization/mixed_precision_speedup_mnist.py>`."
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:92
msgid ""
"For more parameters about the class 'TensorRTModelSpeedUp', you can refer"
" to `Model Compression API Reference "
"<https://nni.readthedocs.io/en/stable/Compression/CompressionReference.html"
"#quantization-speedup>`__\\."
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:96
msgid "Mnist test"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:98
msgid "on one GTX2080 GPU, input tensor: ``torch.randn(128, 1, 28, 28)``"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:105
#: ../../Compression/QuantizationSpeedup.rst:131
msgid "quantization strategy"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:106
#: ../../Compression/QuantizationSpeedup.rst:132
msgid "Latency"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:107
#: ../../Compression/QuantizationSpeedup.rst:133
msgid "accuracy"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:108
#: ../../Compression/QuantizationSpeedup.rst:134
msgid "all in 32bit"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:109
msgid "0.001199961"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:110
#: ../../Compression/QuantizationSpeedup.rst:113
msgid "96%"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:111
msgid "mixed precision(average bit 20.4)"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:112
msgid "0.000753688"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:114
#: ../../Compression/QuantizationSpeedup.rst:140
msgid "all in 8bit"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:115
msgid "0.000229869"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:116
msgid "93.7%"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:120
msgid "Cifar10 resnet18 test(train one epoch)"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:123
msgid "on one GTX2080 GPU, input tensor: ``torch.randn(128, 3, 32, 32)``"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:135
msgid "0.003286268"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:136
msgid "54.21%"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:137
msgid "mixed precision(average bit 11.55)"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:138
msgid "0.001358022"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:139
msgid "54.78%"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:141
msgid "0.000859139"
msgstr ""

#: ../../Compression/QuantizationSpeedup.rst:142
msgid "52.81%"
msgstr ""

#: ../../Compression/Quantizer.rst:2
msgid "Supported Quantization Algorithms on NNI"
msgstr ""

#: ../../Compression/Quantizer.rst:4
msgid "Index of supported quantization algorithms"
msgstr ""

#: ../../Compression/Quantizer.rst:7
msgid "`Naive Quantizer <#naive-quantizer>`__"
msgstr ""

#: ../../Compression/Quantizer.rst:8
msgid "`QAT Quantizer <#qat-quantizer>`__"
msgstr ""

#: ../../Compression/Quantizer.rst:9
msgid "`DoReFa Quantizer <#dorefa-quantizer>`__"
msgstr ""

#: ../../Compression/Quantizer.rst:10
msgid "`BNN Quantizer <#bnn-quantizer>`__"
msgstr ""

#: ../../Compression/Quantizer.rst:11
msgid "`LSQ Quantizer <#lsq-quantizer>`__"
msgstr ""

#: ../../Compression/Quantizer.rst:12
msgid "`Observer Quantizer <#observer-quantizer>`__"
msgstr ""

#: ../../Compression/Quantizer.rst:15
msgid "Naive Quantizer"
msgstr ""

#: ../../Compression/Quantizer.rst:17
msgid ""
"We provide Naive Quantizer to quantizer weight to default 8 bits, you can"
" use it to test quantize algorithm without any configure."
msgstr ""

#: ../../Compression/Quantizer.rst:22
msgid "pytorch"
msgstr ""

#: ../../Compression/Quantizer.rst:31
msgid "QAT Quantizer"
msgstr ""

#: ../../Compression/Quantizer.rst:33
msgid ""
"In `Quantization and Training of Neural Networks for Efficient Integer-"
"Arithmetic-Only Inference "
"<http://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf>`__\\"
" , authors Benoit Jacob and Skirmantas Kligys provide an algorithm to "
"quantize the model with training."
msgstr ""

#: ../../Compression/Quantizer.rst:37
msgid ""
"We propose an approach that simulates quantization effects in the forward"
" pass of training. Backpropagation still happens as usual, and all "
"weights and biases are stored in floating point so that they can be "
"easily nudged by small amounts. The forward propagation pass however "
"simulates quantized inference as it will happen in the inference engine, "
"by implementing in floating-point arithmetic the rounding behavior of the"
" quantization scheme"
msgstr ""

#: ../../Compression/Quantizer.rst:40
msgid ""
"Weights are quantized before they are convolved with the input. If batch "
"normalization (see [17]) is used for the layer, the batch normalization "
"parameters are folded into the weights before quantization."
msgstr ""

#: ../../Compression/Quantizer.rst:41
msgid ""
"Activations are quantized at points where they would be during inference,"
" e.g. after the activation function is applied to a convolutional or "
"fully connected layers output, or after a bypass connection adds or "
"concatenates the outputs of several layers together such as in ResNets."
msgstr ""

#: ../../Compression/Quantizer.rst:47
msgid ""
"You can quantize your model to 8 bits with the code below before your "
"training code."
msgstr ""

#: ../../Compression/Quantizer.rst:71 ../../Compression/Quantizer.rst:244
msgid "You can view example for more information"
msgstr ""

#: ../../Compression/Quantizer.rst:74
msgid "User configuration for QAT Quantizer"
msgstr ""

#: ../../Compression/Quantizer.rst:76 ../../Compression/Quantizer.rst:214
msgid ""
"common configuration needed by compression algorithms can be found at "
"`Specification of `config_list <./QuickStart.rst>`__."
msgstr ""

#: ../../Compression/Quantizer.rst:78 ../../Compression/Quantizer.rst:216
#: ../../Compression/Quantizer.rst:251 ../../Compression/Quantizer.rst:297
msgid "configuration needed by this algorithm :"
msgstr ""

#: ../../Compression/Quantizer.rst:81
msgid "**quant_start_step:** int"
msgstr ""

#: ../../Compression/Quantizer.rst:83
msgid ""
"disable quantization until model are run by certain number of steps, this"
" allows the network to enter a more stable state where activation "
"quantization ranges do not exclude a signicant fraction of values, "
"default value is 0"
msgstr ""

#: ../../Compression/Quantizer.rst:87
msgid "Batch normalization folding"
msgstr ""

#: ../../Compression/Quantizer.rst:89
msgid ""
"Batch normalization folding is supported in QAT quantizer. It can be "
"easily enabled by passing an argument `dummy_input` to the quantizer, "
"like:"
msgstr ""

#: ../../Compression/Quantizer.rst:102
msgid ""
"The quantizer will automatically detect Conv-BN patterns and simulate "
"batch normalization folding process in the training graph. Note that when"
" the quantization aware training process is finished, the folded "
"weight/bias would be restored after calling `quantizer.export_model`."
msgstr ""

#: ../../Compression/Quantizer.rst:107
msgid "Quantization dtype and scheme customization"
msgstr ""

#: ../../Compression/Quantizer.rst:108
msgid ""
"Different backends on different devices use different quantization "
"strategies (i.e. dtype (int or uint) and scheme (per-tensor or per-"
"channel and symmetric or affine)). QAT quantizer supports customization "
"of mainstream dtypes and schemes. There are two ways to set them. One way"
" is setting them globally through a function named "
"`set_quant_scheme_dtype` like:"
msgstr ""

#: ../../Compression/Quantizer.rst:124
msgid ""
"The other way is more detailed. You can customize the dtype and scheme in"
" each quantization config list like:"
msgstr ""

#: ../../Compression/Quantizer.rst:144
msgid "Multi-GPU training"
msgstr ""

#: ../../Compression/Quantizer.rst:145
msgid ""
"QAT quantizer natively supports multi-gpu training (DataParallel and "
"DistributedDataParallel). Note that the quantizer instantiation should "
"happen before you wrap your model with DataParallel or "
"DistributedDataParallel. For example:"
msgstr ""

#: ../../Compression/Quantizer.rst:167
msgid "LSQ Quantizer"
msgstr ""

#: ../../Compression/Quantizer.rst:169
msgid ""
"In `LEARNED STEP SIZE QUANTIZATION "
"<https://arxiv.org/pdf/1902.08153.pdf>`__\\ , authors Steven K. Esser and"
" Jeffrey L. McKinstry provide an algorithm to train the scales with "
"gradients."
msgstr ""

#: ../../Compression/Quantizer.rst:173
msgid ""
"The authors introduce a novel means to estimate and scale the task loss "
"gradient at each weight and activation layers quantizer step size, such "
"that it can be learned in conjunction with other network parameters."
msgstr ""

#: ../../Compression/Quantizer.rst:178
msgid ""
"You can add codes below before your training codes. Three things must be "
"done:"
msgstr ""

#: ../../Compression/Quantizer.rst:181 ../../Compression/Quantizer.rst:327
msgid ""
"configure which layer to be quantized and which tensor "
"(input/output/weight) of that layer to be quantized."
msgstr ""

#: ../../Compression/Quantizer.rst:182
msgid "construct the lsq quantizer"
msgstr ""

#: ../../Compression/Quantizer.rst:183
msgid "call the `compress` API"
msgstr ""

#: ../../Compression/Quantizer.rst:209
msgid ""
"You can view example for more information. "
":githublink:`examples/model_compress/quantization/LSQ_torch_quantizer.py "
"<examples/model_compress/quantization/LSQ_torch_quantizer.py>`"
msgstr ""

#: ../../Compression/Quantizer.rst:212
msgid "User configuration for LSQ Quantizer"
msgstr ""

#: ../../Compression/Quantizer.rst:222
msgid "DoReFa Quantizer"
msgstr ""

#: ../../Compression/Quantizer.rst:224
msgid ""
"In `DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with "
"Low Bitwidth Gradients <https://arxiv.org/abs/1606.06160>`__\\ , authors "
"Shuchang Zhou and Yuxin Wu provide an algorithm named DoReFa to quantize "
"the weight, activation and gradients with training."
msgstr ""

#: ../../Compression/Quantizer.rst:229
msgid ""
"To implement DoReFa Quantizer, you can add code below before your "
"training code"
msgstr ""

#: ../../Compression/Quantizer.rst:247
msgid "User configuration for DoReFa Quantizer"
msgstr ""

#: ../../Compression/Quantizer.rst:249 ../../Compression/Quantizer.rst:295
msgid ""
"common configuration needed by compression algorithms can be found at "
"`Specification of ``config_list`` <./QuickStart.rst>`__."
msgstr ""

#: ../../Compression/Quantizer.rst:256
msgid "BNN Quantizer"
msgstr ""

#: ../../Compression/Quantizer.rst:258
msgid ""
"In `Binarized Neural Networks: Training Deep Neural Networks with Weights"
" and Activations Constrained to +1 or -1 "
"<https://arxiv.org/abs/1602.02830>`__\\ ,"
msgstr ""

#: ../../Compression/Quantizer.rst:262
msgid ""
"We introduce a method to train Binarized Neural Networks (BNNs) - neural "
"networks with binary weights and activations at run-time. At training-"
"time the binary weights and activations are used for computing the "
"parameters gradients. During the forward pass, BNNs drastically reduce "
"memory size and accesses, and replace most arithmetic operations with "
"bit-wise operations, which is expected to substantially improve power-"
"efficiency."
msgstr ""

#: ../../Compression/Quantizer.rst:290
msgid ""
"You can view example "
":githublink:`examples/model_compress/quantization/BNN_quantizer_cifar10.py"
" <examples/model_compress/quantization/BNN_quantizer_cifar10.py>` for "
"more information."
msgstr ""

#: ../../Compression/Quantizer.rst:293
msgid "User configuration for BNN Quantizer"
msgstr ""

#: ../../Compression/Quantizer.rst:300
msgid "Experiment"
msgstr ""

#: ../../Compression/Quantizer.rst:302
msgid ""
"We implemented one of the experiments in `Binarized Neural Networks: "
"Training Deep Neural Networks with Weights and Activations Constrained to"
" +1 or -1 <https://arxiv.org/abs/1602.02830>`__\\ , we quantized the "
"**VGGNet** for CIFAR-10 in the paper. Our experiments results are as "
"follows:"
msgstr ""

#: ../../Compression/Quantizer.rst:309
msgid "Accuracy"
msgstr ""

#: ../../Compression/Quantizer.rst:311
msgid "86.93%"
msgstr ""

#: ../../Compression/Quantizer.rst:314
msgid ""
"The experiments code can be found at "
":githublink:`examples/model_compress/quantization/BNN_quantizer_cifar10.py"
" <examples/model_compress/quantization/BNN_quantizer_cifar10.py>`"
msgstr ""

#: ../../Compression/Quantizer.rst:318
msgid "Observer Quantizer"
msgstr ""

#: ../../Compression/Quantizer.rst:322
msgid ""
"Observer quantizer is a framework of post-training quantization. It will "
"insert observers into the place where the quantization will happen. "
"During quantization calibration, each observer will record all the "
"tensors it 'sees'. These tensors will be used to calculate the "
"quantization statistics after calibration."
msgstr ""

#: ../../Compression/Quantizer.rst:328
msgid "construct the observer quantizer."
msgstr ""

#: ../../Compression/Quantizer.rst:329
msgid "do quantization calibration."
msgstr ""

#: ../../Compression/Quantizer.rst:330
msgid ""
"call the `compress` API to calculate the scale and zero point for each "
"tensor and switch model to evaluation mode."
msgstr ""

#: ../../Compression/Quantizer.rst:360
msgid ""
"You can view example "
":githublink:`examples/model_compress/quantization/observer_quantizer.py "
"<examples/model_compress/quantization/observer_quantizer.py>` for more "
"information."
msgstr ""

#: ../../Compression/Quantizer.rst:363
msgid "User configuration for Observer Quantizer"
msgstr ""

#: ../../Compression/Quantizer.rst:364
msgid ""
"Common configuration needed by compression algorithms can be found at "
"`Specification of `config_list <./QuickStart.rst>`__."
msgstr ""

#: ../../Compression/Quantizer.rst:368
msgid ""
"This quantizer is still under development for now. Some quantizer "
"settings are hard-coded:"
msgstr ""

#: ../../Compression/Quantizer.rst:370
msgid "weight observer: per_tensor_symmetric, qint8"
msgstr ""

#: ../../Compression/Quantizer.rst:371
msgid "output observer: per_tensor_affine, quint8, reduce_range=True"
msgstr ""

#: ../../Compression/Quantizer.rst:373
msgid "Other settings (such as quant_type and op_names) can be configured."
msgstr ""

#: ../../Compression/Quantizer.rst:376
msgid "About the compress API"
msgstr ""

#: ../../Compression/Quantizer.rst:377
msgid ""
"Before the `compress` API is called, the model will only record tensors' "
"statistics and no quantization process will be executed. After the "
"`compress` API is called, the model will NOT record tensors' statistics "
"any more. The quantization scale and zero point will be generated for "
"each tensor and will be used to quantize each tensor during inference (we"
" call it evaluation mode)"
msgstr ""

#: ../../Compression/Quantizer.rst:382
msgid "About calibration"
msgstr ""

#: ../../Compression/Quantizer.rst:383
msgid ""
"Usually we pick up about 100 training/evaluation examples for "
"calibration. If you found the accuracy is a bit low, try to reduce the "
"number of calibration examples."
msgstr ""

#: ../../Compression/QuickStart.rst:4
msgid "Notebook Example"
msgstr ""

#: ../../Compression/QuickStart.rst:2
msgid "Quick Start"
msgstr ""

#: ../../Compression/QuickStart.rst:10
msgid ""
"Model compression usually consists of three stages: 1) pre-training a "
"model, 2) compress the model, 3) fine-tuning the model. NNI mainly "
"focuses on the second stage and provides very simple APIs for compressing"
" a model. Follow this guide for a quick look at how easy it is to use NNI"
" to compress a model."
msgstr ""

#: ../../Compression/QuickStart.rst:12
msgid ""
"A `compression pipeline example <./compression_pipeline_example.rst>`__ "
"with Jupyter notebook is supported and refer the code :githublink:`here "
"<examples/notebooks/compression_pipeline_example.ipynb>`."
msgstr ""

#: ../../Compression/QuickStart.rst:15
msgid "Model Pruning"
msgstr ""

#: ../../Compression/QuickStart.rst:17
msgid ""
"Here we use `level pruner <../Compression/Pruner.rst#level-pruner>`__ as "
"an example to show the usage of pruning in NNI."
msgstr ""

#: ../../Compression/QuickStart.rst:20 ../../Compression/QuickStart.rst:78
msgid "Step1. Write configuration"
msgstr ""

#: ../../Compression/QuickStart.rst:22
msgid ""
"Write a configuration to specify the layers that you want to prune. The "
"following configuration means pruning all the ``default``\\ ops to "
"sparsity 0.5 while keeping other layers unpruned."
msgstr ""

#: ../../Compression/QuickStart.rst:31
msgid ""
"The specification of configuration can be found `here <./Tutorial.rst"
"#specify-the-configuration>`__. Note that different pruners may have "
"their own defined fields in configuration. Please refer to each pruner's "
"`usage <./Pruner.rst>`__ for details, and adjust the configuration "
"accordingly."
msgstr ""

#: ../../Compression/QuickStart.rst:34
msgid "Step2. Choose a pruner and compress the model"
msgstr ""

#: ../../Compression/QuickStart.rst:36
msgid ""
"First instantiate the chosen pruner with your model and configuration as "
"arguments, then invoke ``compress()`` to compress your model. Note that, "
"some algorithms may check gradients for compressing, so we may also "
"define a trainer, an optimizer, a criterion and pass them to the pruner."
msgstr ""

#: ../../Compression/QuickStart.rst:45
msgid ""
"Some pruners (e.g., L1FilterPruner, FPGMPruner) prune once, some pruners "
"(e.g., AGPPruner) prune your model iteratively, the masks are adjusted "
"epoch by epoch during training."
msgstr ""

#: ../../Compression/QuickStart.rst:47
msgid ""
"So if the pruners prune your model iteratively or they need training or "
"inference to get gradients, you need pass finetuning logic to pruner."
msgstr ""

#: ../../Compression/QuickStart.rst:49
msgid "For example:"
msgstr ""

#: ../../Compression/QuickStart.rst:59 ../../Compression/QuickStart.rst:114
msgid "Step3. Export compression result"
msgstr ""

#: ../../Compression/QuickStart.rst:61
msgid ""
"After training, you can export model weights to a file, and the generated"
" masks to a file as well. Exporting onnx model is also supported."
msgstr ""

#: ../../Compression/QuickStart.rst:67
msgid ""
"Plese refer to :githublink:`mnist example "
"<examples/model_compress/pruning/naive_prune_torch.py>` for example code."
msgstr ""

#: ../../Compression/QuickStart.rst:69
msgid ""
"More examples of pruning algorithms can be found in "
":githublink:`basic_pruners_torch "
"<examples/model_compress/pruning/basic_pruners_torch.py>` and "
":githublink:`auto_pruners_torch "
"<examples/model_compress/pruning/auto_pruners_torch.py>`."
msgstr ""

#: ../../Compression/QuickStart.rst:73
msgid "Model Quantization"
msgstr ""

#: ../../Compression/QuickStart.rst:75
msgid ""
"Here we use `QAT  Quantizer <../Compression/Quantizer.rst#qat-"
"quantizer>`__ as an example to show the usage of pruning in NNI."
msgstr ""

#: ../../Compression/QuickStart.rst:100
msgid ""
"The specification of configuration can be found `here <./Tutorial.rst"
"#quantization-specific-keys>`__."
msgstr ""

#: ../../Compression/QuickStart.rst:103
msgid "Step2. Choose a quantizer and compress the model"
msgstr ""

#: ../../Compression/QuickStart.rst:116
msgid ""
"After training and calibration, you can export model weight to a file, "
"and the generated calibration parameters to a file as well. Exporting "
"onnx model is also supported."
msgstr ""

#: ../../Compression/QuickStart.rst:122
msgid ""
"Plese refer to :githublink:`mnist example "
"<examples/model_compress/quantization/QAT_torch_quantizer.py>` for "
"example code."
msgstr ""

#: ../../Compression/QuickStart.rst:124
msgid ""
"Congratulations! You've compressed your first model via NNI. To go a bit "
"more in depth about model compression in NNI, check out the `Tutorial "
"<./Tutorial.rst>`__."
msgstr ""

#: ../../Compression/Tutorial.rst:2
msgid "Tutorial"
msgstr ""

#: ../../Compression/Tutorial.rst:6
msgid ""
"In this tutorial, we will explain more detailed usage about the model "
"compression in NNI."
msgstr ""

#: ../../Compression/Tutorial.rst:9
msgid "Setup compression goal"
msgstr ""

#: ../../Compression/Tutorial.rst:12
msgid "Specify the configuration"
msgstr ""

#: ../../Compression/Tutorial.rst:14
msgid ""
"Users can specify the configuration (i.e., ``config_list``\\ ) for a "
"compression algorithm. For example, when compressing a model, users may "
"want to specify the sparsity ratio, to specify different ratios for "
"different types of operations, to exclude certain types of operations, or"
" to compress only a certain types of operations. For users to express "
"these kinds of requirements, we define a configuration specification. It "
"can be seen as a python ``list`` object, where each element is a ``dict``"
" object."
msgstr ""

#: ../../Compression/Tutorial.rst:16
msgid ""
"The ``dict``\\ s in the ``list`` are applied one by one, that is, the "
"configurations in latter ``dict`` will overwrite the configurations in "
"former ones on the operations that are within the scope of both of them."
msgstr ""

#: ../../Compression/Tutorial.rst:18
msgid ""
"There are different keys in a ``dict``. Some of them are common keys "
"supported by all the compression algorithms:"
msgstr ""

#: ../../Compression/Tutorial.rst:20
msgid ""
"**op_types**\\ : This is to specify what types of operations to be "
"compressed. 'default' means following the algorithm's default setting. "
"All suported module types are defined in :githublink:`default_layers.py "
"<nni/compression/pytorch/default_layers.py>` for pytorch."
msgstr ""

#: ../../Compression/Tutorial.rst:21
msgid ""
"**op_names**\\ : This is to specify by name what operations to be "
"compressed. If this field is omitted, operations will not be filtered by "
"it."
msgstr ""

#: ../../Compression/Tutorial.rst:22
msgid ""
"**exclude**\\ : Default is False. If this field is True, it means the "
"operations with specified types and names will be excluded from the "
"compression."
msgstr ""

#: ../../Compression/Tutorial.rst:24
msgid ""
"Some other keys are often specific to a certain algorithm, users can "
"refer to `pruning algorithms <./Pruner.rst>`__ and `quantization "
"algorithms <./Quantizer.rst>`__ for the keys allowed by each algorithm."
msgstr ""

#: ../../Compression/Tutorial.rst:26
msgid ""
"To prune all ``Conv2d`` layers with the sparsity of 0.6, the "
"configuration can be written as:"
msgstr ""

#: ../../Compression/Tutorial.rst:35
msgid ""
"To control the sparsity of specific layers, the configuration can be "
"written as:"
msgstr ""

#: ../../Compression/Tutorial.rst:52
msgid ""
"It means following the algorithm's default setting for compressed "
"operations with sparsity 0.8, but for ``op_name1`` and ``op_name2`` use "
"sparsity 0.6, and do not compress ``op_name3``."
msgstr ""

#: ../../Compression/Tutorial.rst:55
msgid "Quantization specific keys"
msgstr ""

#: ../../Compression/Tutorial.rst:57
msgid ""
"Besides the keys explained above, if you use quantization algorithms you "
"need to specify more keys in ``config_list``\\ , which are explained "
"below."
msgstr ""

#: ../../Compression/Tutorial.rst:59
msgid "**quant_types** : list of string."
msgstr ""

#: ../../Compression/Tutorial.rst:61
msgid ""
"Type of quantization you want to apply, currently support 'weight', "
"'input', 'output'. 'weight' means applying quantization operation to the "
"weight parameter of modules. 'input' means applying quantization "
"operation to the input of module forward method. 'output' means applying "
"quantization operation to the output of module forward method, which is "
"often called as 'activation' in some papers."
msgstr ""

#: ../../Compression/Tutorial.rst:65
msgid "**quant_bits** : int or dict of {str : int}"
msgstr ""

#: ../../Compression/Tutorial.rst:67
msgid ""
"bits length of quantization, key is the quantization type, value is the "
"quantization bits length, eg."
msgstr ""

#: ../../Compression/Tutorial.rst:78
msgid ""
"when the value is int type, all quantization types share same bits "
"length. eg."
msgstr ""

#: ../../Compression/Tutorial.rst:86
msgid "**quant_dtype** : str or dict of {str : str}"
msgstr ""

#: ../../Compression/Tutorial.rst:88
msgid ""
"quantization dtype, used to determine the range of quantized value. Two "
"choices can be used:"
msgstr ""

#: ../../Compression/Tutorial.rst:90
msgid "int: the range is singed"
msgstr ""

#: ../../Compression/Tutorial.rst:91
msgid "uint: the range is unsigned"
msgstr ""

#: ../../Compression/Tutorial.rst:93
msgid ""
"Two ways to set it. One is that the key is the quantization type, and the"
" value is the quantization dtype, eg."
msgstr ""

#: ../../Compression/Tutorial.rst:104
msgid ""
"The other is that the value is str type, and all quantization types share"
" the same dtype. eg."
msgstr ""

#: ../../Compression/Tutorial.rst:112
msgid ""
"There are totally two kinds of `quant_dtype` you can set, they are 'int' "
"and 'uint'."
msgstr ""

#: ../../Compression/Tutorial.rst:114
msgid "**quant_scheme** : str or dict of {str : str}"
msgstr ""

#: ../../Compression/Tutorial.rst:116
msgid ""
"quantization scheme, used to determine the quantization manners. Four "
"choices can used:"
msgstr ""

#: ../../Compression/Tutorial.rst:118
msgid "per_tensor_affine: per tensor, asymmetric quantization"
msgstr ""

#: ../../Compression/Tutorial.rst:119
msgid "per_tensor_symmetric: per tensor, symmetric quantization"
msgstr ""

#: ../../Compression/Tutorial.rst:120
msgid "per_channel_affine: per channel, asymmetric quantization"
msgstr ""

#: ../../Compression/Tutorial.rst:121
msgid "per_channel_symmetric: per channel, symmetric quantization"
msgstr ""

#: ../../Compression/Tutorial.rst:123
msgid ""
"Two ways to set it. One is that the key is the quantization type, value "
"is the quantization scheme, eg."
msgstr ""

#: ../../Compression/Tutorial.rst:134
msgid ""
"The other is that the value is str type, all quantization types share the"
" same quant_scheme. eg."
msgstr ""

#: ../../Compression/Tutorial.rst:142
msgid ""
"There are totally four kinds of `quant_scheme` you can set, they are "
"'per_tensor_affine', 'per_tensor_symmetric', 'per_channel_affine' and "
"'per_channel_symmetric'."
msgstr ""

#: ../../Compression/Tutorial.rst:144
msgid ""
"The following example shows a more complete ``config_list``\\ , it uses "
"``op_names`` (or ``op_types``\\ ) to specify the target layers along with"
" the quantization bits for those layers."
msgstr ""

#: ../../Compression/Tutorial.rst:178
msgid ""
"In this example, 'op_names' is the name of layer and four layers will be "
"quantized to different quant_bits."
msgstr ""

#: ../../Compression/Tutorial.rst:182
msgid "Export compression result"
msgstr ""

#: ../../Compression/Tutorial.rst:185
msgid "Export the pruned model"
msgstr ""

#: ../../Compression/Tutorial.rst:187
msgid ""
"You can easily export the pruned model using the following API if you are"
" pruning your model, ``state_dict`` of the sparse model weights will be "
"stored in ``model.pth``\\ , which can be loaded by "
"``torch.load('model.pth')``. Note that, the exported ``model.pth``\\ has "
"the same parameters as the original model except the masked weights are "
"zero. ``mask_dict`` stores the binary value that produced by the pruning "
"algorithm, which can be further used to speed up the model."
msgstr ""

#: ../../Compression/Tutorial.rst:200
msgid "export model in ``onnx`` format(\\ ``input_shape`` need to be specified):"
msgstr ""

#: ../../Compression/Tutorial.rst:208
msgid "Export the quantized model"
msgstr ""

#: ../../Compression/Tutorial.rst:210
msgid ""
"You can export the quantized model directly by using ``torch.save`` api "
"and the quantized model can be loaded by ``torch.load`` without any extra"
" modification. The following example shows the normal procedure of "
"saving, loading quantized model and get related parameters in QAT."
msgstr ""

#: ../../Compression/Tutorial.rst:235
msgid "Speed up the model"
msgstr ""

#: ../../Compression/Tutorial.rst:237
msgid ""
"Masks do not provide real speedup of your model. The model should be "
"speeded up based on the exported masks, thus, we provide an API to speed "
"up your model as shown below. After invoking "
"``apply_compression_results`` on your model, your model becomes a smaller"
" one with shorter inference latency."
msgstr ""

#: ../../Compression/Tutorial.rst:248
msgid ""
"Please refer to `here <ModelSpeedup.rst>`__ for detailed description. The"
" example code for model speedup can be found :githublink:`here "
"<examples/model_compress/pruning/model_speedup.py>`"
msgstr ""

#: ../../Compression/Tutorial.rst:252
msgid "Control the Fine-tuning process"
msgstr ""

#: ../../Compression/Tutorial.rst:255
msgid "Enhance the fine-tuning process"
msgstr ""

#: ../../Compression/Tutorial.rst:257
msgid ""
"Knowledge distillation effectively learns a small student model from a "
"large teacher model. Users can enhance the fine-tuning process that "
"utilize knowledge distillation to improve the performance of the "
"compressed model. Example code can be found :githublink:`here "
"<examples/model_compress/pruning/finetune_kd_torch.py>`"
msgstr ""

#: ../../Compression/advanced.rst:4
msgid "Framework"
msgstr ""

#: ../../Compression/advanced.rst:4
msgid "Customize a new algorithm"
msgstr ""

#: ../../Compression/advanced.rst:4
msgid "Automatic Model Compression (Beta)"
msgstr ""

#: ../../Compression/compression_pipeline_example.ipynb:9
msgid "1. Prepare model"
msgstr ""

#: ../../Compression/compression_pipeline_example.ipynb:677
msgid "2. Prepare config_list for pruning"
msgstr ""

#: ../../Compression/compression_pipeline_example.ipynb:705
msgid "3. Choose a pruner and pruning"
msgstr ""

#: ../../Compression/compression_pipeline_example.ipynb:1588
msgid "4. Speed Up"
msgstr ""

#: ../../Compression/compression_pipeline_example.ipynb:1933
msgid "5. Prepare config_list for quantization"
msgstr ""

#: ../../Compression/compression_pipeline_example.ipynb:1959
msgid "6. Choose a quantizer and quantizing"
msgstr ""

#: ../../Compression/compression_pipeline_example.ipynb:2154
msgid "7. Speed Up"
msgstr ""

#: ../../Compression/pruning.rst:20
msgid "Dependency Aware Mode"
msgstr ""

#: ../../Compression/pruning.rst:3
msgid "Pruning"
msgstr ""

#: ../../Compression/pruning.rst:5
msgid ""
"Pruning is a common technique to compress neural network models. The "
"pruning methods explore the redundancy in the model weights(parameters) "
"and try to remove/prune the redundant and uncritical weights. The "
"redundant elements are pruned from the model, their values are zeroed and"
" we make sure they don't take part in the back-propagation process."
msgstr ""

#: ../../Compression/pruning.rst:9
msgid ""
"From pruning granularity perspective, fine-grained pruning or "
"unstructured pruning refers to pruning each individual weights "
"separately. Coarse-grained pruning or structured pruning is pruning "
"entire group of weights, such as a convolutional filter."
msgstr ""

#: ../../Compression/pruning.rst:12
msgid ""
"NNI provides multiple unstructured pruning and structured pruning "
"algorithms. It supports Tensorflow and PyTorch with unified interface. "
"For users to prune their models, they only need to add several lines in "
"their code. For the structured filter pruning, NNI also provides a "
"dependency-aware mode. In the dependency-aware mode, the filter pruner "
"will get better speed gain after the speedup."
msgstr ""

#: ../../Compression/pruning.rst:18 ../../Compression/v2_pruning.rst:19
msgid "For details, please refer to the following tutorials:"
msgstr ""

#: ../../Compression/quantization.rst:3
msgid "Quantization"
msgstr ""

#: ../../Compression/quantization.rst:5
msgid ""
"Quantization refers to compressing models by reducing the number of bits "
"required to represent weights or activations, which can reduce the "
"computations and the inference time. In the context of deep neural "
"networks, the major numerical format for model weights is 32-bit float, "
"or FP32. Many research works have demonstrated that weights and "
"activations can be represented using 8-bit integers without significant "
"loss in accuracy. Even lower bit-widths, such as 4/2/1 bits, is an active"
" field of research."
msgstr ""

#: ../../Compression/quantization.rst:11
msgid ""
"A quantizer is a quantization algorithm implementation in NNI, NNI "
"provides multiple quantizers as below. You can also create your own "
"quantizer using NNI model compression interface."
msgstr ""

#: ../../Compression/v2_pruning.rst:21 ../../Compression/v2_scheduler.rst:2
msgid "Pruning Scheduler"
msgstr ""

#: ../../Compression/v2_pruning.rst:21
msgid "Pruning Config List"
msgstr ""

#: ../../Compression/v2_pruning.rst:2
msgid "Pruning V2"
msgstr ""

#: ../../Compression/v2_pruning.rst:4
msgid ""
"Pruning V2 is a refactoring of the old version and provides more powerful"
" functions. Compared with the old version, the iterative pruning process "
"is detached from the pruner and the pruner is only responsible for "
"pruning and generating the masks once. What's more, pruning V2 unifies "
"the pruning process and provides a more free combination of pruning "
"components. Task generator only cares about the pruning effect that "
"should be achieved in each round, and uses a config list to express how "
"to pruning in the next step. Pruner will reset with the model and config "
"list given by task generator then generate the masks in current step."
msgstr ""

#: ../../Compression/v2_pruning.rst:10
msgid "For a clearer structure vision, please refer to the figure below."
msgstr ""

#: ../../Compression/v2_pruning.rst:16
msgid ""
"In V2, a pruning process is usually driven by a pruning scheduler, it "
"contains a specific pruner and a task generator. But users can also use "
"pruner directly like in the pruning V1."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:2
msgid "Supported Pruning Algorithms in NNI"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:4
msgid ""
"NNI provides several pruning algorithms that reproducing from the papers."
" In pruning v2, NNI split the pruning algorithm into more detailed "
"components. This means users can freely combine components from different"
" algorithms, or easily use a component of their own implementation to "
"replace a step in the original algorithm to implement their own pruning "
"algorithm."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:8
msgid ""
"Right now, pruning algorithms with how to generate masks in one step are "
"implemented as pruners, and how to schedule sparsity in each iteration "
"are implemented as iterative pruners."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:11
msgid "**Pruner**"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:13
msgid "`Level Pruner <#level-pruner>`__"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:14
msgid "`L1 Norm Pruner <#l1-norm-pruner>`__"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:15
msgid "`L2 Norm Pruner <#l2-norm-pruner>`__"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:18
msgid "`Activation APoZ Rank Pruner <#activation-apoz-rank-pruner>`__"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:19
msgid "`Activation Mean Rank Pruner <#activation-mean-rank-pruner>`__"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:20
msgid "`Taylor FO Weight Pruner <#taylor-fo-weight-pruner>`__"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:22
msgid "`Movement Pruner <#movement-pruner>`__"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:24
msgid "**Iterative Pruner**"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:26
msgid "`Linear Pruner <#linear-pruner>`__"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:28
msgid "`Lottery Ticket Pruner <#lottery-ticket-pruner>`__"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:29
msgid "`Simulated Annealing Pruner <#simulated-annealing-pruner>`__"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:30
msgid "`Auto Compress Pruner <#auto-compress-pruner>`__"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:36
msgid ""
"This is a basic pruner, and in some papers called it magnitude pruning or"
" fine-grained pruning."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:38
msgid ""
"It will mask the weight in each specified layer with smaller absolute "
"value by a ratio configured in the config list."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:50
msgid ""
"For detailed example please refer to "
":githublink:`examples/model_compress/pruning/v2/level_pruning_torch.py "
"<examples/model_compress/pruning/v2/level_pruning_torch.py>`"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.LevelPruner:3
#: nni.algorithms.compression.v2.pytorch.pruning.movement_pruner.MovementPruner:3
#: of
msgid ""
"Supported keys:     - sparsity : This is to specify the sparsity for each"
" layer in this config to be compressed.     - sparsity_per_layer : Equals"
" to sparsity.     - op_types : Operation types to be pruned.     - "
"op_names : Operation names to be pruned.     - op_partial_names: "
"Operation partial names to be pruned, will be autocompleted by NNI.     -"
" exclude : Set True then the layers setting by op_types and op_names will"
" be excluded from pruning."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:12
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.FPGMPruner:5
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L1NormPruner:5
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L2NormPruner:5
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.LevelPruner:5
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:5
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:5
#: nni.algorithms.compression.v2.pytorch.pruning.movement_pruner.MovementPruner:5
#: of
msgid ""
"sparsity : This is to specify the sparsity for each layer in this config "
"to be compressed."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:13
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.FPGMPruner:6
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L1NormPruner:6
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L2NormPruner:6
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.LevelPruner:6
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:6
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:6
#: nni.algorithms.compression.v2.pytorch.pruning.movement_pruner.MovementPruner:6
#: of
msgid "sparsity_per_layer : Equals to sparsity."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:15
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.LevelPruner:7
#: nni.algorithms.compression.v2.pytorch.pruning.movement_pruner.MovementPruner:7
#: of
msgid "op_types : Operation types to be pruned."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:16
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.FPGMPruner:8
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L1NormPruner:8
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L2NormPruner:8
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.LevelPruner:8
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:10
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:10
#: nni.algorithms.compression.v2.pytorch.pruning.movement_pruner.MovementPruner:8
#: of
msgid "op_names : Operation names to be pruned."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:16
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:17
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.FPGMPruner:9
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L1NormPruner:9
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L2NormPruner:9
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.LevelPruner:9
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:11
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:11
#: nni.algorithms.compression.v2.pytorch.pruning.movement_pruner.MovementPruner:9
#: of
msgid ""
"op_partial_names: Operation partial names to be pruned, will be "
"autocompleted by NNI."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:18
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.FPGMPruner:10
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L1NormPruner:10
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L2NormPruner:10
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.LevelPruner:10
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:12
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:12
#: nni.algorithms.compression.v2.pytorch.pruning.movement_pruner.MovementPruner:10
#: of
msgid ""
"exclude : Set True then the layers setting by op_types and op_names will "
"be excluded from pruning."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:60
msgid "L1 Norm Pruner"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:62
msgid ""
"L1 norm pruner computes the l1 norm of the layer weight on the first "
"dimension, then prune the weight blocks on this dimension with smaller l1"
" norm values. i.e., compute the l1 norm of the filters in convolution "
"layer as metric values, compute the l1 norm of the weight by rows in "
"linear layer as metric values."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:69
msgid "In addition, L1 norm pruner also supports dependency-aware mode."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:81
#: ../../Compression/v2_pruning_algo.rst:107
msgid ""
"For detailed example please refer to "
":githublink:`examples/model_compress/pruning/v2/norm_pruning_torch.py "
"<examples/model_compress/pruning/v2/norm_pruning_torch.py>`"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:84
msgid "User configuration for L1 Norm Pruner"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L1NormPruner:3
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L2NormPruner:3 of
msgid ""
"Supported keys:     - sparsity : This is to specify the sparsity for each"
" layer in this config to be compressed.     - sparsity_per_layer : Equals"
" to sparsity.     - op_types : Conv2d and Linear are supported in "
"L1NormPruner.     - op_names : Operation names to be pruned.     - "
"op_partial_names: Operation partial names to be pruned, will be "
"autocompleted by NNI.     - exclude : Set True then the layers setting by"
" op_types and op_names will be excluded from pruning."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L1NormPruner:7
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L2NormPruner:7 of
msgid "op_types : Conv2d and Linear are supported in L1NormPruner."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L1NormPruner:12
#: of
msgid ""
"'normal' or 'dependency_aware'. If prune the model in a dependency-aware "
"way, this pruner will prune the model according to the l1-norm of weights"
" and the channel-dependency or group-dependency of the model. In this "
"way, the pruner will force the conv layers that have dependencies to "
"prune the same channels, so the speedup module can better harvest the "
"speed benefit from the pruned model. Note that, if set 'dependency_aware'"
" , the dummy_input cannot be None, because the pruner needs a dummy input"
" to trace the dependency between the conv layers."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:91
msgid "L2 Norm Pruner"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:93
msgid ""
"L2 norm pruner is a variant of L1 norm pruner. It uses l2 norm as metric "
"to determine which weight elements should be pruned."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:95
msgid "L2 norm pruner also supports dependency-aware mode."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:110
msgid "User configuration for L2 Norm Pruner"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.L2NormPruner:12
#: of
msgid ""
"'normal' or 'dependency_aware'. If prune the model in a dependency-aware "
"way, this pruner will prune the model according to the l2-norm of weights"
" and the channel-dependency or group-dependency of the model. In this "
"way, the pruner will force the conv layers that have dependencies to "
"prune the same channels, so the speedup module can better harvest the "
"speed benefit from the pruned model. Note that, if set 'dependency_aware'"
" , the dummy_input cannot be None, because the pruner needs a dummy input"
" to trace the dependency between the conv layers."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:119
msgid ""
"FPGM pruner prunes the blocks of the weight on the first dimension with "
"the smallest geometric median. FPGM chooses the weight blocks with the "
"most replaceable contribution."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:122
msgid ""
"For more details, please refer to `Filter Pruning via Geometric Median "
"for Deep Convolutional Neural Networks Acceleration "
"<https://arxiv.org/abs/1811.00250>`__."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:124
msgid "FPGM pruner also supports dependency-aware mode."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:136
msgid ""
"For detailed example please refer to "
":githublink:`examples/model_compress/pruning/v2/fpgm_pruning_torch.py "
"<examples/model_compress/pruning/v2/fpgm_pruning_torch.py>`"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.FPGMPruner:3 of
msgid ""
"Supported keys:     - sparsity : This is to specify the sparsity for each"
" layer in this config to be compressed.     - sparsity_per_layer : Equals"
" to sparsity.     - op_types : Conv2d and Linear are supported in "
"FPGMPruner.     - op_names : Operation names to be pruned.     - "
"op_partial_names: Operation partial names to be pruned, will be "
"autocompleted by NNI.     - exclude : Set True then the layers setting by"
" op_types and op_names will be excluded from pruning."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.FPGMPruner:7 of
msgid "op_types : Conv2d and Linear are supported in FPGMPruner."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.FPGMPruner:12 of
msgid ""
"'normal' or 'dependency_aware'. If prune the model in a dependency-aware "
"way, this pruner will prune the model according to the FPGM of weights "
"and the channel-dependency or group-dependency of the model. In this way,"
" the pruner will force the conv layers that have dependencies to prune "
"the same channels, so the speedup module can better harvest the speed "
"benefit from the pruned model. Note that, if set 'dependency_aware' , the"
" dummy_input cannot be None, because the pruner needs a dummy input to "
"trace the dependency between the conv layers."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:148
msgid ""
"Slim pruner adds sparsity regularization on the scaling factors of batch "
"normalization (BN) layers during training to identify unimportant "
"channels. The channels with small scaling factor values will be pruned."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:151
msgid ""
"For more details, please refer to `Learning Efficient Convolutional "
"Networks through Network Slimming "
"<https://arxiv.org/abs/1708.06519>`__\\."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:168
msgid ""
"For detailed example please refer to "
":githublink:`examples/model_compress/pruning/v2/slim_pruning_torch.py "
"<examples/model_compress/pruning/v2/slim_pruning_torch.py>`"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:3 of
msgid ""
"Supported keys:     - sparsity : This is to specify the sparsity for each"
" layer in this config to be compressed.     - sparsity_per_layer : Equals"
" to sparsity.     - total_sparsity : This is to specify the total "
"sparsity for all layers in this config, each layer may have different "
"sparsity.     - max_sparsity_per_layer : Always used with total_sparsity."
" Limit the max sparsity of each layer.     - op_types : Only BatchNorm2d "
"is supported in SlimPruner.     - op_names : Operation names to be "
"pruned.     - op_partial_names: Operation partial names to be pruned, "
"will be autocompleted by NNI.     - exclude : Set True then the layers "
"setting by op_types and op_names will be excluded from pruning."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:12
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:7
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:7
#: of
msgid ""
"total_sparsity : This is to specify the total sparsity for all layers in "
"this config, each layer may have different sparsity."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:13
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:8
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:8
#: of
msgid ""
"max_sparsity_per_layer : Always used with total_sparsity. Limit the max "
"sparsity of each layer."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:9 of
msgid "op_types : Only BatchNorm2d is supported in SlimPruner."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:20
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:14
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:14
#: nni.algorithms.compression.v2.pytorch.pruning.movement_pruner.MovementPruner:12
#: of
msgid ""
"A callable function used to train model or just inference. Take model, "
"optimizer, criterion as input. The model will be trained or inferenced "
"`training_epochs` epochs.  Example::      def trainer(model: Module, "
"optimizer: Optimizer, criterion: Callable[[Tensor, Tensor], Tensor]):"
"         training = model.training         model.train(mode=True)"
"         device = torch.device(\"cuda\" if torch.cuda.is_available() else"
" \"cpu\")         for batch_idx, (data, target) in "
"enumerate(train_loader):             data, target = data.to(device), "
"target.to(device)             optimizer.zero_grad()             output = "
"model(data)             loss = criterion(output, target)             "
"loss.backward()             # If you don't want to update the model, you "
"can skip `optimizer.step()`, and set train mode False.             "
"optimizer.step()         model.train(mode=training)"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:12
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:20
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:14
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:14
#: nni.algorithms.compression.v2.pytorch.pruning.movement_pruner.MovementPruner:12
#: of
msgid ""
"A callable function used to train model or just inference. Take model, "
"optimizer, criterion as input. The model will be trained or inferenced "
"`training_epochs` epochs."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:23
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:17
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:17
#: nni.algorithms.compression.v2.pytorch.pruning.movement_pruner.MovementPruner:15
#: of
msgid "Example::"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:15
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:39
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:33
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:33
#: nni.algorithms.compression.v2.pytorch.pruning.movement_pruner.MovementPruner:31
#: of
msgid ""
"The traced optimizer instance which the optimizer class is wrapped by "
"nni.trace. E.g. traced_optimizer = "
"nni.trace(torch.nn.Adam)(model.parameters())."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:18
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:42
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:36
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:36
#: nni.algorithms.compression.v2.pytorch.pruning.movement_pruner.MovementPruner:34
#: of
msgid ""
"The criterion function used in trainer. Take model output and target "
"value as input, and return the loss."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:38 of
msgid "The epoch number for training model to sparsify the BN weight."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:40 of
msgid "Penalty parameter for sparsification, which could reduce overfitting."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.SlimPruner:42 of
msgid ""
"'normal' or 'global'. If prune the model in a global way, all layer "
"weights with same config will be considered uniformly. That means a "
"single layer may not reach or exceed the sparsity setting in config, but "
"the total pruned weights meet the sparsity setting."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:178
msgid "Activation APoZ Rank Pruner"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:180
msgid ""
"Activation APoZ rank pruner is a pruner which prunes on the first weight "
"dimension, with the smallest importance criterion ``APoZ`` calculated "
"from the output activations of convolution layers to achieve a preset "
"level of network sparsity. The pruning criterion ``APoZ`` is explained in"
" the paper `Network Trimming: A Data-Driven Neuron Pruning Approach "
"towards Efficient Deep Architectures "
"<https://arxiv.org/abs/1607.03250>`__."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:188
msgid "Activation APoZ rank pruner also supports dependency-aware mode."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:205
#: ../../Compression/v2_pruning_algo.rst:238
msgid ""
"For detailed example please refer to "
":githublink:`examples/model_compress/pruning/v2/activation_pruning_torch.py"
" <examples/model_compress/pruning/v2/activation_pruning_torch.py>`"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:208
msgid "User configuration for Activation APoZ Rank Pruner"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:215
msgid "Activation Mean Rank Pruner"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:217
msgid ""
"Activation mean rank pruner is a pruner which prunes on the first weight "
"dimension, with the smallest importance criterion ``mean activation`` "
"calculated from the output activations of convolution layers to achieve a"
" preset level of network sparsity. The pruning criterion ``mean "
"activation`` is explained in section 2.2 of the paper `Pruning "
"Convolutional Neural Networks for Resource Efficient Inference "
"<https://arxiv.org/abs/1611.06440>`__."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:221
msgid "Activation mean rank pruner also supports dependency-aware mode."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:241
#: ../../Compression/v2_pruning_algo.rst:278
msgid "User configuration for Activation Mean Rank Pruner"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:248
msgid "Taylor FO Weight Pruner"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:250
msgid ""
"Taylor FO weight pruner is a pruner which prunes on the first weight "
"dimension, based on estimated importance calculated from the first order "
"taylor expansion on weights to achieve a preset level of network "
"sparsity. The estimated importance is defined as the paper `Importance "
"Estimation for Neural Network Pruning "
"<http://jankautz.com/publications/Importance4NNPruning_CVPR19.pdf>`__."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:256
msgid "Taylor FO weight pruner also supports dependency-aware mode."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:258
msgid ""
"What's more, we provide a global-sort mode for this pruner which is "
"aligned with paper implementation."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:275
msgid ""
"For detailed example please refer to "
":githublink:`examples/model_compress/pruning/v2/taylorfo_pruning_torch.py"
" <examples/model_compress/pruning/v2/taylorfo_pruning_torch.py>`"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:3
#: of
msgid ""
"Supported keys:     - sparsity : This is to specify the sparsity for each"
" layer in this config to be compressed.     - sparsity_per_layer : Equals"
" to sparsity.     - total_sparsity : This is to specify the total "
"sparsity for all layers in this config, each layer may have different "
"sparsity.     - max_sparsity_per_layer : Always used with total_sparsity."
" Limit the max sparsity of each layer.     - op_types : Conv2d and Linear"
" are supported in TaylorFOWeightPruner.     - op_names : Operation names "
"to be pruned.     - op_partial_names: Operation partial names to be "
"pruned, will be autocompleted by NNI.     - exclude : Set True then the "
"layers setting by op_types and op_names will be excluded from pruning."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:9
#: of
msgid "op_types : Conv2d and Linear are supported in TaylorFOWeightPruner."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:38
#: of
msgid "The batch number used to collect activations."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:40
#: of
msgid ""
"'normal', 'dependency_aware' or 'global'.  If prune the model in a "
"dependency-aware way, this pruner will prune the model according to the "
"taylorFO and the channel-dependency or group-dependency of the model. In "
"this way, the pruner will force the conv layers that have dependencies to"
" prune the same channels, so the speedup module can better harvest the "
"speed benefit from the pruned model. Note that, if set 'dependency_aware'"
" , the dummy_input cannot be None, because the pruner needs a dummy input"
" to trace the dependency between the conv layers.  If prune the model in "
"a global way, all layer weights with same config will be considered "
"uniformly. That means a single layer may not reach or exceed the sparsity"
" setting in config, but the total pruned weights meet the sparsity "
"setting."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:40
#: of
msgid "'normal', 'dependency_aware' or 'global'."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:42
#: of
msgid ""
"If prune the model in a dependency-aware way, this pruner will prune the "
"model according to the taylorFO and the channel-dependency or group-"
"dependency of the model. In this way, the pruner will force the conv "
"layers that have dependencies to prune the same channels, so the speedup "
"module can better harvest the speed benefit from the pruned model. Note "
"that, if set 'dependency_aware' , the dummy_input cannot be None, because"
" the pruner needs a dummy input to trace the dependency between the conv "
"layers."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.TaylorFOWeightPruner:50
#: of
msgid ""
"If prune the model in a global way, all layer weights with same config "
"will be considered uniformly. That means a single layer may not reach or "
"exceed the sparsity setting in config, but the total pruned weights meet "
"the sparsity setting."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:291
msgid ""
"During the process of solving these two subproblems, the weights of the "
"original model will be changed. Then a fine-grained pruning will be "
"applied to prune the model according to the config list given."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:313
msgid ""
"For detailed example please refer to "
":githublink:`examples/model_compress/pruning/v2/admm_pruning_torch.py "
"<examples/model_compress/pruning/v2/admm_pruning_torch.py>`"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:1 of
msgid ""
"ADMM (Alternating Direction Method of Multipliers) Pruner is a kind of "
"mathematical optimization technique. The metric used in this pruner is "
"the absolute value of the weight. In each iteration, the weight with "
"small magnitudes will be set to zero. Only in the final iteration, the "
"mask will be generated and apply to model wrapper."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:6 of
msgid "The original paper refer to: https://arxiv.org/abs/1804.03294."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:10 of
msgid ""
"Supported keys:     - sparsity : This is to specify the sparsity for each"
" layer in this config to be compressed.     - sparsity_per_layer : Equals"
" to sparsity.     - rho : Penalty parameters in ADMM algorithm.     - "
"op_types : Operation types to be pruned.     - op_names : Operation names"
" to be pruned.     - op_partial_names: Operation partial names to be "
"pruned, will be autocompleted by NNI.     - exclude : Set True then the "
"layers setting by op_types and op_names will be excluded from pruning."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:14 of
msgid "rho : Penalty parameters in ADMM algorithm."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:20
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:44 of
msgid "The total iteration number in admm pruning algorithm."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:22
#: nni.algorithms.compression.v2.pytorch.pruning.basic_pruner.ADMMPruner:46 of
msgid "The epoch number for training model in each iteration."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:323
msgid "Movement Pruner"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:325
msgid ""
"Movement pruner is an implementation of movement pruning. This is a "
"\"fine-pruning\" algorithm, which means the masks may change during each "
"fine-tuning step. Each weight element will be scored by the opposite of "
"the sum of the product of weight and its gradient during each step. This "
"means the weight elements moving towards zero will accumulate negative "
"scores, the weight elements moving away from zero will accumulate "
"positive scores. The weight elements with low scores will be masked "
"during inference."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:331
msgid ""
"The following figure from the paper shows the weight pruning by movement "
"pruning."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:337
msgid ""
"For more details, please refer to `Movement Pruning: Adaptive Sparsity by"
" Fine-Tuning <https://arxiv.org/abs/2005.07683>`__."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:355
msgid "User configuration for Movement Pruner"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.movement_pruner.MovementPruner:36
#: of
msgid ""
"The total epoch number for training the model. Make sure the total "
"`optimizer.step()` in `training_epochs` is bigger than "
"`cool_down_beginning_step`."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.movement_pruner.MovementPruner:39
#: of
msgid ""
"The total `optimizer.step()` number before start pruning for warm up. "
"Make sure `warm_up_step` is smaller than `cool_down_beginning_step`."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.movement_pruner.MovementPruner:42
#: of
msgid ""
"The number of steps at which sparsity stops growing, note that the "
"sparsity stop growing doesn't mean masks not changed. The sparsity after "
"each `optimizer.step()` is: total_sparsity * (1 - (1 - (current_step - "
"warm_up_step) / (cool_down_beginning_step - warm_up_step)) ** 3)."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:369
msgid "Dataset"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:370
msgid "Remaining Weights"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:371
msgid "MaP acc.(paper/ours)"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:372
msgid "MvP acc.(paper/ours)"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:373
msgid "Bert base"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:374
msgid "MNLI - Dev"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:375
msgid "10%"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:376
msgid "77.8% / 73.6%"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:377
msgid "79.3% / 78.8%"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:380
msgid "Linear Pruner"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:382
msgid ""
"Linear pruner is an iterative pruner, it will increase sparsity evenly "
"from scratch during each iteration. For example, the final sparsity is "
"set as 0.5, and the iteration number is 5, then the sparsity used in each"
" iteration are ``[0, 0.1, 0.2, 0.3, 0.4, 0.5]``."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:396
#: ../../Compression/v2_pruning_algo.rst:426
#: ../../Compression/v2_pruning_algo.rst:467
msgid ""
"For detailed example please refer to "
":githublink:`examples/model_compress/pruning/v2/iterative_pruning_torch.py"
" <examples/model_compress/pruning/v2/iterative_pruning_torch.py>`"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:399
msgid "User configuration for Linear Pruner"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:1
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.AGPPruner:1
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LinearPruner:1
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LotteryTicketPruner:1
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.SimulatedAnnealingPruner:1
#: of
msgid "The origin unwrapped pytorch model to be pruned."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:3
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.AGPPruner:3
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LinearPruner:3
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LotteryTicketPruner:3
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.SimulatedAnnealingPruner:3
#: of
msgid "The origin config list provided by the user."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.AGPPruner:5
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LinearPruner:5
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LotteryTicketPruner:5
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.SimulatedAnnealingPruner:15
#: of
msgid ""
"Supported pruning algorithm ['level', 'l1', 'l2', 'fpgm', 'slim', 'apoz',"
" 'mean_activation', 'taylorfo', 'admm']. This iterative pruner will use "
"the chosen corresponding pruner to prune the model in each iteration."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:5
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.AGPPruner:8
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LinearPruner:8
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LotteryTicketPruner:8
#: of
msgid "The total iteration number."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:26
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.AGPPruner:10
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LinearPruner:10
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LotteryTicketPruner:10
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.SimulatedAnnealingPruner:20
#: of
msgid ""
"The log directory use to saving the result, you can find the best result "
"under this folder."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:28
#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:43
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.AGPPruner:12
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LinearPruner:12
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LotteryTicketPruner:12
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.SimulatedAnnealingPruner:22
#: of
msgid ""
"If keeping the intermediate result, including intermediate model and "
"masks during each iteration."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.AGPPruner:14
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LinearPruner:14
#: of
msgid ""
"The finetuner handled all finetune logic, use a pytorch module as input. "
"It will be called at the end of each iteration, usually for neutralizing "
"the accuracy loss brought by the pruning in this iteration."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:48
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.AGPPruner:17
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LinearPruner:17
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LotteryTicketPruner:17
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.SimulatedAnnealingPruner:26
#: of
msgid ""
"If set True, speed up the model at the end of each iteration to make the "
"pruned model compact."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:50
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.AGPPruner:19
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LinearPruner:19
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LotteryTicketPruner:19
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.SimulatedAnnealingPruner:28
#: of
msgid ""
"If `speed_up` is True, `dummy_input` is required for tracing the model in"
" speed up."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.AGPPruner:21
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LinearPruner:21
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LotteryTicketPruner:21
#: of
msgid ""
"Evaluate the pruned model and give a score. If evaluator is None, the "
"best result refers to the latest result."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:39
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.AGPPruner:24
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LinearPruner:24
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LotteryTicketPruner:26
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.SimulatedAnnealingPruner:18
#: of
msgid ""
"If the chosen pruning_algorithm has extra parameters, put them as a dict "
"to pass in."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:408
msgid ""
"This is an iterative pruner, which the sparsity is increased from an "
"initial sparsity value :math:`s_{i}` (usually 0) to a final sparsity "
"value :math:`s_{f}` over a span of :math:`n` pruning iterations, starting"
" at training step :math:`t_{0}` and with pruning frequency :math:`\\Delta"
" t`:"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:436
msgid "Lottery Ticket Pruner"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:470
msgid "User configuration for Lottery Ticket Pruner"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LotteryTicketPruner:14
#: of
msgid ""
"The finetuner handled all finetune logic, use a pytorch module as input. "
"It will be called at the end of each iteration if reset_weight is False, "
"will be called at the beginning of each iteration otherwise."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.LotteryTicketPruner:24
#: of
msgid ""
"If set True, the model weight will reset to the original model weight at "
"the end of each iteration step."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:477
msgid "Simulated Annealing Pruner"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:479
msgid ""
"We implement a guided heuristic search method, Simulated Annealing (SA) "
"algorithm. As mentioned in the paper, this method is enhanced on guided "
"search based on prior experience. The enhanced SA technique is based on "
"the observation that a DNN layer with more number of weights often has a "
"higher degree of model compression with less impact on overall accuracy."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:503
msgid ""
"For detailed example please refer to "
":githublink:`examples/model_compress/pruning/v2/simulated_anealing_pruning_torch.py"
" "
"<examples/model_compress/pruning/v2/simulated_anealing_pruning_torch.py>`"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:506
msgid "User configuration for Simulated Annealing Pruner"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:21
#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:7
#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:27
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.SimulatedAnnealingPruner:5
#: of
msgid "Evaluate the pruned model and give a score."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:30
#: nni.algorithms.compression.v2.pytorch.pruning.iterative_pruner.SimulatedAnnealingPruner:24
#: of
msgid ""
"The finetuner handled all finetune logic, use a pytorch module as input, "
"will be called in each iteration."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:513
msgid "Auto Compress Pruner"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:515
msgid ""
"For total iteration number :math:`N`, AutoCompressPruner prune the model "
"that survive the previous iteration for a fixed sparsity ratio (e.g., "
":math:`1-{(1-0.8)}^{(1/N)}`) to achieve the overall sparsity (e.g., "
":math:`0.8`):"
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:550
msgid ""
"The full script can be found :githublink:`here "
"<examples/model_compress/pruning/v2/auto_compress_pruner.py>`."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:553
msgid "User configuration for Auto Compress Pruner"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:9
#: of
msgid ""
"The parameters passed to the ADMMPruner.  - trainer : Callable[[Module, "
"Optimizer, Callable].     A callable function used to train model or just"
" inference. Take model, optimizer, criterion as input.     The model will"
" be trained or inferenced `training_epochs` epochs. - traced_optimizer : "
"nni.common.serializer.Traceable(torch.optim.Optimizer)     The traced "
"optimizer instance which the optimizer class is wrapped by nni.trace."
"     E.g. traced_optimizer = "
"nni.trace(torch.nn.Adam)(model.parameters()). - criterion : "
"Callable[[Tensor, Tensor], Tensor].     The criterion function used in "
"trainer. Take model output and target value as input, and return the "
"loss. - iterations : int.     The total iteration number in admm pruning "
"algorithm. - training_epochs : int.     The epoch number for training "
"model in each iteration."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:9
#: of
msgid "The parameters passed to the ADMMPruner."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:12
#: of
msgid "trainer"
msgstr ""

#: of
msgid "Callable[[Module, Optimizer, Callable]."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:15
#: of
msgid "traced_optimizer"
msgstr ""

#: of
msgid "nni.common.serializer.Traceable(torch.optim.Optimizer)"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:17
#: of
msgid "criterion"
msgstr ""

#: of
msgid "Callable[[Tensor, Tensor], Tensor]."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:19
#: of
msgid "iterations"
msgstr ""

#: of
msgid "int."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:21
#: of
msgid "training_epochs"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:24
#: of
msgid ""
"The parameters passed to the SimulatedAnnealingPruner.  - evaluator : "
"Callable[[Module], float]. Required.     Evaluate the pruned model and "
"give a score. - start_temperature : float. Default: `100`.     Start "
"temperature of the simulated annealing process. - stop_temperature : "
"float. Default: `20`.     Stop temperature of the simulated annealing "
"process. - cool_down_rate : float. Default: `0.9`.     Cooldown rate of "
"the temperature. - perturbation_magnitude : float. Default: `0.35`.     "
"Initial perturbation magnitude to the sparsities. The magnitude decreases"
" with current temperature. - pruning_algorithm : str. Default: `'level'`."
"     Supported pruning algorithm ['level', 'l1', 'l2', 'fpgm', 'slim', "
"'apoz', 'mean_activation', 'taylorfo', 'admm']. - pruning_params : Dict. "
"Default: `{}`.     If the chosen pruning_algorithm has extra parameters, "
"put them as a dict to pass in."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:24
#: of
msgid "The parameters passed to the SimulatedAnnealingPruner."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:26
#: of
msgid "evaluator"
msgstr ""

#: of
msgid "Callable[[Module], float]. Required."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:28
#: of
msgid "start_temperature : float. Default: `100`."
msgstr ""

#: of
msgid "float. Default:"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:30
#: of
msgid "stop_temperature : float. Default: `20`."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:32
#: of
msgid "cool_down_rate : float. Default: `0.9`."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:33
#: of
msgid "Cooldown rate of the temperature."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:34
#: of
msgid "perturbation_magnitude : float. Default: `0.35`."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:36
#: of
msgid "pruning_algorithm : str. Default: `'level'`."
msgstr ""

#: of
msgid "str. Default:"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:37
#: of
msgid ""
"Supported pruning algorithm ['level', 'l1', 'l2', 'fpgm', 'slim', 'apoz',"
" 'mean_activation', 'taylorfo', 'admm']."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:38
#: of
msgid "pruning_params : Dict. Default: `{}`."
msgstr ""

#: of
msgid "Dict. Default:"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:41
#: of
msgid ""
"The log directory used to save the result, you can find the best result "
"under this folder."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.auto_compress_pruner.AutoCompressPruner:45
#: of
msgid ""
"The finetuner handles all finetune logic, takes a pytorch module as "
"input. It will be called at the end of each iteration, usually for "
"neutralizing the accuracy loss brought by the pruning in this iteration."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:562
msgid ""
"AMC pruner leverages reinforcement learning to provide the model "
"compression policy. According to the author, this learning-based "
"compression policy outperforms conventional rule-based compression policy"
" by having a higher compression ratio, better preserving the accuracy and"
" freeing human labor."
msgstr ""

#: ../../Compression/v2_pruning_algo.rst:580
msgid ""
"The full script can be found :githublink:`here "
"<examples/model_compress/pruning/v2/amc_pruning_torch.py>`."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:1 of
msgid ""
"A pytorch implementation of AMC: AutoML for Model Compression and "
"Acceleration on Mobile Devices. (https://arxiv.org/pdf/1802.03494.pdf) "
"Suggust config all `total_sparsity` in `config_list` a same value. AMC "
"pruner will treat the first sparsity in `config_list` as the global "
"sparsity."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:6 of
msgid "The total episode number."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:10 of
msgid ""
"Supported keys :     - total_sparsity : This is to specify the total "
"sparsity for all layers in this config, each layer may have different "
"sparsity.     - max_sparsity_per_layer : Always used with total_sparsity."
" Limit the max sparsity of each layer.     - op_types : Operation type to"
" be pruned.     - op_names : Operation name to be pruned.     - "
"op_partial_names: Operation partial names to be pruned, will be "
"autocompleted by NNI.     - exclude  : Set True then the layers setting "
"by op_types and op_names will be excluded from pruning."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:16 of
msgid "Supported keys :"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:14 of
msgid "op_types : Operation type to be pruned."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:15 of
msgid "op_names : Operation name to be pruned."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:17 of
msgid ""
"exclude  : Set True then the layers setting by op_types and op_names will"
" be excluded from pruning."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:19 of
msgid ""
"`dummy_input` is required for speed-up and tracing the model in RL "
"environment."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:23 of
msgid ""
"Supported pruning algorithm ['l1', 'l2', 'fpgm', 'apoz', "
"'mean_activation', 'taylorfo']. This iterative pruner will use the chosen"
" corresponding pruner to prune the model in each iteration."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:32 of
msgid ""
"Configuration dict to configure the DDPG agent, any key unset will be set"
" to default implicitly. - hidden1: hidden num of first fully connect "
"layer. Default: 300 - hidden2: hidden num of second fully connect layer. "
"Default: 300 - lr_c: learning rate for critic. Default: 1e-3 - lr_a: "
"learning rate for actor. Default: 1e-4 - warmup: number of episodes "
"without training but only filling the replay memory. During warmup "
"episodes, random actions ares used for pruning. Default: 100 - discount: "
"next Q value discount for deep Q value target. Default: 0.99 - bsize: "
"minibatch size for training DDPG agent. Default: 64 - rmsize: memory size"
" for each layer. Default: 100 - window_length: replay buffer window "
"length. Default: 1 - tau: moving average for target network being used by"
" soft_update. Default: 0.99 - init_delta: initial variance of truncated "
"normal distribution. Default: 0.5 - delta_decay: delta decay during "
"exploration. Default: 0.99 # parameters for training ddpg agent - "
"max_episode_length: maximum episode length. Default: 1e9 - epsilon: "
"linear decay of exploration policy. Default: 50000"
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:49 of
msgid ""
"If the pruner corresponding to the chosen pruning_algorithm has extra "
"parameters, put them as a dict to pass in."
msgstr ""

#: nni.algorithms.compression.v2.pytorch.pruning.amc_pruner.AMCPruner:51 of
msgid ""
"'flops' or 'params'. Note that the sparsity in other pruners always means"
" the parameters sparse, but in AMC, you can choose flops sparse. This "
"parameter is used to explain what the sparsity setting in config_list "
"refers to."
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:2
msgid "Pruning Config Specification"
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:5
msgid "The Keys in Config List"
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:7
msgid ""
"Each sub-config in the config list is a dict, and the scope of each "
"setting (key) is only internal to each sub-config. If multiple sub-"
"configs are configured for the same layer, the later ones will overwrite "
"the previous ones."
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:11
msgid "op_types"
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:13
msgid ""
"The type of the layers targeted by this sub-config. If ``op_names`` is "
"not set in this sub-config, all layers in the model that satisfy the type"
" will be selected. If ``op_names`` is set in this sub-config, the "
"selected layers should satisfy both type and name."
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:18
msgid "op_names"
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:20
msgid ""
"The name of the layers targeted by this sub-config. If ``op_types`` is "
"set in this sub-config, the selected layer should satisfy both type and "
"name."
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:24
msgid "op_partial_names"
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:26
msgid ""
"This key is for the layers to be pruned with names that have the same "
"sub-string. NNI will find all names in the model, find names that contain"
" one of ``op_partial_names``, and append them into the ``op_names``."
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:30
msgid "sparsity_per_layer"
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:32
msgid "The sparsity ratio of each selected layer."
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:34
msgid ""
"e.g., the ``sparsity_per_layer`` is 0.8 means each selected layer will "
"mask 80% values on the weight. If ``layer_1`` (500 parameters) and "
"``layer_2`` (1000 parameters) are selected in this sub-config, then "
"``layer_1`` will be masked 400 parameters and ``layer_2`` will be masked "
"800 parameters."
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:39
msgid "total_sparsity"
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:41
msgid ""
"The sparsity ratio of all selected layers, means that sparsity ratio may "
"no longer be even between layers."
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:43
#, python-format
msgid ""
"e.g., the ``total_sparsity`` is 0.8 means 80% of parameters in this sub-"
"config will be masked. If ``layer_1`` (500 parameters) and ``layer_2`` "
"(1000 parameters) are selected in this sub-config, then ``layer_1`` and "
"``layer_2`` will be masked a total of 1200 parameters, how these total "
"parameters are distributed between the two layers is determined by the "
"pruning algorithm."
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:49
msgid "sparsity"
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:51
msgid ""
"``sparsity`` is an old config key from the pruning v1, it has the same "
"meaning as ``sparsity_per_layer``. You can also use ``sparsity`` right "
"now, but it will be deprecated in the future."
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:55
msgid "max_sparsity_per_layer"
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:57
msgid ""
"This key is usually used with ``total_sparsity``. It limits the maximum "
"sparsity ratio of each layer."
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:59
msgid ""
"In ``total_sparsity`` example, there are 1200 parameters that need to be "
"masked and all parameters in ``layer_1`` may be totally masked. To avoid "
"this situation, ``max_sparsity_per_layer`` can be set as 0.9, this means "
"up to 450 parameters can be masked in ``layer_1``, and 900 parameters can"
" be masked in ``layer_2``."
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:64
msgid "exclude"
msgstr ""

#: ../../Compression/v2_pruning_config_list.rst:66
msgid ""
"The ``exclude`` and ``sparsity`` keyword are mutually exclusive and "
"cannot exist in the same sub-config. If ``exclude`` is set in sub-config,"
" the layers selected by this config will not be pruned."
msgstr ""

#: ../../Compression/v2_scheduler.rst:4
msgid ""
"Pruning scheduler is new feature supported in pruning v2. It can bring "
"more flexibility for pruning the model iteratively. All the built-in "
"iterative pruners (e.g., AGPPruner, SimulatedAnnealingPruner) are based "
"on three abstracted components: pruning scheduler, pruners and task "
"generators. In addition to using the NNI built-in iterative pruners, "
"users can directly use the pruning schedulers to customize their own "
"iterative pruning logic."
msgstr ""

#: ../../Compression/v2_scheduler.rst:10
msgid "Workflow of Pruning Scheduler"
msgstr ""

#: ../../Compression/v2_scheduler.rst:12
msgid ""
"In iterative pruning, the final goal will be broken down into different "
"small goals, and complete a small goal in each iteration. For example, "
"each iteration increases a little sparsity ratio, and after several "
"pruning iterations, the continuous pruned model reaches the final overall"
" sparsity; fix the overall sparsity, try different ways to allocate "
"sparsity between layers in each iteration, and find the best allocation "
"way."
msgstr ""

#: ../../Compression/v2_scheduler.rst:16
msgid ""
"We define a small goal as ``Task``, it usually includes states inherited "
"from previous iterations (eg. pruned model and masks) and description of "
"the current goal (eg. a config list that describes how to allocate "
"sparsity). Details about ``Task`` can be found in this :githublink:`file "
"<nni/algorithms/compression/v2/pytorch/base/scheduler.py>`."
msgstr ""

#: ../../Compression/v2_scheduler.rst:19
msgid ""
"Pruning scheduler handles two main components, a basic pruner, and a task"
" generator. The logic of generating ``Task`` is encapsulated in the task "
"generator. In an iteration (one pruning step), pruning scheduler parses "
"the ``Task`` getting from the task generator, and reset the pruner by "
"``model``, ``masks``, ``config_list`` parsing from the ``Task``. Then "
"pruning scheduler generates the new masks by the pruner. During an "
"iteration, the new masked model may also experience speed-up, finetuning,"
" and evaluating. After one iteration is done, the pruning scheduler "
"collects the compact model, new masks and evaluation score, packages them"
" into ``TaskResult``, and passes it to task generator. The iteration "
"process will end until the task generator has no more ``Task``."
msgstr ""

#: ../../Compression/v2_scheduler.rst:27
msgid "How to Customized Iterative Pruning"
msgstr ""

#: ../../Compression/v2_scheduler.rst:29
msgid ""
"Using AGP Pruning as an example to explain how to implement an iterative "
"pruning by scheduler in NNI."
msgstr ""

#: ../../Compression/v2_scheduler.rst:43
msgid ""
"The full script can be found :githublink:`here "
"<examples/model_compress/pruning/v2/scheduler_torch.py>`."
msgstr ""

#: ../../Compression/v2_scheduler.rst:45
msgid ""
"In this example, we use ``dependency_aware`` mode L1 Norm Pruner as a "
"basic pruner during each iteration. Note we do not need to pass ``model``"
" and ``config_list`` to the pruner, because in each iteration the "
"``model`` and ``config_list`` used by the pruner are received from the "
"task generator. Then we can use ``scheduler`` as an iterative pruner "
"directly. In fact, this is the implementation of ``AGPPruner`` in NNI."
msgstr ""

#: ../../Compression/v2_scheduler.rst:50
msgid "More about Task Generator"
msgstr ""

#: ../../Compression/v2_scheduler.rst:52
msgid ""
"The task generator is used to give the model that needs to be pruned in "
"each iteration and the corresponding config_list. For example, "
"``AGPTaskGenerator`` will give the model pruned in the previous iteration"
" and compute the sparsity using in the current iteration. "
"``TaskGenerator`` put all these pruning information into ``Task`` and "
"pruning scheduler will get the ``Task``, then run it. The pruning result "
"will return to the ``TaskGenerator`` at the end of each iteration and "
"``TaskGenerator`` will judge whether and how to generate the next "
"``Task``."
msgstr ""

#: ../../Compression/v2_scheduler.rst:57
msgid ""
"The information included in the ``Task`` and ``TaskResult`` can be found "
":githublink:`here "
"<nni/algorithms/compression/v2/pytorch/base/scheduler.py>`."
msgstr ""

#: ../../Compression/v2_scheduler.rst:59
msgid ""
"A clearer iterative pruning flow chart can be found `here "
"<v2_pruning.rst>`__."
msgstr ""

#: ../../Compression/v2_scheduler.rst:61
msgid ""
"If you want to implement your own task generator, please following the "
"``TaskGenerator`` :githublink:`interface "
"<nni/algorithms/compression/v2/pytorch/pruning/tools/base.py>`. Two main "
"functions should be implemented, ``init_pending_tasks(self) -> "
"List[Task]`` and ``generate_tasks(self, task_result: TaskResult) -> "
"List[Task]``."
msgstr ""

#: ../../Compression/v2_scheduler.rst:65
msgid "Why Use Pruning Scheduler"
msgstr ""

#: ../../Compression/v2_scheduler.rst:67
msgid ""
"One of the benefits of using a scheduler to do iterative pruning is users"
" can use more functions of NNI pruning components, because of simplicity "
"of the interface and the restoration of the paper, NNI not fully exposing"
" all the low-level interfaces to the upper layer. For example, resetting "
"weight value to the original model in each iteration is a key point in "
"lottery ticket pruning algorithm, and this is implemented in "
"``LotteryTicketPruner``. To reduce the complexity of the interface, we "
"only support this function in ``LotteryTicketPruner``, not other pruners."
" If users want to reset weight during each iteration in AGP pruning, "
"``AGPPruner`` can not do this, but users can easily set "
"``reset_weight=True`` in ``PruningScheduler`` to implement this."
msgstr ""

#: ../../Compression/v2_scheduler.rst:73
msgid ""
"What's more, for a customized pruner or task generator, using scheduler "
"can easily enhance the algorithm. In addition, users can also customize "
"the scheduling process to implement their own scheduler."
msgstr ""

