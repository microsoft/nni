PBT Tuners on NNI
===

## PBTTuner

Population Based Training(PBT) comes from [Population Based Training of Neural Networks](https://arxiv.org/abs/1711.09846v1). It's a simple asynchronous optimization algorithm which effectively utilizes a fixed computational budget to jointly optimize a population of models and their hyperparameters to maximize performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. PBTTuner will initialize a population with several trials. You can set a specific number of training epochs. After a certain number of epochs, the parameters and hyperparameters in the trial with bad metrics will be replaced with a better trial (exploit). Then the hyperparameters are purturbed (explore). In our implementation, the checkpoint is not assigned explicitly, but by continuously changing load_checkpoint_dir and save_checkpoint_dir, we can directly change load_checkpoint_dir each time we perform an exploitation. Therefore, these two directory need to be accessible by all trials. If the experiment is local mode, users could provide all_checkpoint_dir which decides load_checkpoint_dir and save_checkpoint_dir, otherwise the directory would be "~/nni/checkpoint/<exp-id>". If the experiment is not local mode, then users should provide a path in a shared storage which can be accessed by all the trials as all_checkpoint_dir. 