# Trials

## Overview

In NNI, hyper-parameter searching space will be first defined using a json file (often called searchspace.json). According to the search space defined by this file, different hyper-parameter combinations will be generated to try and get feedback information like final accuracy.

A **Trial** in NNI is such an individual attempt at applying a set of hyper-parameters on a model. 



NNI provides two approaches for you to define a trial: `NNI API` and `NNI Python annotation`.

## NNI API
> Step 1 - Prepare a SearchSpace parameters file. 

- First we should define a SearchSpace file so NNI could generate different sets of hyper-parameter, each of which is used to run an individual trial.

  - An example of a SearchSpace parameters file is shown below: 
    ```
    {
        "dropout_rate":{"_type":"uniform","_value":[0.1,0.5]},
        "conv_size":{"_type":"choice","_value":[2,3,5,7]},
        "hidden_size":{"_type":"choice","_value":[124, 512, 1024]},
        "learning_rate":{"_type":"uniform","_value":[0.0001, 0.1]}
    }
    ```

  - And a set of hyper-parameter, or a hyper-parameter combination generated by the search space above might be:
    ```
    {
        "dropout_rate": 0.3,
        "conv_size: 2,
        "hidden_size: 124,
        "learning_rate": 0.01
    }
    ```

- For more information about search space, please refer to [SearchSpaceSpec.md](SearchSpaceSpec.md).

> Step 2 - Update model codes

- Declare NNI API
    Include `import nni` in your trial code to use NNI APIs. 

- Get predefined parameters
    
      - Use `RECEIVED_PARAMS = nni.get_next_parameter()` to get hyper-parameters' values assigned by tuner.
      - Note that `RECEIVED_PARAMS` is an object, for example: 

        {"conv_size": 2, "hidden_size": 124, "learning_rate": 0.0307, "dropout_rate": 0.2029}

- Report NNI results

   - Trial should report `metrics` so NNI could evaluate the current set of hyper-parameters. `metrics` might be things like accuracy or loss.
   - `metrics` could be any python object, but NNI built-in tuners/assessors only receive `metrics` that is:
     - a numerical variable (e.g. float, int).
     - a dict object that has a key named "default" whose value is a numerical variable, which will be used as the default metric
   - Use `nni.report_intermediate_result(metrics)` to send `metrics` to [assessor](Assessors.md). (Optional)
   - Use `nni.report_final_result(metrics)` to send `metrics` to [tuner](Tuners.md). 



> Step 3 - Enable NNI API

- To enable NNI API mode, you need to set useAnnotation to *false* and provide the path of SearchSpace file (you just defined in step 1):

    ```
    useAnnotation: false
    searchSpacePath: /path/to/your/search_space.json
    ```

- You can refer to [here](ExperimentConfig.md) for more information about how to set up experiment configurations.


## NNI Python Annotation

An alternative to writing a trial is to use NNI's syntax for python. Simple as any annotation, NNI annotation is working like comments in your codes. You don't have to make structure or any other big changes to your existing codes. With a few lines of NNI annotation, you will be able to:

* annotate the variables you want to tune 
* specify in which range you want to tune the variables
* annotate which variable you want to report as intermediate result to `assessor`
* annotate which variable you want to report as the final result (e.g. model accuracy) to `tuner`. 

Again, take MNIST as an example, it only requires 2 steps to write a trial with NNI Annotation.

> Step 1 - Update codes with annotations 

The following is a tensorflow code snippet for NNI Annotation, where the highlighted four lines are annotations that help you to: 
  1. tune batch\_size and dropout\_rate
  2. report test\_acc every 100 steps
  3. at last report test\_acc as final result.

What noteworthy is: as these newly added codes are annotations, it does not actually change your previous codes logic, you can still run your code as usual in environments without NNI installed.

```diff
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
+   """@nni.variable(nni.choice(50, 250, 500), name=batch_size)"""
    batch_size = 128
    for i in range(10000):
        batch = mnist.train.next_batch(batch_size)
+       """@nni.variable(nni.choice(1, 5), name=dropout_rate)"""
        dropout_rate = 0.5
        mnist_network.train_step.run(feed_dict={mnist_network.images: batch[0],
                                                mnist_network.labels: batch[1],
                                                mnist_network.keep_prob: dropout_rate})
        if i % 100 == 0:
            test_acc = mnist_network.accuracy.eval(
                feed_dict={mnist_network.images: mnist.test.images,
                            mnist_network.labels: mnist.test.labels,
                            mnist_network.keep_prob: 1.0})
+           """@nni.report_intermediate_result(test_acc)"""

    test_acc = mnist_network.accuracy.eval(
        feed_dict={mnist_network.images: mnist.test.images,
                    mnist_network.labels: mnist.test.labels,
                    mnist_network.keep_prob: 1.0})
+   """@nni.report_final_result(test_acc)"""
```

**NOTE**: 
- `@nni.variable` will take effect on its following line, which is an assignment statement whose leftvalue must be specified by the keyword `name` in `@nni.variable`.
- `@nni.report_intermediate_result`/`@nni.report_final_result` will send the data to assessor/tuner at that line. 

For more information about annotation syntax and its usage, please refer to [Annotation README](../tools/nni_annotation/README.md) . 


>Step 2 - Enable NNI Annotation

In the yaml configure file, you need to set *useAnnotation* to true to enable NNI annotation:
```
useAnnotation: true
```

## Others

### Output

NNI is designed to show each trial's stdout and stderr in the log region of WebUI. However, currently this feature is supported only on PAI and k8s platforms. On other platforms NNI will show trial log path and users can temporarily check the output manually.

### Advance APIs

#### get_sequence_id

- Every trial has a sequence id, which is their unique identifier (a numeric value increasing from zero).
- This function receives no argument and will return the sequence id of the current trial, which can be used to perform task like k-fold validation

You can use it like `trial_id = nni.get_sequence_id()`

## More Trial Example
* [Automatic Model Architecture Search for Reading Comprehension.](../examples/trials/ga_squad/README.md)
