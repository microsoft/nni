{"bert.encoder.layer.0.attention.self.query": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=64;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=8;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [256, 1]}, "block_size_n": 16, "block_size_k": 128}, "bert.encoder.layer.0.attention.self.key": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=64;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=8;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [256, 1]}, "block_size_n": 16, "block_size_k": 128}, "bert.encoder.layer.0.attention.self.value": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=64;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=8;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [256, 1]}, "block_size_n": 16, "block_size_k": 128}, "bert.encoder.layer.0.attention.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=64;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.0.intermediate.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=3072;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=8;\n\n    const unsigned int BLOCK_ROW_WARPS=2;\n    const unsigned int BLOCK_COL_WARPS=1;\n\n    const unsigned int WARP_ROW_TILES =2;\n    const unsigned int WARP_COL_TILES =4;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 64, "block_size_k": 128}, "bert.encoder.layer.0.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=3072;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.1.attention.self.query": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.1.attention.self.key": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.1.attention.self.value": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.1.attention.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.1.intermediate.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=3072;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=8;\n\n    const unsigned int BLOCK_ROW_WARPS=2;\n    const unsigned int BLOCK_COL_WARPS=1;\n\n    const unsigned int WARP_ROW_TILES =2;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [6144, 1]}, "block_size_n": 64, "block_size_k": 128}, "bert.encoder.layer.1.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=3072;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.2.attention.self.query": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.2.attention.self.key": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.2.attention.self.value": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.2.attention.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.2.intermediate.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=3072;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [12288, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.2.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=3072;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.3.attention.self.query": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=64;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [256, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.3.attention.self.key": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=64;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [256, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.3.attention.self.value": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=64;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [256, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.3.attention.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=64;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.3.intermediate.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=3072;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [12288, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.3.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=3072;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.4.attention.self.query": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.4.attention.self.key": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.4.attention.self.value": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.4.attention.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.4.intermediate.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=3072;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [12288, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.4.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=3072;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.5.attention.self.query": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=64;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [256, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.5.attention.self.key": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=64;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [256, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.5.attention.self.value": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=64;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [256, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.5.attention.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=64;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.5.intermediate.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=3072;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [12288, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.5.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=3072;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.6.attention.self.query": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=64;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [256, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.6.attention.self.key": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=64;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [256, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.6.attention.self.value": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=64;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [256, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.6.attention.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=64;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.6.intermediate.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=3072;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [12288, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.6.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=3072;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.7.attention.self.query": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=64;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [256, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.7.attention.self.key": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=64;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [256, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.7.attention.self.value": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=64;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [256, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.7.attention.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=64;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.7.intermediate.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=3072;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [12288, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.7.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=3072;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.8.attention.self.query": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.8.attention.self.key": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.8.attention.self.value": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.8.attention.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.8.intermediate.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=3072;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [12288, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.8.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=3072;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.9.attention.self.query": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.9.attention.self.key": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.9.attention.self.value": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.9.attention.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=8;\n\n    const unsigned int BLOCK_ROW_WARPS=2;\n    const unsigned int BLOCK_COL_WARPS=1;\n\n    const unsigned int WARP_ROW_TILES =2;\n    const unsigned int WARP_COL_TILES =4;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [768, 1]}, "block_size_n": 64, "block_size_k": 128}, "bert.encoder.layer.9.intermediate.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=3072;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [12288, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.9.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=3072;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.10.attention.self.query": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.10.attention.self.key": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.10.attention.self.value": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.10.attention.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.10.intermediate.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=3072;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=2;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =4;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [6144, 1]}, "block_size_n": 16, "block_size_k": 32}, "bert.encoder.layer.10.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=3072;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.11.attention.self.query": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.11.attention.self.key": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.11.attention.self.value": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.11.attention.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.11.intermediate.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=3072;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [12288, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.encoder.layer.11.output.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=4096;\n    const unsigned int K_GLOBAL=3072;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [3072, 1]}, "block_size_n": 16, "block_size_k": 64}, "bert.pooler.dense": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=32;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=768;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=8;\n\n    const unsigned int BLOCK_ROW_WARPS=2;\n    const unsigned int BLOCK_COL_WARPS=1;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [24, 1]}, "block_size_n": 32, "block_size_k": 128}, "classifier": {"code": "extern \"C\" __global__ void MatrixMulCUDA_8bit_bias(float *input0, float *input1, float *input2, float *input3, float *input4, float *input5, float * input6, float *output0) \n{\n    const unsigned int M_GLOBAL=32;\n    const unsigned int K_GLOBAL=768;\n    const unsigned int N_GLOBAL=2;\n    // const parameters\n    const unsigned int  WARP_SIZE=32;\n    const unsigned int  M=16;\n    const unsigned int  N=16;\n    const unsigned int  K=16;\n    const unsigned int  WMMA_M=16;\n    const unsigned int  WMMA_N=16;\n    const unsigned int  WMMA_K=16;\n    /* \n    COMMENT_TAG\n    */\n    const unsigned int  M_TILES=(M_GLOBAL/M);\n    const unsigned int  K_TILES=(K_GLOBAL/K);\n    const unsigned int  N_TILES=(N_GLOBAL/N);\n\n\n\n    // typedef C_LAYOUT wmma::mem_row_major;\n    const unsigned int CHUNK_K=4;\n\n    const unsigned int BLOCK_ROW_WARPS=1;\n    const unsigned int BLOCK_COL_WARPS=2;\n\n    const unsigned int WARP_ROW_TILES =1;\n    const unsigned int WARP_COL_TILES =2;\n\n    const unsigned int WARPS_PER_BLOCK=BLOCK_COL_WARPS * BLOCK_ROW_WARPS;\n    const unsigned int THREADS_PER_BLOCK= (WARP_SIZE * WARPS_PER_BLOCK);\n    const unsigned int CHUNK_LINE_BYTES=(CHUNK_K * K * sizeof(uint8_t));\n    const unsigned int WARP_COPY_BYTES=(WARP_SIZE * sizeof(int4));\n    const unsigned int CHUNK_COPY_LINES_PER_WARP=(WARP_COPY_BYTES / CHUNK_LINE_BYTES);\n    const unsigned int CHUNK_COPY_LINE_LANES=(WARP_SIZE / CHUNK_COPY_LINES_PER_WARP);\n\n\n    const unsigned int LANE_ROW_STRIDE = (WARP_ROW_TILES * N / 8);\n    const unsigned int LANE_COL_STRIDE = (WARP_COL_TILES * M / 4);\n    const unsigned int WARP_STRIDE = (WARP_ROW_TILES * N);\n\n    const unsigned int BLOCK_ROW_TILES =(WARP_ROW_TILES * BLOCK_ROW_WARPS);\n    const unsigned int BLOCK_COL_TILES =(WARP_COL_TILES * BLOCK_COL_WARPS);\n\n    const unsigned int GLOBAL_MEM_STRIDE =N_GLOBAL;\n\n    const unsigned int SHMEM_STRIDE=(N * BLOCK_ROW_TILES);\n    const unsigned int SHMEM_OFFSET=(N * WARP_ROW_TILES);\n\n    const unsigned int OUTPUT_LINE_BYTES = (BLOCK_ROW_TILES * N);\n    const unsigned int OUTPUT_LANE_PER_LINE = OUTPUT_LINE_BYTES / 4;\n    const unsigned int OUTPUT_LINES_PER_BLOCK = THREADS_PER_BLOCK / OUTPUT_LANE_PER_LINE;\n    const unsigned int OUTPUT_ITERS = (BLOCK_COL_WARPS * WARP_COL_TILES * M) / OUTPUT_LINES_PER_BLOCK;\n\n    const unsigned int SKEW_UINT8=32;\n\n    const uint8_t * A = reinterpret_cast<uint8_t*>(input0); // activation\n    const uint8_t * B_val =  reinterpret_cast<uint8_t*>(input1); // weight\n    const int * B_row = reinterpret_cast< int *>(input2);\n    const int * B_col = reinterpret_cast< int *>(input3);\n    const int alpha = (int)(*input4);\n    const int integer = (int)(*input5);\n    int * bias = reinterpret_cast< int *>(input6);\n    uint8_t * D = reinterpret_cast<uint8_t*>(output0);\n\n  //extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];\n  const int shared_size = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M + BLOCK_ROW_TILES * N) *\n                       (CHUNK_K * K + SKEW_UINT8),\n                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *\n                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int));\n  __shared__ uint8_t shmem[shared_size/(CHUNK_K * K + SKEW_UINT8)+1][CHUNK_K * K + SKEW_UINT8];\n\n  // Warp and lane identification.\n  const unsigned int warpId = threadIdx.x / WARP_SIZE;\n  const unsigned int laneId = threadIdx.x % WARP_SIZE;\n\n  // Offset in shared memory from which the B matrix is stored.\n  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;       // BLOCK_COL_TILES * M is shared_A row numbers in one block\n\n\n  // Each CTA slides along the 128 x 128 tiles from the top left corner of the\n  // matrix to the right and down, and selects the next tile to compute. Once\n  // there's no such tile, all warps in this CTA exit.\n\n    unsigned int block_pos = blockIdx.x;\n    const unsigned int block_tile_i =                                   // get the i (row) index of all tiles\n        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);\n    const unsigned int block_tile_j = (block_pos * BLOCK_ROW_TILES) % N_TILES;\n\n    /////////////////// bias ///////////////////\n    int *shmem_load_output = ((int *)&shmem[0][0] + threadIdx.x / OUTPUT_LANE_PER_LINE * (BLOCK_ROW_TILES * N));\n    const size_t gmem_load_output = block_tile_j * N;\n    const int *src_gmem_output = (int *)(&bias[gmem_load_output]);\n\n    // Stream multiple C tiles to shared memory.\n#pragma unroll\n    for (int i = 0; i < OUTPUT_ITERS; i++) {\n      *((int4 *)(shmem_load_output) + threadIdx.x % OUTPUT_LANE_PER_LINE) =\n        *((int4 *)(src_gmem_output) + threadIdx.x % OUTPUT_LANE_PER_LINE);\n      shmem_load_output += OUTPUT_LINES_PER_BLOCK * (BLOCK_ROW_TILES * N);   \n    }\n\n    __syncthreads();\n    \n    // This pointer is used to access the C and D matrix tiles this warp computes.\n    int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +\n    (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES * SHMEM_STRIDE  +    // K * 2 is because one warp calculate k * 2 rows.\n    (warpId % BLOCK_ROW_WARPS) * SHMEM_OFFSET;\n    // Stop when there are no more D matrix tiles to compute in this CTA.\n\n\n    // This warp's pointer to the C matrix data to copy memory from to shared\n    // memory.\n\n    //__syncthreads();\n    // These fragments will accumulate the result of A and B matrix fragment\n    // multiplications along the K_GLOBAL dimension.\n    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]\n                                                     [WARP_ROW_TILES];\n\n\n    // Load the C matrix tiles into fragments from shared memory.\n    #pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n        const int *tile_ptr =\n            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;\n\n        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n\n    // Select what warp copies what matrix to shared memory.\n    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.\n\n    int start_tile = B_row[block_tile_j / BLOCK_ROW_TILES];\n    int end_tile = B_row[block_tile_j / BLOCK_ROW_TILES + 1];\n\n    // int start_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS)];\n    // int end_tile = B_row[block_tile_j / WARP_COL_TILES + (warpId % BLOCK_ROW_WARPS) + 1];\n\n    // Go through the global K dimension by a fixed step at a time.\n#pragma unroll\n    for(int tile_k_idx = start_tile; tile_k_idx < end_tile; tile_k_idx += 1){\n\n      size_t shmem_idx = \n        warpId < (WARPS_PER_BLOCK / 2)\n          ? (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP\n          : (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP + shmem_idx_b_off;\n\n      int4 *lane_ptr = NULL;\n      if(warpId < (WARPS_PER_BLOCK / 2)){\n        const uint8_t *warp_ptr = &A[block_tile_i * M * K_GLOBAL] +\n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * K_GLOBAL;\n        lane_ptr = (int4 *)(warp_ptr + B_col[ tile_k_idx] * K * CHUNK_K +\n                        (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +\n                        (laneId % CHUNK_COPY_LINE_LANES);\n      }else{\n        const uint8_t *warp_ptr = B_val + tile_k_idx * (N * BLOCK_ROW_TILES) * (K * CHUNK_K) + \n          (warpId % (WARPS_PER_BLOCK / 2)) * CHUNK_COPY_LINES_PER_WARP * (K * CHUNK_K);\n        lane_ptr = (int4 *)(warp_ptr + (laneId / CHUNK_COPY_LINE_LANES) * (K * CHUNK_K)) +\n                    (laneId % CHUNK_COPY_LINE_LANES);\n      }\n\n      // Shift the second half of the warp to the next row / column in the\n      // shared memory.\n      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;\n\n      int iter_index = warpId < (WARPS_PER_BLOCK / 2)\n        ? (BLOCK_COL_TILES * M) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP)\n        : (BLOCK_ROW_TILES * N) / ((WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n\n\n      #pragma unroll\n      for (int i = 0; i < iter_index; i++) {\n        // Copy 16 bytes at once in each lane.\n        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =\n            *lane_ptr;\n\n        \n        lane_ptr = warpId < (WARPS_PER_BLOCK / 2) ?\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K_GLOBAL * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP):\n                    (int4 *)((uint8_t *)lane_ptr +\n                                        K * CHUNK_K * (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP);\n        shmem_idx += (WARPS_PER_BLOCK / 2) * CHUNK_COPY_LINES_PER_WARP;\n      }\n\n      __syncthreads();\n\n      for (int k_step = 0; k_step < CHUNK_K; k_step++) {\n        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>\n            a[WARP_COL_TILES];\n        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>\n            b[WARP_ROW_TILES];\n\n#pragma unroll\n        for (int i = 0; i < WARP_COL_TILES; i++) {\n          size_t shmem_idx_a = (warpId / BLOCK_ROW_WARPS) * M * WARP_COL_TILES + (i * M);\n          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];\n\n          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n\n#pragma unroll\n          for (int j = 0; j < WARP_ROW_TILES; j++) {\n            if (i == 0) {\n              // Load the B matrix fragment once, because it is going to be\n              // reused against the other A matrix fragments.\n              size_t shmem_idx_b = shmem_idx_b_off +\n                                   (WARP_ROW_TILES * N) * (warpId % BLOCK_ROW_WARPS) +\n                                   (j * N);\n              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];\n\n              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);\n            }\n\n            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);\n          }\n        }\n      }\n\n      __syncthreads();\n    }\n\n      // Store the D fragments to shared memory.\n#pragma unroll\n    for (int i = 0; i < WARP_COL_TILES; i++) {\n#pragma unroll\n      for (int j = 0; j < WARP_ROW_TILES; j++) {\n#pragma unroll\n        // Uniform, point-wise transformations of ALL fragment elements by ALL\n        // threads in the warp are well-defined even though element indices\n        // within fragment storage are not defined.\n        for (int t = 0; t < c[i][j].num_elements; t++) {\n          c[i][j].x[t] = ((c[i][j].x[t] * alpha) >> integer);\n        }\n\n        int *tile_ptr = shmem_warp_tile_ptr + i * M * SHMEM_STRIDE + j * N;\n\n        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, wmma::mem_row_major);\n      }\n    }\n\n    __syncthreads();\n  \n\n    int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M * SHMEM_STRIDE\n                                    + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N; \n    const size_t gmem_idx =\n        (block_tile_i * M + (warpId / BLOCK_ROW_WARPS) * WARP_COL_TILES * M) * GLOBAL_MEM_STRIDE +\n        block_tile_j * N + (warpId % BLOCK_ROW_WARPS) * WARP_ROW_TILES * N;\n    uint8_t *dst_gmem_warp_stream_ptr = &D[gmem_idx];\n\n    int *shmem_lane_stream_ptr = \n        shmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * SHMEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n    uint8_t *dst_gmem_lane_stream_ptr = \n        dst_gmem_warp_stream_ptr + \n        laneId * LANE_ROW_STRIDE / WARP_STRIDE * LANE_COL_STRIDE * GLOBAL_MEM_STRIDE +\n        laneId * LANE_ROW_STRIDE % WARP_STRIDE;\n\n#pragma unroll\n    for (int i = 0; i < LANE_COL_STRIDE; i++){\n      for(int k = 0; k < LANE_ROW_STRIDE; k++){\n        *(dst_gmem_lane_stream_ptr + GLOBAL_MEM_STRIDE * i + k) =\n          (uint8_t)(*(shmem_lane_stream_ptr + SHMEM_STRIDE * i + k));\n      }\n    }\n\n    __syncthreads();\n\n}", "launch_config": {"dimBlock": [64, 1], "dimGrid": [0, 1]}, "block_size_n": 16, "block_size_k": 64}}